{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Andy's Notes on Interesting Topics Encountered Throughout My Embedded Engineering Career","text":"<p>Welcome to the Andy's Notes! This site is a collection of notes, insights, and lessons learned throughout my journey as an embedded engineer. It covers a wide range of topics, including software development, hardware design, and system-level concepts essential to building reliable and efficient embedded systems.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Artificial Intelligence</li> <li>Automotive</li> <li>Cyber Security</li> <li>Digital Systems Design</li> <li>Physics</li> <li>Electronics</li> <li>Protocols and Interfaces</li> <li>Senior Design</li> <li>Signals</li> <li>Software</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>Embedded Systems Engineering Roadmap</li> </ul>"},{"location":"#to-do","title":"To-Do","text":"<ol> <li>Learn LoRa and/or LoRaWAN</li> </ol>"},{"location":"Automotive/pages/","title":"Overview","text":"<ul> <li>Vehicle Electrical/Electronic Architecture</li> <li>Sensors and Signals</li> <li>Controller Area Network (CAN)</li> <li>Power Distribution Module</li> </ul>"},{"location":"Automotive/pages/#resources","title":"Resources","text":"<ol> <li>My ramp-up paper on Cryptography in an Automotive Zero Trust Architecture</li> </ol>"},{"location":"Automotive/pages/architecture/","title":"Vehicle Electrical/Electronic Architecture","text":"<p>Vehicle Electrical/Electronic (E/E) architecture refers to the overall design and integration of a vehicle's electrical and electronic systems, encompassing hardware, software, and communication networks. It defines how components such as sensors, actuators, electronic control units (ECUs), wiring, and communication protocols (e.g., CAN, LIN, Ethernet) are organized and interconnected to enable vehicle functionality. Modern E/E architectures are evolving to support increasing system complexity, including advanced driver-assistance systems (ADAS), infotainment, and electrification, while meeting stringent requirements for performance, scalability, safety, and cybersecurity.</p>"},{"location":"Automotive/pages/architecture/#electronic-control-units-ecus","title":"Electronic Control Units (ECUs)","text":"<p>An Electronic Control Unit (ECU) is an embedded system within automotive electronics that manages a system or task in a vehicle. ECUs act as the brains processing sensory or other relevant data to send control signals to actuators and interfaces. Like most embedded systems, ECUs can take many hardware forms such as microcontrollers, microprocessors, ASICs, and FPGAs.</p>"},{"location":"Automotive/pages/architecture/#automotive-domains","title":"Automotive domains","text":"<p>A domain refers to a system typically composed of a collection of ECUs and their subordinate systems, working together to perform a specific vehicle-level function. What a domain is and which electronics belong in each domain can vary depending on who you ask and the specifics of the vehicle, especially as vehicles become more complex and interconnected over time. Below is how Dr. Ahmad MK Nasser breaks down the domains and provides some example ECUs/Modules for each. I added some images from seperate sources to help visualize these domains.Notice how the images don't line up with Dr. Ahmad's example ECUs, automotive electronics changes wildly depending on the specific vehicle type, manufacturer, model, trim, and even specific factories.</p>"},{"location":"Automotive/pages/architecture/#fuel-based-powertrain-domain","title":"Fuel-Based Powertrain Domain","text":"<p>This domain manages internal combustion engines (ICEs), focusing on tasks such as engine performance, gear shifting, and emissions control.</p> <ul> <li>Engine Control Module (ECM): Regulates engine functions, including ignition timing, fuel injection, and engine cooling</li> <li>Transmission Control Module (TCM): Optimizes gear shifting in automatic transmissions by analyzing input data such as engine speed, vehicle speed, throttle position</li> </ul>"},{"location":"Automotive/pages/architecture/#electric-drive-powertrain-domain","title":"Electric Drive Powertrain Domain","text":"<p>This domain handles battery management and electric motor control in electric vehicles (EVs).</p> <ul> <li>Battery Management System (BMS): Monitors the state of charge (SoC) and state of health (SoH), and ensures battery safety and thermal management.</li> <li>Powertrain Electronic Control Unit (PECU): Manages motor speed and acceleration, and controls voltage and frequency supplied to the motor.</li> </ul>"},{"location":"Automotive/pages/architecture/#chassis-safety-control-domain","title":"Chassis Safety Control Domain","text":"<p>The chassis domain ensures vehicle stability and safety through active and passive safety systems.</p> <ul> <li>Electronic Braking Control Module (EBCM): Provides functions like ABS, ESC, and emergency braking.</li> <li>Electronic Power Steering (EPS): Assists steering and enables lane correction.</li> <li>Airbag Control Module: Deploys airbags during collisions for passenger safety.</li> <li>Advanced Driver Assistance (ADAS) control module: In modern vehicles, a dedicated ECU is often used to achieve advanced levels of autonomy, even though some ADAS functions can be integrated into the EBCM and EPS. This specialized module coordinates with the ECM, EBCM, and EPS to manage engine torque, braking, and steering as required by the situation. By processing inputs from various sensors, the ADAS ECU enables features like autonomous highway driving and automated parking, among other autonomy functions.</li> </ul>"},{"location":"Automotive/pages/architecture/#interior-cabin-domain","title":"Interior Cabin Domain","text":"<p>This domain focuses on comfort, convenience, and security features.</p> <ul> <li>Body Control Module (BCM): Controls keyless entry, seat adjustments, and lighting.</li> <li>Climate Control Module (CCM): Manages cabin heating, cooling, and ventilation.</li> </ul>"},{"location":"Automotive/pages/architecture/#infotainment-and-connectivity-domain","title":"Infotainment and Connectivity Domain","text":"<p>This domain integrates driver and passenger-facing interfaces, focusing on entertainment and connectivity. As you would expect, this domain is becoming increasing significant to electronics engineers due to advancments in V2X communications.</p> <ul> <li>In-Vehicle Infotainment (IVI): Provides entertainment, navigation, and driver information.</li> <li>Telematics Control Unit (TCU): Facilitates GPS, remote connectivity, and over-the-air (OTA) updates.</li> </ul>"},{"location":"Automotive/pages/architecture/#cross-domain-communication","title":"Cross-Domain Communication","text":"<p>Cross-domain functionality provides a reliable communication framework for inter-domain message exchange.</p> <ul> <li>Central Gateway (CGW): Acts as an in-vehicle router, enabling communication across different network segments (e.g., CAN to Ethernet) and supporting cybersecurity measures to block unwanted traffic.</li> </ul>"},{"location":"Automotive/pages/architecture/#vehicle-architecture-types","title":"Vehicle Architecture Types","text":"<p>The design of a vehicle's electrical and electronic (E/E) architecture significantly impacts functionality, security, and resource usage. Over time, E/E architectures have evolved from highly distributed systems to more centralized configurations, aiming to consolidate vehicle functions and enhance computational power. This evolution can be categorized into three main types: highly distributed architectures, domain-centralized architectures, and zone architectures.</p> <p>Note: Vehicle manufacturers (OEMs) may adopt different approaches to evolving their E/E architectures. As a result, some vehicles may feature hybrid architectures that are transitional stages between the types described here. The evolution of vehicle E/E architecture is ongoing, and new architectural classes may emerge in the future.</p>"},{"location":"Automotive/pages/architecture/#highly-distributed-ee-architecture","title":"Highly Distributed E/E Architecture","text":"<p>In a highly distributed architecture, Electronic Control Units (ECUs) are grouped based on similar or interdependent functionalities within shared network segments. Communication protocols such as Controller Area Network (CAN), Local Interconnect Network (LIN), or FlexRay facilitate message exchange between these ECUs. This architecture enables direct mapping between vehicle functions and specific ECUs.</p> <p>However, the addition of new functionalities often requires local gateways to relay messages between network segments (e.g., CAN to LIN). While this allows for scalability without a complete network redesign, it introduces vulnerabilities. Gateways may fail to isolate network segments effectively, increasing the risk of unauthorized access to critical ECUs. Additionally, ECUs with varying levels of security exposure\u2014such as infotainment systems and safety-critical ECUs like braking systems\u2014may be grouped together, heightening security risks.</p> <p>The On-Board Diagnostics (OBD) connector also presents a potential security issue. It is typically connected directly to internal network segments, such as the powertrain or chassis domains, providing a potential entry point for attackers.</p>"},{"location":"Automotive/pages/architecture/#domain-centralized-ee-architecture","title":"Domain-Centralized E/E Architecture","text":"<p>The domain-centralized E/E architecture addresses the challenges of cost, maintenance, and security seen in highly distributed systems. In this configuration, ECUs are grouped into well-defined domains (e.g., powertrain, chassis, infotainment, and body), and communication between these domains is managed through dedicated gateways.</p> <p>A central gateway equipped with a high-speed Ethernet backbone serves as the communication hub, enabling high-bandwidth data transfer across domains. This setup supports advanced features like Diagnostics over IP (DoIP), allowing parallel flashing and diagnostics of multiple ECUs. The gateway enforces network filtering rules, improving security by isolating domains from unauthorized access.</p> <p>By consolidating multiple ECUs into domain-specific controllers, this architecture simplifies maintenance, reduces complexity, and enhances overall security by minimizing the attack surface.</p>"},{"location":"Automotive/pages/architecture/#zone-architecture","title":"Zone Architecture","text":"<p>Zone architecture represents the next step in E/E architecture evolution. This configuration employs powerful vehicle computers or zone ECUs, which connect directly to sensors and actuators within specific physical areas of the vehicle. This design reduces the total number of ECUs, enhances computational efficiency, and simplifies network structures.</p> <p>Zone architectures improve scalability and flexibility while providing robust security. Functions are isolated within distinct zones, making it easier to implement security controls and monitor communications. This design supports advanced features such as autonomous driving and over-the-air updates, meeting the demands of modern vehicles.</p>"},{"location":"Automotive/pages/architecture/#references","title":"References","text":"<ol> <li>Ahmad MK Nasser, Automotive Cybersecurity Engineering Handbook.</li> </ol>"},{"location":"Automotive/pages/architecture/#image-sources","title":"Image Sources","text":"<ol> <li>Electric Drive Powertrain Domain: https://evreporter.com/ev-powertrain-components/</li> <li>Fuel-Based Powertrain Domain: https://afdc.energy.gov/files/vehicles/flexfuel-high-res.jpg</li> <li>Chassis Safety Control Domai: https://www.automotivesafetycouncil.org/wp-content/uploads/2017/01/air-bag-sensor-car-layout-700x441.jpg</li> <li>Interior Cabin Domain: https://d17ocfn2f5o4rl.cloudfront.net/wp-content/uploads/2019/01/In-car-electronics-controlled-via-BCM.jpg</li> <li>Infotainment and Connectivity Domain: https://www.tek.com/en/blog/taking-in-vehicle-infotainment-into-the-future</li> <li>Vehicle Architecture Types: Ahmad MK Nasser, Automotive Cybersecurity Engineering Handbook.</li> </ol>"},{"location":"Automotive/pages/pdm/","title":"MoTeC's Power Distribution Module (PDM)","text":""},{"location":"Automotive/pages/pdm/#overview","title":"Overview","text":"<p>MoTeC's Power Distribution Module (PDM) is an advanced, fully programmable solution for managing the power supply of complex electrical systems in automotive, motorsport, marine, and industrial applications. By replacing traditional fuses and relays with solid-state electronics, the PDM improves reliability, reduces wiring complexity, and enables enhanced system diagnostics.</p>"},{"location":"Automotive/pages/pdm/#key-features","title":"Key Features","text":"<p>1. Solid-State Switching</p> <ul> <li>Eliminates mechanical relays and fuses, providing long-lasting reliability.</li> <li>Enables precise control over current flow and voltage levels.</li> </ul> <p>2. Fully Programmable</p> <ul> <li>Customizable logic for power distribution and circuit protection.</li> <li>Configurable via MoTeC's software to suit various applications.</li> </ul> <p>3. Advanced Diagnostics</p> <ul> <li>Continuous monitoring of current, voltage, and temperature for all channels.</li> <li>Real-time feedback for troubleshooting and optimization.</li> </ul> <p>4. Lightweight and Compact Design</p> <ul> <li>Reduces overall weight and space requirements compared to traditional wiring systems.</li> <li>Ideal for high-performance and space-constrained applications.</li> </ul> <p>5. Flexible Inputs and Outputs</p> <ul> <li>Multiple input and output channels for integration with complex electrical systems.</li> <li>Options for both high-current and low-current loads.</li> </ul>"},{"location":"Automotive/pages/pdm/#benefits","title":"Benefits","text":"<ul> <li>Enhanced Safety: Real-time current monitoring prevents overloads and short circuits.</li> <li>Simplified Wiring: Reduces the need for traditional fuse and relay blocks.</li> <li>Ease of Integration: Seamless compatibility with other MoTeC products such as ECUs and data loggers.</li> <li>Scalability: Suitable for applications ranging from small vehicles to large industrial systems.</li> </ul>"},{"location":"Automotive/pages/pdm/#input-output-circuitry","title":"Input / Output Circuitry","text":""},{"location":"Automotive/pages/pdm/#increase-max-amps","title":"Increase Max Amps","text":""},{"location":"Automotive/pages/pdm/#resources","title":"Resources","text":"<ol> <li>MoTeC's official website.</li> </ol>"},{"location":"Automotive/pages/sensorsSignals/","title":"External Automotive Sensors and Signals","text":"<p>Modern vehicles integrate a variety of sensors to interact with their external environment. These sensors are critical for safety, efficiency, and comfort. Below is a description of key external sensors, their functionalities, and the data they provide.</p>"},{"location":"Automotive/pages/sensorsSignals/#vision-based-sensors","title":"Vision-Based Sensors","text":""},{"location":"Automotive/pages/sensorsSignals/#cameras","title":"Cameras","text":"<p>Cameras capture visual data for various applications. They are used for digital rear-view mirrors, traffic sign recognition, surround view systems, and lane-keeping assistance.</p> <p>Operation: Cameras detect light on a photosensitive surface using a lens and process the data into images for use by ECUs. They typically support 8 MP resolution at 60 FPS, use GMSL links for data transmission at rates up to 6 GB/s, and are often configured via I2C.</p> <p>Risks: Interference can lead to misidentification of objects or privacy violations.</p>"},{"location":"Automotive/pages/sensorsSignals/#infrared-sensors","title":"Infrared Sensors","text":"<p>Infrared sensors detect heat signatures, improving visibility in low-light conditions.</p> <p>Applications: Night vision systems, pedestrian detection, and animal detection.</p> <p>Operation: Detect emitted infrared radiation and process it into thermal images.</p> <p>Risks: Sensor obfuscation can reduce visibility.</p>"},{"location":"Automotive/pages/sensorsSignals/#proximity-and-object-detection-sensors","title":"Proximity and Object Detection Sensors","text":""},{"location":"Automotive/pages/sensorsSignals/#lidar-light-detection-and-ranging","title":"LiDAR (Light Detection and Ranging)","text":"<p>LiDAR generates 3D spatial maps by emitting laser pulses. It is used for object detection and classification, as well as autonomous navigation.</p> <p>Operation: Measures time-of-flight of laser pulses to calculate distances and interfaces with the host ECU via 1000Base-T1 Ethernet.</p> <p>Risks: Tampering can disrupt object detection and compromise safety systems.</p>"},{"location":"Automotive/pages/sensorsSignals/#radar-radio-detection-and-ranging","title":"RADAR (Radio Detection and Ranging)","text":"<p>RADAR detects objects using electromagnetic waves. It is applied in adaptive cruise control, collision avoidance, and blind-spot monitoring.</p> <p>Operation: Measures range and speed using the Doppler effect and connects to the host ECU via CAN or Ethernet.</p> <p>Risks: Jamming or spoofing can impact obstacle detection.</p>"},{"location":"Automotive/pages/sensorsSignals/#ultrasonic-sensors","title":"Ultrasonic Sensors","text":"<p>Ultrasonic sensors detect nearby objects using high-frequency sound waves. They are commonly used for parking assistance and proximity alerts.</p> <p>Operation: Emit sound waves and calculate the distance based on reflected echoes. They communicate with the ECU via CAN.</p> <p>Risks: Signal interference can compromise obstacle detection.</p>"},{"location":"Automotive/pages/sensorsSignals/#communication-and-navigation-systems","title":"Communication and Navigation Systems","text":""},{"location":"Automotive/pages/sensorsSignals/#vehicle-to-everything-v2x-communication","title":"Vehicle-to-Everything (V2X) Communication","text":"<p>V2X communication enables interaction between vehicles, infrastructure, and other road users.</p> <p>Applications: Traffic management and collision prevention.</p> <p>Operation: Uses DSRC or cellular networks for wireless data exchange.</p> <p>Risks: Cyberattacks can compromise communication reliability.</p>"},{"location":"Automotive/pages/sensorsSignals/#telematics-systems","title":"Telematics Systems","text":"<p>Telematics systems integrate GPS, cellular communication, and sensors to enable real-time data exchange with remote servers.</p> <p>Applications: Fleet management, remote diagnostics, over-the-air (OTA) updates, and stolen vehicle recovery.</p> <p>Operation: Collects data from sensors like GPS and speedometers, sends it to cloud servers via cellular networks, and allows bidirectional communication for remote management.</p> <p>Risks: Data breaches can expose sensitive information, and communication hijacking can lead to unauthorized vehicle control or tracking.</p>"},{"location":"Automotive/pages/sensorsSignals/#gnss-global-navigation-satellite-system","title":"GNSS (Global Navigation Satellite System)","text":"<p>GNSS provides location, velocity, and timing information. It is used for navigation, geofencing, and route optimization.</p> <p>Operation: Receives satellite signals to determine position and interfaces with the host ECU via CAN or UART.</p> <p>Risks: Spoofing or jamming can disrupt localization and timing.</p>"},{"location":"Automotive/pages/sensorsSignals/#access-and-monitoring-systems","title":"Access and Monitoring Systems","text":""},{"location":"Automotive/pages/sensorsSignals/#passive-keyless-entry-pke-and-remote-keyless-entry-rke","title":"Passive Keyless Entry (PKE) and Remote Keyless Entry (RKE)","text":"<p>PKE and RKE systems enable automatic vehicle access and remote locking/unlocking.</p> <p>Operation: PKE detects key proximity using low-frequency (LF) and ultra-high frequency (UHF) signals, while RKE uses RF signals for remote interaction.</p> <p>Risks: Relay attacks can exploit vulnerabilities in signal transmission.</p>"},{"location":"Automotive/pages/sensorsSignals/#tire-pressure-monitoring-system-tpms","title":"Tire Pressure Monitoring System (TPMS)","text":"<p>TPMS monitors tire pressure to ensure safety and efficiency.</p> <p>Operation: Sensors transmit pressure data wirelessly to the ECU, typically at 315 MHz or 433 MHz.</p> <p>Risks: Signal spoofing can result in false alerts or undetected issues.</p>"},{"location":"Automotive/pages/sensorsSignals/#summary","title":"Summary","text":"<p>These external sensors and systems form the backbone of modern vehicles, enabling advanced functionalities and ensuring safety. Grouping and understanding their roles, communication methods, and potential vulnerabilities is critical for optimizing performance and securing operations.</p>"},{"location":"Cybersecurity/pages/","title":"Overview","text":""},{"location":"Cybersecurity/pages/#resources","title":"Resources","text":"<ol> <li>My ramp-up paper on Cryptography in an Automotive Zero Trust Architecture</li> <li>CERT Coding ptandards</li> </ol>"},{"location":"DSD/","title":"Digital System Design","text":"<p>Digital System Design (DSD) focuses on creating and implementing digital systems using hardware description languages (HDLs) like Verilog and VHDL. It involves designing, simulating, and optimizing combinational and sequential circuits for tasks such as data processing, communication, and control. DSD bridges the gap between theoretical logic design and practical hardware implementation, enabling the development of complex digital systems like processors, controllers, and FPGA-based solutions.</p> <ul> <li>Verilog Basics</li> <li>Logic Design</li> <li>Simulations</li> <li>Applications and Examples</li> </ul>"},{"location":"DSD/#resources","title":"Resources","text":"<p>I worked on several engaging projects for the Nexys A7 Board, focusing on tasks such as handling inputs from buttons, switches, and PWM signals. These projects showcase computations and outputs via hex displays, VGA, and PWM control. You can download a zip file containing all the projects here: DSD Lab Projects</p> <p>Lab reports and Homework</p>"},{"location":"DSD/applications/","title":"Applications and Examples","text":""},{"location":"DSD/applications/#4-bit-adder","title":"4-bit Adder","text":"<p>A 4-bit adder adds two 4-bit binary numbers together, producing a 4-bit sum output along with a carry-out bit that indicates an overflow if the sum exceeds the capacity of 4 bits.</p> <p></p>"},{"location":"DSD/applications/#ripple-carry-adder-rca","title":"Ripple Carry Adder (RCA)","text":"<pre><code>module FullAdder (A, B, Ci, Co, S);\n\n    input A, B, Ci;    // Inputs: A, B, and Carry In (Ci)\n    output S, Co;      // Outputs: Sum (S) and Carry Out (Co)\n\n    assign S = A ^ B ^ Ci;      // Sum calculation\n    assign Co = (A &amp; B) | (B &amp; Ci) | (A &amp; Ci);  // Carry Out calculation\n\nendmodule\n\n\nmodule Adder4 (S, Co, A, B, Ci);\n\n    input [3:0] A, B;  // 4-bit inputs\n    input Ci;          // Carry input\n    output [3:0] S;    // 4-bit sum output\n    output Co;         // Carry out\n    wire [3:1] C;      // Internal carry signals\n\n    // Instantiate 4 Full Adders\n    FullAdder FA0 (A[0], B[0], Ci, C[1], S[0]);\n    FullAdder FA1 (A[1], B[1], C[1], C[2], S[1]);\n    FullAdder FA2 (A[2], B[2], C[2], C[3], S[2]);\n    FullAdder FA3 (A[3], B[3], C[3], Co, S[3]);\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#look-ahead-adder-cla","title":"Look-Ahead Adder (CLA)","text":"<p>A 4-bit carry look-ahead adder (CLA) is a faster alternative to the ripple carry adder, as it calculates the carries in parallel rather than waiting for each carry to propagate through the stages sequentially. The carry look-ahead adder computes the carry signals using the generate (G) and propagate (P) functions.</p> <pre><code>module cla_4bit (\n    input [3:0] A, B,\n    input Cin,\n    output [3:0] Sum,\n    output Cout\n);\n    wire [3:0] G, P;  // Generate and propagate signals\n    wire C1, C2, C3;\n\n    // Generate and propagate signals\n    assign G = A &amp; B;  // Generate\n    assign P = A | B;  // Propagate\n\n    // Carry look-ahead logic\n    assign #5 C1 = G[0] | (P[0] &amp; Cin);\n    assign #5 C2 = G[1] | (P[1] &amp; G[0]) | (P[1] &amp; P[0] &amp; Cin);\n    assign #5 C3 = G[2] | (P[2] &amp; G[1]) | (P[2] &amp; P[1] &amp; G[0]) | (P[2] &amp; P[1] &amp; P[0] &amp; Cin);\n    assign #5 Cout = G[3] | (P[3] &amp; G[2]) | (P[3] &amp; P[2] &amp; G[1]) | (P[3] &amp; P[2] &amp; P[1] &amp; G[0]) | (P[3] &amp; P[2] &amp; P[1] &amp; P[0] &amp; Cin);\n\n    // Sum calculation\n    assign #5 Sum[0] = A[0] ^ B[0] ^ Cin;\n    assign #5 Sum[1] = A[1] ^ B[1] ^ C1;\n    assign #5 Sum[2] = A[2] ^ B[2] ^ C2;\n    assign #5 Sum[3] = A[3] ^ B[3] ^ C3;\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#delay-comparison-carry-look-ahead-adder-cla-vs-ripple-carry-adder-rca","title":"Delay Comparison: Carry Look-Ahead Adder (CLA) vs. Ripple Carry Adder (RCA)","text":""},{"location":"DSD/applications/#1-ripple-carry-adder-rca","title":"1. Ripple Carry Adder (RCA)","text":"<ul> <li>Delay Growth: Linear <code>O(N)</code> (increases with the number of bits)</li> <li>Total Delay: <code>N * Gate Delay per Full Adder</code></li> </ul>"},{"location":"DSD/applications/#2-carry-look-ahead-adder-cla","title":"2. Carry Look-Ahead Adder (CLA)","text":"<ul> <li>Delay Growth: Logarithmic <code>O(log N)</code> (grows slower as the number of bits increases)</li> <li>Total Delay: <code>log_2(N) * Gate Delay per Stage</code></li> </ul>"},{"location":"DSD/applications/#3-comparison-of-delays","title":"3. Comparison of Delays","text":"Number of Bits RCA Delay (Linear) CLA Delay (Logarithmic) 4 bits 20 ns 20 ns 8 bits 40 ns 15 ns 16 bits 80 ns 20 ns 32 bits 160 ns 25 ns 64 bits 320 ns 30 ns"},{"location":"DSD/applications/#4-to-1-mux","title":"4-to-1 Mux","text":"<p>A 4-to-1 multiplexer selects one of four input signals based on a 2-bit selection input and forwards the selected input to the output.</p> <p></p> <pre><code>module Mux4to1 (\n    input wire I0, I1, I2, I3,  // 4 data inputs\n    input wire S0, S1,          // 2 select lines\n    output wire Y               // Output\n);\n\nassign Y = (~S1 &amp; ~S0 &amp; I0) |  // Select I0 when S1 = 0, S0 = 0\n           (~S1 &amp;  S0 &amp; I1) |  // Select I1 when S1 = 0, S0 = 1\n           ( S1 &amp; ~S0 &amp; I2) |  // Select I2 when S1 = 1, S0 = 0\n           ( S1 &amp;  S0 &amp; I3);   // Select I3 when S1 = 1, S0 = 1\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#shift-register","title":"Shift Register","text":""},{"location":"DSD/applications/#4-bit-left-shift","title":"4-bit Left Shift","text":"<p>A 4-bit left shift register shifts the contents of the register one bit to the left on each clock cycle, with a new bit introduced at the least significant bit (LSB) and the most significant bit (MSB) being discarded.</p> <p></p> <pre><code>module LeftShiftRegister (\n    input wire clk,           // Clock input\n    input wire reset,         // Reset input\n    input wire D_in,          // Serial data input\n    output reg [3:0] Q        // 4-bit parallel output\n);\n\n// On the positive edge of the clock, perform the left shift\nalways @(posedge clk or posedge reset) begin\n    if (reset) begin\n        Q &lt;= 4'b0000;         // Reset all outputs to 0\n    end else begin\n        Q &lt;= {Q[2:0], D_in};  // Shift left and input new data bit on Q0\n    end\nend\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#8-bit-left-shift","title":"8-bit Left Shift","text":"<pre><code>module shift_register_8bit (\n    input wire SI,     // Serial input\n    input wire Clk,    // Clock signal (rising edge)\n    input wire EN,     // Enable (active high)\n    output reg SO      // Serial output\n);\n\n    // Internal 8-bit register to hold the shift data\n    reg [7:0] shift_reg;\n\n    // Always block triggered on rising edge of clock\n    always @(posedge Clk) begin\n        if (EN) begin\n            // Shift register: shift in SI and shift out MSB (SO)\n            SO &lt;= shift_reg[7];      // MSB is shifted out\n            shift_reg &lt;= {shift_reg[6:0], SI};  // Shift left and input SI\n        end\n    end\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#8-bit-counter-lab-3","title":"8-bit Counter (Lab 3)","text":"<p>Lab simulation: 3-Bit Up/Down counter with a clear and load.</p> <p>This lab involves designing an 8-bit up/down counter using Verilog, implemented on the NEXYS-4 FPGA board. A Vivado project is provided for this example.</p> <p>The CD74HC190/191 and CD54HC190/191 are highly versatile presettable up/down counters commonly used in digital electronics for tasks requiring accurate counting, such as digital clocks, timers, and frequency dividers. These ICs can be configured to count in either binary (CD74HC191) or BCD (Binary-Coded Decimal) format (CD74HC190), and support synchronous counting with a variety of control options, including asynchronous presetting, counting direction control, and enabling inputs. With additional outputs like ripple carry and terminal count, these counters are ideal for cascading multiple units to create larger counting systems.</p> <p>In Lab 3, we implemented an 8-bit up/down counter based on the functionality of the 74HC190/191 but directly in Verilog, bypassing the need for discrete hardware. This allowed us to overcome common design challenges such as signal ripple effects and cascading hazards that typically occur when connecting multiple 4-bit counters. By using Verilog, we streamlined the design, implementing advanced features such as parallel loading, which enables the counter to start from a user-defined value, and a clock control to slow down the counting process for easier observation.</p>"},{"location":"DSD/applications/#identifying-required-number-of-bits","title":"Identifying Required Number of bits","text":"<p>To determine how many bits are needed to count to a specific number \\(N\\), use the following steps:</p> <ol> <li>Use the formula: <code>n = log2(N)</code> where <code>n</code> is the number of bits.</li> <li>Round up the result to the nearest whole number.</li> <li>A counter with <code>n</code> bits can represent numbers from <code>0</code> to <code>2^n - 1</code>, which must be greater than or equal to \\(N\\).</li> </ol>"},{"location":"DSD/applications/#example","title":"Example:","text":"<p>To count up to 100:</p> <ol> <li><code>n = log2(100)</code> \u2248 6.64</li> <li>Round up to 7.</li> </ol> <p>You would need 7 bits to count up to 100, as 7 bits can represent values from 0 to 127.</p>"},{"location":"DSD/applications/#parallel-loading","title":"Parallel Loading","text":"<p>Parallel loading allows a counter to load a preset value into all its flip-flops simultaneously in a single clock cycle. By using input lines (like A-D or D0-D3) and a load signal, the counter can instantly update its value upon the next clock cycle, bypassing the need for sequential counting. This is particularly useful for initializing the counter or synchronizing multiple counters efficiently. In this lab, we used parallel loading to preset the 8-bit counter with values determined by the input switches, adding greater flexibility to the counting process.</p>"},{"location":"DSD/applications/#synchronous-4-bit-updown-counter-74hc192","title":"Synchronous 4-bit Up/Down Counter (74HC192)","text":"<pre><code>module counter_74HC192 (\n    input wire Clr,      // Clear the counter\n    input wire Load,     // Load the counter with a value\n    input wire Up,       // Count up when asserted\n    input wire Down,     // Count down when asserted\n    input wire [3:0] P,  // Data input for loading preset\n    input wire Clk,      // Clock input\n    output reg [3:0] Q   // 4-bit counter output\n);\n\n// Always block, sensitive to Clk, Clr, Load, Up, Down\nalways @(posedge Clk or posedge Clr or posedge Load) begin\n    if (Clr) begin\n        Q &lt;= 4'b0000;  // Clear the counter\n    end\n    else if (Load) begin\n        Q &lt;= D;  // Load the counter with input value\n    end\n    else if (Up &amp;&amp; !Down) begin\n        Q &lt;= Q + 1;  // Increment counter\n    end\n    else if (Down &amp;&amp; !Up) begin\n        Q &lt;= Q - 1;  // Decrement counter\n    end\nend\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#sr-latch","title":"SR Latch","text":""},{"location":"DSD/applications/#sr-latch-using-a-boolean-equation","title":"SR Latch Using a Boolean Equation","text":"<pre><code>module sr_latch_boolean (\n    input wire S,   // Set input\n    input wire R,   // Reset input\n    output reg Q,   // Output\n    output reg Qn   // Inverted output\n);\n\n    always @(*) begin\n        if (S &amp;&amp; !R) begin\n            Q &lt;= 1;   // Set\n            Qn &lt;= 0;\n        end\n        else if (!S &amp;&amp; R) begin\n            Q &lt;= 0;   // Reset\n            Qn &lt;= 1;\n        end\n        // If S == 0 and R == 0, Q and Qn retain previous state (no action)\n    end\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#sr-latch-using-gate-level-description","title":"SR Latch Using Gate-Level Description","text":"<pre><code>module sr_latch_gate_level (\n    input wire S,   // Set input\n    input wire R,   // Reset input\n    output wire Q,  // Output\n    output wire Qn  // Inverted output\n);\n\n    wire nand1_out, nand2_out;\n\n    // NAND gates for SR Latch\n    nand (nand1_out, S, Qn);  // First NAND gate\n    nand (nand2_out, R, Q);   // Second NAND gate\n\n    assign Q = nand1_out;\n    assign Qn = nand2_out;\n\nendmodule\n</code></pre>"},{"location":"DSD/applications/#4-bit-comparator","title":"4-bit Comparator","text":"<p>A 4-bit comparator is a digital circuit that compares two 4-bit binary numbers and outputs the result of their comparison. It determines whether one number is greater than, less than, or equal to the other.</p>"},{"location":"DSD/applications/#inputs","title":"Inputs","text":"<ul> <li>A[3:0]: 4-bit binary number (A3, A2, A1, A0)</li> <li>B[3:0]: 4-bit binary number (B3, B2, B1, B0)</li> </ul>"},{"location":"DSD/applications/#outputs","title":"Outputs","text":"<ul> <li>A &gt; B: Output is 1 if the binary number A is greater than B.</li> <li>A &lt; B: Output is 1 if the binary number A is less than B.</li> <li>A = B: Output is 1 if the binary number A is equal to B.</li> </ul>"},{"location":"DSD/applications/#logic-for-comparison","title":"Logic for Comparison","text":"<p>To compare two 4-bit numbers, the comparison starts from the most significant bit (MSB) down to the least significant bit (LSB):</p> <ol> <li>A &gt; B:</li> <li>If A3 &gt; B3, then A &gt; B regardless of the lower bits.</li> <li>If A3 = B3, move to compare A2 and B2.</li> <li> <p>Repeat for all bits until a difference is found or A = B.</p> </li> <li> <p>A &lt; B:</p> </li> <li>If A3 &lt; B3, then A &lt; B.</li> <li>If A3 = B3, move to compare A2 and B2.</li> <li> <p>Continue this until a difference is found or A = B.</p> </li> <li> <p>A = B:</p> </li> <li>If all bits (A3 to A0) are equal to their corresponding B bits, then A = B.</li> </ol>"},{"location":"DSD/applications/#truth-table","title":"Truth Table","text":"A3 A2 A1 A0 B3 B2 B1 B0 A &gt; B A &lt; B A = B 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 ... ... ... ... ... ... ... ... ... ... ..."},{"location":"DSD/applications/#verilog-implementation","title":"Verilog Implementation","text":"<p>A simple implementation in Verilog:</p> <pre><code>module Comparator4bit(\n    input [3:0] A, B,\n    output A_greater, A_less, A_equal\n);\n\nassign A_greater = (A &gt; B);\nassign A_less = (A &lt; B);\nassign A_equal = (A == B);\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/","title":"Digital System Design","text":""},{"location":"DSD/dsd_old/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Verilog Basics<ul> <li>Data Types</li> <li>Vectors in Verilog</li> <li>Primitives</li> <li>Verilog Operator Precednece</li> <li>Shifts</li> <li>Functions and Tasks</li> </ul> </li> <li>Logic Design<ul> <li>Boolean Algebra</li> <li>Sequential and Combinational Assignments</li> <li>State Machines</li> </ul> </li> <li>Simulations<ul> <li>Delays in Verilog</li> <li>Wait Statements</li> </ul> </li> <li>Applications<ul> <li>4-bit Adder</li> <li>4-to-1 Mux</li> <li>Left Shift Register</li> <li>8 Bit Counter (Lab 3)</li> <li>Synchronous 4-bit Up/Down Counter</li> <li>SR Latch</li> </ul> </li> <li>Nexys A7</li> <li>To Do</li> </ul>"},{"location":"DSD/dsd_old/#verilog-basics","title":"Verilog Basics","text":""},{"location":"DSD/dsd_old/#data-types","title":"Data Types","text":"Data Type Purpose Characteristics <code>wire</code> Combinational logic and connections Cannot hold state, used in continuous assignments <code>reg</code> Sequential and combinational logic Holds state, used inside <code>always</code> blocks <code>integer</code> Signed 32-bit value for loops/counters Used in loops and non-synthesizable code <code>real</code> Floating-point value Used in non-synthesizable, behavioral code <code>time</code> 64-bit value to represent simulation time Used for timing and measuring delays in simulation <code>tri</code> Tri-state buffer signal Can take high-impedance (<code>Z</code>) values"},{"location":"DSD/dsd_old/#vectors-in-verilog","title":"Vectors in Verilog","text":"<p>Vectors in Verilog are used to represent multi-bit signals, which are crucial when dealing with buses, registers, or large numbers. They allow for grouping multiple bits into a single variable.</p> <p>A vector is declared by specifying the range of bits using <code>[MSB:LSB]</code>, where MSB is the most significant bit and LSB is the least significant bit.</p> <pre><code>wire [3:0] bus;   // 4-bit wide wire (vector)\nreg  [7:0] data;  // 8-bit register\n</code></pre> <p>Individual bits or a range of bits within a vector can be accessed as follows:</p> <pre><code>wire [7:0] data;\nassign bit3 = data[3];     // Accessing the 3rd bit of data\nassign lower_nibble = data[3:0];  // Accessing the lower 4 bits of data\n</code></pre> <p>You can assign values directly to vectors:</p> <pre><code>reg [3:0] result;\nresult = 4'b1010;  // Assigning binary value\nresult = 4'hA;     // Assigning hexadecimal value\n</code></pre> <p>A vector can have zero width when the MSB and LSB are the same, meaning it's a single-bit signal:</p> <pre><code>wire [0:0] single_bit;  // Equivalent to a scalar\n</code></pre> <p>By default, vectors are unsigned, but they can be declared as signed if needed:</p> <pre><code>signed reg [7:0] signed_data;  // Signed 8-bit register\n</code></pre> <p>In signed vectors, the most significant bit (MSB) is treated as the sign bit.</p>"},{"location":"DSD/dsd_old/#primitives","title":"Primitives","text":"<p>A Verilog primitive is a pre-defined logic element used in digital designs. These include basic gates like <code>and</code>, <code>or</code>, <code>nand</code>, and <code>xor</code>, with fixed functions that don't require module definitions.</p> <p>Output Declaration: In Verilog, when declaring a UDP, the output must always be listed first, followed by the input(s). This order is essential for the proper functioning of the UDP.</p> <p>Types of Primitives: - Combinational Primitives: e.g., <code>and</code>, <code>or</code>, <code>xor</code> - Sequential Primitives: Flip-flops, latches</p>"},{"location":"DSD/dsd_old/#user-defined-primitives-udps","title":"User Defined Primitives (UDPs)","text":"<p>UDPs are custom-defined logic, either combinational or sequential, declared using the <code>primitive</code> keyword. They use a truth table to define behavior.</p> <ul> <li>Example (Combinational UDP):     ```verilog<pre><code>primitive my_and (out, in1, in2);  \noutput out;  \ninput in1, in2;\n\n// The 'table' defines the behavior of this custom primitive.\n// Each row in the table specifies input combinations and the corresponding output.\n// For an AND gate, the output is 1 only when both inputs are 1.\ntable\n    0 0 : 0;  // If both inputs are 0, the output is 0.\n    1 1 : 1;  // If both inputs are 1, the output is 1.\n    // For simplicity, intermediate input states (like 0 1 or 1 0) are not explicitly defined here,\n    // but in a full AND gate implementation, these would typically output 0.\nendtable\n\nendprimitive\n</code></pre> <p>```</p> </li> </ul>"},{"location":"DSD/dsd_old/#verilog-operator-precedence","title":"Verilog Operator Precedence","text":"<p>In Verilog, operators follow a specific order of precedence. This determines how expressions are evaluated when there are multiple operators in the same expression. Below is the list of operators in order of precedence, from highest to lowest:</p> <ol> <li>Unary operators</li> <li><code>+</code>, <code>-</code> (unary plus and minus)</li> <li><code>!</code> (logical NOT)</li> <li><code>~</code> (bitwise NOT)</li> <li><code>&amp;</code>, <code>~&amp;</code> (reduction AND, NAND)</li> <li><code>|</code>, <code>~|</code> (reduction OR, NOR)</li> <li> <p><code>^</code>, <code>~^</code>, <code>^~</code> (reduction XOR, XNOR)</p> </li> <li> <p>Multiplicative operators</p> </li> <li> <p><code>*</code>, <code>/</code>, <code>%</code> (multiply, divide, modulus)</p> </li> <li> <p>Additive operators</p> </li> <li> <p><code>+</code>, <code>-</code> (addition, subtraction)</p> </li> <li> <p>Shift operators</p> </li> <li> <p><code>&lt;&lt;</code>, <code>&gt;&gt;</code> (logical shift left, right)</p> </li> <li> <p>Relational operators</p> </li> <li> <p><code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code> (less than, less than or equal, greater than, greater than or equal)</p> </li> <li> <p>Equality operators</p> </li> <li><code>==</code>, <code>!=</code> (logical equality, inequality)</li> <li> <p><code>===</code>, <code>!==</code> (case equality, case inequality)</p> </li> <li> <p>Bitwise operators</p> </li> <li> <p><code>&amp;</code>, <code>|</code>, <code>^</code>, <code>^~</code>, <code>~^</code> (AND, OR, XOR, XNOR)</p> </li> <li> <p>Logical operators</p> </li> <li><code>&amp;&amp;</code> (logical AND)</li> <li> <p><code>||</code> (logical OR)</p> </li> <li> <p>Conditional operator</p> </li> <li> <p><code>? :</code> (ternary operator)</p> </li> <li> <p>Assignment operators</p> <ul> <li><code>=</code>, <code>+=</code>, <code>-=</code>, <code>*=</code>, <code>/=</code>, <code>%=</code>, <code>&lt;&lt;=</code>, <code>&gt;&gt;=</code>, <code>&amp;=</code>, <code>|=</code>, <code>^=</code> (assignment and compound assignments)</li> </ul> </li> </ol>"},{"location":"DSD/dsd_old/#shifts","title":"Shifts","text":"<p>In Verilog, shift operations move bits of a value to the left or right. There are two types: logical shifts and arithmetic shifts.</p>"},{"location":"DSD/dsd_old/#1-logical-shifts","title":"1. Logical Shifts","text":"<p>Logical shifts move bits and fill the vacated positions with zeros.</p> <p>a. Logical Left Shift (<code>&lt;&lt;</code>) Shifts bits to the left by the specified amount, inserting zeros on the right. This is equivalent to multiplying by a power of 2.</p> <p>Syntax:</p> <pre><code>result = value &lt;&lt; shift_amount;\n</code></pre> <p>Example:</p> <pre><code>wire [3:0] value = 4'b1010; \nassign result = value &lt;&lt; 1;  // Result: 0100\n</code></pre> <p>b. Logical Right Shift (<code>&gt;&gt;</code>) Shifts bits to the right, inserting zeros on the left.</p> <p>Syntax:</p> <pre><code>result = value &gt;&gt; shift_amount;\n</code></pre> <p>Example:</p> <pre><code>wire [3:0] value = 4'b1010;\nassign result = value &gt;&gt; 1;  // Result: 0101\n</code></pre>"},{"location":"DSD/dsd_old/#2-arithmetic-shifts","title":"2. Arithmetic Shifts","text":"<p>Arithmetic shifts preserve the sign of signed numbers when shifting right.</p> <p>a. Arithmetic Right Shift (<code>&gt;&gt;&gt;</code>) Shifts bits to the right, preserving the sign by filling the leftmost bits with the sign bit (MSB).</p> <p>Syntax:</p> <pre><code>result = value &gt;&gt;&gt; shift_amount;\n</code></pre> <p>Example:</p> <pre><code>wire signed [3:0] value = -4;  // Binary: 1100 (two's complement)\nassign result = value &gt;&gt;&gt; 1;   // Result: 1110\n</code></pre>"},{"location":"DSD/dsd_old/#functions-and-tasks","title":"Functions and Tasks","text":"Aspect Functions Tasks Return Type Returns a single value. Can return multiple values via <code>output</code> ports. Time Control No timing control (<code>#</code>, <code>@</code>, <code>wait</code> not allowed). Supports timing control (can use <code>#</code>, <code>@</code>, <code>wait</code>). Arguments Only <code>input</code> arguments. Can have <code>input</code>, <code>output</code>, and <code>inout</code> arguments. Usage Used in expressions directly. Called as a separate statement. Execution Time Executes in zero simulation time. Takes simulation time to execute. Use Case Simple, combinational calculations. Complex tasks with delays or multiple outputs."},{"location":"DSD/dsd_old/#example-function","title":"Example Function:","text":"<pre><code>function [3:0] add;\n  input [3:0] a, b;\n  begin\n    add = a + b;\n  end\nendfunction\n</code></pre>"},{"location":"DSD/dsd_old/#example-task","title":"Example Task:","text":"<pre><code>task add_sub;\n  input [3:0] a, b;\n  output [3:0] sum, diff;\n  begin\n    sum = a + b;\n    diff = a - b;\n  end\nendtask\n</code></pre>"},{"location":"DSD/dsd_old/#example-task-with-event-control","title":"Example Task (With Event Control):","text":"<pre><code>task event_control_example;\n  input [3:0] a, b;\n  output reg [3:0] result;\n  begin\n    wait (a == b);   // Wait until a equals b\n    result = a + b;  // Perform addition\n  end\nendtask\n</code></pre>"},{"location":"DSD/dsd_old/#example-task-with-output-and-inout-arguments-called-as-a-separate-statement","title":"Example Task (with <code>output</code> and <code>inout</code> Arguments Called as a Separate Statement):","text":"<pre><code>module task_example;\n  reg [3:0] x, y, z, sum_result, diff_result;\n\n  initial begin\n    x = 4'b1010;              // Assign some values\n    y = 4'b0110;\n    z = 4'b0011;\n\n    // Call the task as a separate statement\n    arithmetic_operations(x, y, sum_result, z);  // `z` will be modified in-place as an inout\n  end\nendmodule\n\ntask arithmetic_operations;\n  input [3:0] a, b;           // Input arguments\n  output reg [3:0] sum;       // Output argument\n  inout [3:0] diff;           // Inout argument\n  begin\n    sum = a + b;              // Perform addition\n    diff = diff - a;          // Modify the inout value\n  end\nendtask\n</code></pre>"},{"location":"DSD/dsd_old/#logic-design","title":"Logic Design","text":""},{"location":"DSD/dsd_old/#boolean-algebra","title":"Boolean Algebra","text":"Rule Expression Hint Identity Law A + 0 = A No change when OR'ed with 0 Identity Law A \u2022 1 = A No change when AND'ed with 1 Null Law A + 1 = 1 OR'ing with 1 results in 1 Null Law A \u2022 0 = 0 AND'ing with 0 results in 0 Complement Law A + A' = 1 A variable OR'ed with its complement is 1 Complement Law A \u2022 A' = 0 A variable AND'ed with its complement is 0 Idempotent Law A + A = A OR'ing a variable with itself leaves it unchanged Idempotent Law A \u2022 A = A AND'ing a variable with itself leaves it unchanged Domination Law A + A'B = A + B Apply the Distributive Law to simplify Distributive Law A(B + C) = AB + AC Distributes AND over OR Distributive Law A + BC = (A + B)(A + C) Distributes OR over AND Absorption Law A + AB = A Removes redundant terms Absorption Law A(A + B) = A Removes redundant terms Double Negation Law (A')' = A Negation of a negation returns the original value De Morgan\u2019s Law (A \u2022 B)' = A' + B' Apply to break AND terms when converting SOP to POS De Morgan\u2019s Law (A + B)' = A' \u2022 B' Apply to break OR terms when converting POS to SOP Involution Law (A'') = A A variable twice negated is equal to itself Consensus Theorem AB + A'C + BC = AB + A'C Simplifies expressions by eliminating redundant terms Distributive (SOP to POS hint) A + BC = (A + B)(A + C) Useful for converting SOP to POS Distributive (POS to SOP hint) A(B + C) = AB + AC Useful for converting POS to SOP Demorgans (SOP to POS hint) (A \u2022 B)' = A' + B' Apply De Morgan\u2019s Law during the conversion Demorgans (POS to SOP hint) (A + B)' = A' \u2022 B' Apply De Morgan\u2019s Law during the conversion Redundancy Law AB + AB' = A Removes redundant variables from the equation"},{"location":"DSD/dsd_old/#sequential-and-combinational-assignments","title":"Sequential and Combinational Assignments","text":"<ul> <li>Sequential = Procedural: Sequential logic updates state based on clock edges and uses procedural assignments, typically with non-blocking (<code>&lt;=</code>) assignments. It is modeled inside <code>always</code> blocks that are sensitive to clock edges (e.g., <code>posedge clk</code>). This describes systems like flip-flops or registers that rely on previous states.</li> </ul> <p>Example: <code>verilog   always @(posedge clk or posedge reset) begin       if (reset)           q &lt;= 0;      // Asynchronous reset       else           q &lt;= d;      // Update q with d at clock edge   end</code></p> <ul> <li> <p>Combinational = Continuous: Combinational logic depends purely on the current inputs and is described using continuous (blocking) assignments. It models circuits like AND, OR gates, where output is updated as soon as inputs change, without regard to clock cycles.</p> <p>Example: <code>verilog assign y = a &amp; b;  // Output y changes immediately based on a and b</code></p> </li> </ul>"},{"location":"DSD/dsd_old/#relevant-constructs","title":"Relevant Constructs","text":"<ul> <li> <p><code>always</code> blocks: Used to describe both sequential and combinational logic. For sequential logic, <code>always @(posedge clk)</code> or <code>always @(negedge clk)</code> is used, whereas for combinational logic, <code>always @(*)</code> is used to capture all input changes automatically.     Example: <code>verilog     always @(*) begin         result = a | b;  // Combinational logic triggered by any input change     end</code></p> </li> <li> <p><code>initial</code> blocks: Used to define initial conditions in simulations. They execute once at the start of the simulation and are commonly used for testbenches or initializing registers/variables in simulation but are not synthesized into hardware.     Example <code>verilog     initial begin         reg_x = 0;  // Initialize reg_x to 0 at simulation start     end</code></p> </li> </ul>"},{"location":"DSD/dsd_old/#key-points","title":"Key Points","text":"<ul> <li>Blocking (<code>=</code>) vs. Non-blocking (<code>&lt;=</code>): In sequential logic (<code>always @(posedge clk)</code>), use non-blocking assignments (<code>&lt;=</code>) to ensure parallel updates. In combinational logic, blocking assignments (<code>=</code>) can be used to execute statements sequentially.</li> </ul>"},{"location":"DSD/dsd_old/#state-machine","title":"State Machine","text":"<p>A state machine is a computational model used to design both software and hardware systems. It consists of a set of states, transitions between states, and actions that occur based on inputs.</p>"},{"location":"DSD/dsd_old/#components","title":"Components","text":"<ol> <li>States: Defined conditions or situations the system can be in.</li> <li>Transitions: Conditions that trigger a change from one state to another.</li> <li>Inputs: External events or conditions that affect state transitions.</li> <li>Outputs: Actions or results produced during or after a state transition.</li> </ol>"},{"location":"DSD/dsd_old/#types-of-state-machines","title":"Types of State Machines","text":"<ol> <li>Finite State Machine (FSM): Has a finite number of states and transitions between them.</li> <li>Deterministic FSM (DFA): Every state has exactly one transition for each input.</li> <li> <p>Non-deterministic FSM (NFA): A state can have multiple transitions for the same input.</p> </li> <li> <p>Mealy Machine: The output depends on both the current state and the input.</p> </li> <li>Moore Machine: The output depends only on the current state.</li> </ol>"},{"location":"DSD/dsd_old/#applications","title":"Applications","text":"<ul> <li>Control Systems: Used in embedded systems for managing device behavior.</li> <li>Protocols: Helps in defining the sequence of operations in communication protocols.</li> <li>Game Design: To model different game states such as playing, paused, or game over.</li> </ul>"},{"location":"DSD/dsd_old/#example","title":"Example","text":"<pre><code>module fsm_example (\n    input wire clk,        // Clock input\n    input wire reset,      // Asynchronous reset signal (active high)\n    input wire trigger,    // Trigger input to transition between states\n    output reg out         // Output signal that depends on the current state\n);\n\n    // Define the states as an enumerated type using a 2-bit register\n    typedef enum reg [1:0] {\n        IDLE   = 2'b00,    // State 0: Idle state (default)\n        STATE1 = 2'b01,    // State 1: Represents the first active state\n        STATE2 = 2'b10     // State 2: Represents the second active state\n    } state_t;\n\n    // Current state and next state registers\n    reg state_t current_state, next_state;\n\n    // Sequential logic for state transition\n    // This block updates the current state on every clock edge or reset\n    always @(posedge clk or posedge reset) begin\n        if (reset) begin\n            current_state &lt;= IDLE;   // On reset, go to IDLE state\n        end else begin\n            current_state &lt;= next_state;  // On clock, update to the next state\n        end\n    end\n\n    // Combinational logic for state transition based on current state and input trigger\n    always @(*) begin\n        // Default next state and output values\n        next_state = current_state; // Hold the current state by default\n        out = 1'b0;                 // Default output is 0\n\n        // State transition logic\n        case (current_state)\n            IDLE: begin\n                // In IDLE, if 'trigger' is high, move to STATE1\n                if (trigger)\n                    next_state = STATE1;\n                // Output remains 0 in IDLE\n            end\n\n            STATE1: begin\n                // In STATE1, set output high\n                out = 1'b1;\n                // If 'trigger' is high, move to STATE2, otherwise go back to IDLE\n                if (trigger)\n                    next_state = STATE2;\n                else\n                    next_state = IDLE;\n            end\n\n            STATE2: begin\n                // In STATE2, output remains high\n                out = 1'b1;\n                // If 'trigger' goes low, return to IDLE\n                if (!trigger)\n                    next_state = IDLE;\n            end\n\n            default: begin\n                // Default case to handle any undefined state (shouldn't happen)\n                next_state = IDLE; // Return to IDLE state if something goes wrong\n            end\n        endcase\n    end\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#simulations","title":"Simulations","text":""},{"location":"DSD/dsd_old/#delays-in-verilog","title":"Delays in Verilog","text":"<p>Propagation delays in Verilog simulate the time it takes for signals to propagate through circuits.</p>"},{"location":"DSD/dsd_old/#delayed-assignments","title":"Delayed Assignments","text":"<p>In continuous assignments, use <code>#delay</code> to specify how long it takes for the output to update after a change in the inputs.</p> <pre><code> assign #5 y = a &amp; b;  // y updates 5 time units after a or b changes \n ```\n\n### Why Use Delays?\n\n- **Timing Simulation:** Models real-world signal delays.\n- **Accurate Behavior:** Ensures proper timing in combinational logic.\n\n### Delays in Procedural Assignments\n\nDelays can also be applied in **procedural blocks** like `always`:\n\n```verilog\n always @ (a or b) begin\n    #3 out = a &amp; b;  // out updates 3 time units after input changes\nend \n</code></pre>"},{"location":"DSD/dsd_old/#inertial-vs-transport-delays","title":"Inertial vs. Transport Delays","text":"<ul> <li>Inertial Delay: This is the default type of delay in Verilog. It filters out glitches, meaning only input pulses longer than the delay propagate to the output. </li> <li>Example: If <code>#5</code> is used, pulses shorter than 5 time units will be filtered.</li> </ul> <p><code>verilog   assign #5 y = a &amp; b;  // Inertial delay, y updates only if changes persist for 5 time units</code></p> <ul> <li>Transport Delay: This models a physical delay without filtering any input pulses. Even if an input changes rapidly, the signal is propagated to the output after the delay.</li> <li>Example: Use <code>transport</code> to force transport delay behavior.</li> </ul> <p><code>verilog   assign #5 y = transport a &amp; b;  // Transport delay, y updates exactly 5 time units after input change</code></p>"},{"location":"DSD/dsd_old/#key-differences","title":"Key Differences:","text":"<ul> <li>Inertial delay: Mimics real-world circuits where short glitches are filtered.</li> <li>Transport delay: Models pure signal propagation without glitch filtering.</li> </ul>"},{"location":"DSD/dsd_old/#wait-statements","title":"Wait Statements","text":"<p>In Verilog, the <code>wait</code> statement is used to pause the execution of a block until a certain condition becomes true. Unlike an <code>always</code> or <code>@(posedge clk)</code> block that waits for specific events (like clock edges), the <code>wait</code> statement halts the execution until the specified condition is met. Primarily used with delays to simulate hardware restrictions.</p>"},{"location":"DSD/dsd_old/#syntax","title":"Syntax:","text":"<pre><code>wait (condition) begin\n    // Code to execute after the condition is true\nend\n</code></pre>"},{"location":"DSD/dsd_old/#example_1","title":"Example:","text":"<pre><code>reg signal, result;\n\ninitial begin\n    result = 0;\n    signal = 0;\n    #10 signal = 1;  // Signal changes after 10 time units\n    wait (signal == 1) begin\n        result = 1;   // Result is updated after signal becomes high\n    end\nend\n</code></pre>"},{"location":"DSD/dsd_old/#applications_1","title":"Applications","text":""},{"location":"DSD/dsd_old/#4-bit-adder","title":"4-bit Adder","text":"<p>A 4-bit adder adds two 4-bit binary numbers together, producing a 4-bit sum output along with a carry-out bit that indicates an overflow if the sum exceeds the capacity of 4 bits.</p> <p></p>"},{"location":"DSD/dsd_old/#ripple-carry-adder-rca","title":"Ripple Carry Adder (RCA)","text":"<pre><code>module FullAdder (A, B, Ci, Co, S);\n\n    input A, B, Ci;    // Inputs: A, B, and Carry In (Ci)\n    output S, Co;      // Outputs: Sum (S) and Carry Out (Co)\n\n    assign S = A ^ B ^ Ci;      // Sum calculation\n    assign Co = (A &amp; B) | (B &amp; Ci) | (A &amp; Ci);  // Carry Out calculation\n\nendmodule\n\n\nmodule Adder4 (S, Co, A, B, Ci);\n\n    input [3:0] A, B;  // 4-bit inputs\n    input Ci;          // Carry input\n    output [3:0] S;    // 4-bit sum output\n    output Co;         // Carry out\n    wire [3:1] C;      // Internal carry signals\n\n    // Instantiate 4 Full Adders\n    FullAdder FA0 (A[0], B[0], Ci, C[1], S[0]);\n    FullAdder FA1 (A[1], B[1], C[1], C[2], S[1]);\n    FullAdder FA2 (A[2], B[2], C[2], C[3], S[2]);\n    FullAdder FA3 (A[3], B[3], C[3], Co, S[3]);\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#look-ahead-adder-cla","title":"Look-Ahead Adder (CLA)","text":"<p>A 4-bit carry look-ahead adder (CLA) is a faster alternative to the ripple carry adder, as it calculates the carries in parallel rather than waiting for each carry to propagate through the stages sequentially. The carry look-ahead adder computes the carry signals using the generate (G) and propagate (P) functions.</p> <pre><code>module cla_4bit (\n    input [3:0] A, B,\n    input Cin,\n    output [3:0] Sum,\n    output Cout\n);\n    wire [3:0] G, P;  // Generate and propagate signals\n    wire C1, C2, C3;\n\n    // Generate and propagate signals\n    assign G = A &amp; B;  // Generate\n    assign P = A | B;  // Propagate\n\n    // Carry look-ahead logic\n    assign #5 C1 = G[0] | (P[0] &amp; Cin);\n    assign #5 C2 = G[1] | (P[1] &amp; G[0]) | (P[1] &amp; P[0] &amp; Cin);\n    assign #5 C3 = G[2] | (P[2] &amp; G[1]) | (P[2] &amp; P[1] &amp; G[0]) | (P[2] &amp; P[1] &amp; P[0] &amp; Cin);\n    assign #5 Cout = G[3] | (P[3] &amp; G[2]) | (P[3] &amp; P[2] &amp; G[1]) | (P[3] &amp; P[2] &amp; P[1] &amp; G[0]) | (P[3] &amp; P[2] &amp; P[1] &amp; P[0] &amp; Cin);\n\n    // Sum calculation\n    assign #5 Sum[0] = A[0] ^ B[0] ^ Cin;\n    assign #5 Sum[1] = A[1] ^ B[1] ^ C1;\n    assign #5 Sum[2] = A[2] ^ B[2] ^ C2;\n    assign #5 Sum[3] = A[3] ^ B[3] ^ C3;\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#delay-comparison-carry-look-ahead-adder-cla-vs-ripple-carry-adder-rca","title":"Delay Comparison: Carry Look-Ahead Adder (CLA) vs. Ripple Carry Adder (RCA)","text":""},{"location":"DSD/dsd_old/#1-ripple-carry-adder-rca","title":"1. Ripple Carry Adder (RCA)","text":"<ul> <li>Delay Growth: Linear <code>O(N)</code> (increases with the number of bits)</li> <li>Total Delay: <code>N * Gate Delay per Full Adder</code></li> </ul>"},{"location":"DSD/dsd_old/#2-carry-look-ahead-adder-cla","title":"2. Carry Look-Ahead Adder (CLA)","text":"<ul> <li>Delay Growth: Logarithmic <code>O(log N)</code> (grows slower as the number of bits increases)</li> <li>Total Delay: <code>log_2(N) * Gate Delay per Stage</code></li> </ul>"},{"location":"DSD/dsd_old/#3-comparison-of-delays","title":"3. Comparison of Delays","text":"Number of Bits RCA Delay (Linear) CLA Delay (Logarithmic) 4 bits 20 ns 20 ns 8 bits 40 ns 15 ns 16 bits 80 ns 20 ns 32 bits 160 ns 25 ns 64 bits 320 ns 30 ns"},{"location":"DSD/dsd_old/#4-to-1-mux","title":"4-to-1 Mux","text":"<p>A 4-to-1 multiplexer selects one of four input signals based on a 2-bit selection input and forwards the selected input to the output.</p> <p></p> <pre><code>module Mux4to1 (\n    input wire I0, I1, I2, I3,  // 4 data inputs\n    input wire S0, S1,          // 2 select lines\n    output wire Y               // Output\n);\n\nassign Y = (~S1 &amp; ~S0 &amp; I0) |  // Select I0 when S1 = 0, S0 = 0\n           (~S1 &amp;  S0 &amp; I1) |  // Select I1 when S1 = 0, S0 = 1\n           ( S1 &amp; ~S0 &amp; I2) |  // Select I2 when S1 = 1, S0 = 0\n           ( S1 &amp;  S0 &amp; I3);   // Select I3 when S1 = 1, S0 = 1\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#shift-register","title":"Shift Register","text":""},{"location":"DSD/dsd_old/#4-bit-left-shift","title":"4-bit Left Shift","text":"<p>A 4-bit left shift register shifts the contents of the register one bit to the left on each clock cycle, with a new bit introduced at the least significant bit (LSB) and the most significant bit (MSB) being discarded.</p> <p></p> <pre><code>module LeftShiftRegister (\n    input wire clk,           // Clock input\n    input wire reset,         // Reset input\n    input wire D_in,          // Serial data input\n    output reg [3:0] Q        // 4-bit parallel output\n);\n\n// On the positive edge of the clock, perform the left shift\nalways @(posedge clk or posedge reset) begin\n    if (reset) begin\n        Q &lt;= 4'b0000;         // Reset all outputs to 0\n    end else begin\n        Q &lt;= {Q[2:0], D_in};  // Shift left and input new data bit on Q0\n    end\nend\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#8-bit-left-shift","title":"8-bit Left Shift","text":"<pre><code>module shift_register_8bit (\n    input wire SI,     // Serial input\n    input wire Clk,    // Clock signal (rising edge)\n    input wire EN,     // Enable (active high)\n    output reg SO      // Serial output\n);\n\n    // Internal 8-bit register to hold the shift data\n    reg [7:0] shift_reg;\n\n    // Always block triggered on rising edge of clock\n    always @(posedge Clk) begin\n        if (EN) begin\n            // Shift register: shift in SI and shift out MSB (SO)\n            SO &lt;= shift_reg[7];      // MSB is shifted out\n            shift_reg &lt;= {shift_reg[6:0], SI};  // Shift left and input SI\n        end\n    end\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#8-bit-counter-lab-3","title":"8-bit Counter (Lab 3)","text":"<p>Lab simulation: 3-Bit Up/Down counter with a clear and load.</p> <p>This lab involves designing an 8-bit up/down counter using Verilog, implemented on the NEXYS-4 FPGA board. A Vivado project is provided for this example.</p> <p>The CD74HC190/191 and CD54HC190/191 are highly versatile presettable up/down counters commonly used in digital electronics for tasks requiring accurate counting, such as digital clocks, timers, and frequency dividers. These ICs can be configured to count in either binary (CD74HC191) or BCD (Binary-Coded Decimal) format (CD74HC190), and support synchronous counting with a variety of control options, including asynchronous presetting, counting direction control, and enabling inputs. With additional outputs like ripple carry and terminal count, these counters are ideal for cascading multiple units to create larger counting systems.</p> <p>In Lab 3, we implemented an 8-bit up/down counter based on the functionality of the 74HC190/191 but directly in Verilog, bypassing the need for discrete hardware. This allowed us to overcome common design challenges such as signal ripple effects and cascading hazards that typically occur when connecting multiple 4-bit counters. By using Verilog, we streamlined the design, implementing advanced features such as parallel loading, which enables the counter to start from a user-defined value, and a clock control to slow down the counting process for easier observation.</p>"},{"location":"DSD/dsd_old/#identifying-required-number-of-bits","title":"Identifying Required Number of bits","text":"<p>To determine how many bits are needed to count to a specific number \\(N\\), use the following steps:</p> <ol> <li>Use the formula: <code>n = log2(N)</code> where <code>n</code> is the number of bits.</li> <li>Round up the result to the nearest whole number.</li> <li>A counter with <code>n</code> bits can represent numbers from <code>0</code> to <code>2^n - 1</code>, which must be greater than or equal to \\(N\\).</li> </ol>"},{"location":"DSD/dsd_old/#example_2","title":"Example:","text":"<p>To count up to 100:</p> <ol> <li><code>n = log2(100)</code> \u2248 6.64</li> <li>Round up to 7.</li> </ol> <p>You would need 7 bits to count up to 100, as 7 bits can represent values from 0 to 127.</p>"},{"location":"DSD/dsd_old/#parallel-loading","title":"Parallel Loading","text":"<p>Parallel loading allows a counter to load a preset value into all its flip-flops simultaneously in a single clock cycle. By using input lines (like A-D or D0-D3) and a load signal, the counter can instantly update its value upon the next clock cycle, bypassing the need for sequential counting. This is particularly useful for initializing the counter or synchronizing multiple counters efficiently. In this lab, we used parallel loading to preset the 8-bit counter with values determined by the input switches, adding greater flexibility to the counting process.</p>"},{"location":"DSD/dsd_old/#synchronous-4-bit-updown-counter-74hc192","title":"Synchronous 4-bit Up/Down Counter (74HC192)","text":"<pre><code>module counter_74HC192 (\n    input wire Clr,      // Clear the counter\n    input wire Load,     // Load the counter with a value\n    input wire Up,       // Count up when asserted\n    input wire Down,     // Count down when asserted\n    input wire [3:0] P,  // Data input for loading preset\n    input wire Clk,      // Clock input\n    output reg [3:0] Q   // 4-bit counter output\n);\n\n// Always block, sensitive to Clk, Clr, Load, Up, Down\nalways @(posedge Clk or posedge Clr or posedge Load) begin\n    if (Clr) begin\n        Q &lt;= 4'b0000;  // Clear the counter\n    end\n    else if (Load) begin\n        Q &lt;= D;  // Load the counter with input value\n    end\n    else if (Up &amp;&amp; !Down) begin\n        Q &lt;= Q + 1;  // Increment counter\n    end\n    else if (Down &amp;&amp; !Up) begin\n        Q &lt;= Q - 1;  // Decrement counter\n    end\nend\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#sr-latch","title":"SR Latch","text":""},{"location":"DSD/dsd_old/#sr-latch-using-a-boolean-equation","title":"SR Latch Using a Boolean Equation","text":"<pre><code>module sr_latch_boolean (\n    input wire S,   // Set input\n    input wire R,   // Reset input\n    output reg Q,   // Output\n    output reg Qn   // Inverted output\n);\n\n    always @(*) begin\n        if (S &amp;&amp; !R) begin\n            Q &lt;= 1;   // Set\n            Qn &lt;= 0;\n        end\n        else if (!S &amp;&amp; R) begin\n            Q &lt;= 0;   // Reset\n            Qn &lt;= 1;\n        end\n        // If S == 0 and R == 0, Q and Qn retain previous state (no action)\n    end\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#sr-latch-using-gate-level-description","title":"SR Latch Using Gate-Level Description","text":"<pre><code>module sr_latch_gate_level (\n    input wire S,   // Set input\n    input wire R,   // Reset input\n    output wire Q,  // Output\n    output wire Qn  // Inverted output\n);\n\n    wire nand1_out, nand2_out;\n\n    // NAND gates for SR Latch\n    nand (nand1_out, S, Qn);  // First NAND gate\n    nand (nand2_out, R, Q);   // Second NAND gate\n\n    assign Q = nand1_out;\n    assign Qn = nand2_out;\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#4-bit-comparator","title":"4-bit Comparator","text":"<p>A 4-bit comparator is a digital circuit that compares two 4-bit binary numbers and outputs the result of their comparison. It determines whether one number is greater than, less than, or equal to the other.</p>"},{"location":"DSD/dsd_old/#inputs","title":"Inputs","text":"<ul> <li>A[3:0]: 4-bit binary number (A3, A2, A1, A0)</li> <li>B[3:0]: 4-bit binary number (B3, B2, B1, B0)</li> </ul>"},{"location":"DSD/dsd_old/#outputs","title":"Outputs","text":"<ul> <li>A &gt; B: Output is 1 if the binary number A is greater than B.</li> <li>A &lt; B: Output is 1 if the binary number A is less than B.</li> <li>A = B: Output is 1 if the binary number A is equal to B.</li> </ul>"},{"location":"DSD/dsd_old/#logic-for-comparison","title":"Logic for Comparison","text":"<p>To compare two 4-bit numbers, the comparison starts from the most significant bit (MSB) down to the least significant bit (LSB):</p> <ol> <li>A &gt; B:</li> <li>If A3 &gt; B3, then A &gt; B regardless of the lower bits.</li> <li>If A3 = B3, move to compare A2 and B2.</li> <li> <p>Repeat for all bits until a difference is found or A = B.</p> </li> <li> <p>A &lt; B:</p> </li> <li>If A3 &lt; B3, then A &lt; B.</li> <li>If A3 = B3, move to compare A2 and B2.</li> <li> <p>Continue this until a difference is found or A = B.</p> </li> <li> <p>A = B:</p> </li> <li>If all bits (A3 to A0) are equal to their corresponding B bits, then A = B.</li> </ol>"},{"location":"DSD/dsd_old/#truth-table","title":"Truth Table","text":"A3 A2 A1 A0 B3 B2 B1 B0 A &gt; B A &lt; B A = B 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 ... ... ... ... ... ... ... ... ... ... ..."},{"location":"DSD/dsd_old/#verilog-implementation","title":"Verilog Implementation","text":"<p>A simple implementation in Verilog:</p> <pre><code>module Comparator4bit(\n    input [3:0] A, B,\n    output A_greater, A_less, A_equal\n);\n\nassign A_greater = (A &gt; B);\nassign A_less = (A &lt; B);\nassign A_equal = (A == B);\n\nendmodule\n</code></pre>"},{"location":"DSD/dsd_old/#nexys-a7","title":"Nexys A7","text":"<p>Nexys A7 Reference Manual</p> <p></p>"},{"location":"DSD/dsd_old/#to-do","title":"To do","text":"<ol> <li>Bypass capacitors required in FPGAs</li> <li>Division, except by powers of 2 is not supported in Vivado</li> <li>If you use an if statement, but don\u2019t have an else, the synthesizer may create a latch you did not expect.</li> <li>Synthesis vs implementation</li> </ol>"},{"location":"DSD/logicDesign/","title":"Logic Design","text":""},{"location":"DSD/logicDesign/#boolean-algebra","title":"Boolean Algebra","text":"Rule Expression Hint Identity Law A + 0 = A No change when OR'ed with 0 Identity Law A \u2022 1 = A No change when AND'ed with 1 Null Law A + 1 = 1 OR'ing with 1 results in 1 Null Law A \u2022 0 = 0 AND'ing with 0 results in 0 Complement Law A + A' = 1 A variable OR'ed with its complement is 1 Complement Law A \u2022 A' = 0 A variable AND'ed with its complement is 0 Idempotent Law A + A = A OR'ing a variable with itself leaves it unchanged Idempotent Law A \u2022 A = A AND'ing a variable with itself leaves it unchanged Domination Law A + A'B = A + B Apply the Distributive Law to simplify Distributive Law A(B + C) = AB + AC Distributes AND over OR Distributive Law A + BC = (A + B)(A + C) Distributes OR over AND Absorption Law A + AB = A Removes redundant terms Absorption Law A(A + B) = A Removes redundant terms Double Negation Law (A')' = A Negation of a negation returns the original value De Morgan\u2019s Law (A \u2022 B)' = A' + B' Apply to break AND terms when converting SOP to POS De Morgan\u2019s Law (A + B)' = A' \u2022 B' Apply to break OR terms when converting POS to SOP Involution Law (A'') = A A variable twice negated is equal to itself Consensus Theorem AB + A'C + BC = AB + A'C Simplifies expressions by eliminating redundant terms Distributive (SOP to POS hint) A + BC = (A + B)(A + C) Useful for converting SOP to POS Distributive (POS to SOP hint) A(B + C) = AB + AC Useful for converting POS to SOP Demorgans (SOP to POS hint) (A \u2022 B)' = A' + B' Apply De Morgan\u2019s Law during the conversion Demorgans (POS to SOP hint) (A + B)' = A' \u2022 B' Apply De Morgan\u2019s Law during the conversion Redundancy Law AB + AB' = A Removes redundant variables from the equation"},{"location":"DSD/logicDesign/#sequential-and-combinational-assignments","title":"Sequential and Combinational Assignments","text":"<ul> <li>Sequential = Procedural: Sequential logic updates state based on clock edges and uses procedural assignments, typically with non-blocking (<code>&lt;=</code>) assignments. It is modeled inside <code>always</code> blocks that are sensitive to clock edges (e.g., <code>posedge clk</code>). This describes systems like flip-flops or registers that rely on previous states.</li> </ul> <p>Example: <code>verilog   always @(posedge clk or posedge reset) begin       if (reset)           q &lt;= 0;      // Asynchronous reset       else           q &lt;= d;      // Update q with d at clock edge   end</code></p> <ul> <li> <p>Combinational = Continuous: Combinational logic depends purely on the current inputs and is described using continuous (blocking) assignments. It models circuits like AND, OR gates, where output is updated as soon as inputs change, without regard to clock cycles.</p> <p>Example: <code>verilog assign y = a &amp; b;  // Output y changes immediately based on a and b</code></p> </li> </ul>"},{"location":"DSD/logicDesign/#relevant-constructs","title":"Relevant Constructs","text":"<ul> <li> <p><code>always</code> blocks: Used to describe both sequential and combinational logic. For sequential logic, <code>always @(posedge clk)</code> or <code>always @(negedge clk)</code> is used, whereas for combinational logic, <code>always @(*)</code> is used to capture all input changes automatically.     Example: <code>verilog     always @(*) begin         result = a | b;  // Combinational logic triggered by any input change     end</code></p> </li> <li> <p><code>initial</code> blocks: Used to define initial conditions in simulations. They execute once at the start of the simulation and are commonly used for testbenches or initializing registers/variables in simulation but are not synthesized into hardware.     Example <code>verilog     initial begin         reg_x = 0;  // Initialize reg_x to 0 at simulation start     end</code></p> </li> </ul>"},{"location":"DSD/logicDesign/#key-points","title":"Key Points","text":"<ul> <li>Blocking (<code>=</code>) vs. Non-blocking (<code>&lt;=</code>): In sequential logic (<code>always @(posedge clk)</code>), use non-blocking assignments (<code>&lt;=</code>) to ensure parallel updates. In combinational logic, blocking assignments (<code>=</code>) can be used to execute statements sequentially.</li> </ul>"},{"location":"DSD/logicDesign/#state-machine","title":"State Machine","text":"<p>A state machine is a computational model used to design both software and hardware systems. It consists of a set of states, transitions between states, and actions that occur based on inputs.</p>"},{"location":"DSD/logicDesign/#components","title":"Components","text":"<ol> <li>States: Defined conditions or situations the system can be in.</li> <li>Transitions: Conditions that trigger a change from one state to another.</li> <li>Inputs: External events or conditions that affect state transitions.</li> <li>Outputs: Actions or results produced during or after a state transition.</li> </ol>"},{"location":"DSD/logicDesign/#types-of-state-machines","title":"Types of State Machines","text":"<ol> <li>Finite State Machine (FSM): Has a finite number of states and transitions between them.</li> <li>Deterministic FSM (DFA): Every state has exactly one transition for each input.</li> <li> <p>Non-deterministic FSM (NFA): A state can have multiple transitions for the same input.</p> </li> <li> <p>Mealy Machine: The output depends on both the current state and the input.</p> </li> <li>Moore Machine: The output depends only on the current state.</li> </ol>"},{"location":"DSD/logicDesign/#applications","title":"Applications","text":"<ul> <li>Control Systems: Used in embedded systems for managing device behavior.</li> <li>Protocols: Helps in defining the sequence of operations in communication protocols.</li> <li>Game Design: To model different game states such as playing, paused, or game over.</li> </ul>"},{"location":"DSD/logicDesign/#example","title":"Example","text":"<pre><code>module fsm_example (\n    input wire clk,        // Clock input\n    input wire reset,      // Asynchronous reset signal (active high)\n    input wire trigger,    // Trigger input to transition between states\n    output reg out         // Output signal that depends on the current state\n);\n\n    // Define the states as an enumerated type using a 2-bit register\n    typedef enum reg [1:0] {\n        IDLE   = 2'b00,    // State 0: Idle state (default)\n        STATE1 = 2'b01,    // State 1: Represents the first active state\n        STATE2 = 2'b10     // State 2: Represents the second active state\n    } state_t;\n\n    // Current state and next state registers\n    reg state_t current_state, next_state;\n\n    // Sequential logic for state transition\n    // This block updates the current state on every clock edge or reset\n    always @(posedge clk or posedge reset) begin\n        if (reset) begin\n            current_state &lt;= IDLE;   // On reset, go to IDLE state\n        end else begin\n            current_state &lt;= next_state;  // On clock, update to the next state\n        end\n    end\n\n    // Combinational logic for state transition based on current state and input trigger\n    always @(*) begin\n        // Default next state and output values\n        next_state = current_state; // Hold the current state by default\n        out = 1'b0;                 // Default output is 0\n\n        // State transition logic\n        case (current_state)\n            IDLE: begin\n                // In IDLE, if 'trigger' is high, move to STATE1\n                if (trigger)\n                    next_state = STATE1;\n                // Output remains 0 in IDLE\n            end\n\n            STATE1: begin\n                // In STATE1, set output high\n                out = 1'b1;\n                // If 'trigger' is high, move to STATE2, otherwise go back to IDLE\n                if (trigger)\n                    next_state = STATE2;\n                else\n                    next_state = IDLE;\n            end\n\n            STATE2: begin\n                // In STATE2, output remains high\n                out = 1'b1;\n                // If 'trigger' goes low, return to IDLE\n                if (!trigger)\n                    next_state = IDLE;\n            end\n\n            default: begin\n                // Default case to handle any undefined state (shouldn't happen)\n                next_state = IDLE; // Return to IDLE state if something goes wrong\n            end\n        endcase\n    end\n\nendmodule\n</code></pre>"},{"location":"DSD/simulations/","title":"Simulations","text":""},{"location":"DSD/simulations/#delays-in-verilog","title":"Delays in Verilog","text":"<p>Propagation delays in Verilog simulate the time it takes for signals to propagate through circuits.</p>"},{"location":"DSD/simulations/#delayed-assignments","title":"Delayed Assignments","text":"<p>In continuous assignments, use <code>#delay</code> to specify how long it takes for the output to update after a change in the inputs.</p> <pre><code> assign #5 y = a &amp; b;  // y updates 5 time units after a or b changes \n ```\n\n### Why Use Delays?\n\n- **Timing Simulation:** Models real-world signal delays.\n- **Accurate Behavior:** Ensures proper timing in combinational logic.\n\n### Delays in Procedural Assignments\n\nDelays can also be applied in **procedural blocks** like `always`:\n\n```verilog\n always @ (a or b) begin\n    #3 out = a &amp; b;  // out updates 3 time units after input changes\nend \n</code></pre>"},{"location":"DSD/simulations/#inertial-vs-transport-delays","title":"Inertial vs. Transport Delays","text":"<ul> <li>Inertial Delay: This is the default type of delay in Verilog. It filters out glitches, meaning only input pulses longer than the delay propagate to the output. </li> <li>Example: If <code>#5</code> is used, pulses shorter than 5 time units will be filtered.</li> </ul> <p><code>verilog   assign #5 y = a &amp; b;  // Inertial delay, y updates only if changes persist for 5 time units</code></p> <ul> <li>Transport Delay: This models a physical delay without filtering any input pulses. Even if an input changes rapidly, the signal is propagated to the output after the delay.</li> <li>Example: Use <code>transport</code> to force transport delay behavior.</li> </ul> <p><code>verilog   assign #5 y = transport a &amp; b;  // Transport delay, y updates exactly 5 time units after input change</code></p>"},{"location":"DSD/simulations/#key-differences","title":"Key Differences:","text":"<ul> <li>Inertial delay: Mimics real-world circuits where short glitches are filtered.</li> <li>Transport delay: Models pure signal propagation without glitch filtering.</li> </ul>"},{"location":"DSD/simulations/#wait-statements","title":"Wait Statements","text":"<p>In Verilog, the <code>wait</code> statement is used to pause the execution of a block until a certain condition becomes true. Unlike an <code>always</code> or <code>@(posedge clk)</code> block that waits for specific events (like clock edges), the <code>wait</code> statement halts the execution until the specified condition is met. Primarily used with delays to simulate hardware restrictions.</p>"},{"location":"DSD/simulations/#syntax","title":"Syntax:","text":"<pre><code>wait (condition) begin\n    // Code to execute after the condition is true\nend\n</code></pre>"},{"location":"DSD/simulations/#example","title":"Example:","text":"<pre><code>reg signal, result;\n\ninitial begin\n    result = 0;\n    signal = 0;\n    #10 signal = 1;  // Signal changes after 10 time units\n    wait (signal == 1) begin\n        result = 1;   // Result is updated after signal becomes high\n    end\nend\n</code></pre>"},{"location":"DSD/verilog/","title":"Verilog","text":""},{"location":"DSD/verilog/#data-types","title":"Data Types","text":"Data Type Purpose Characteristics <code>wire</code> Combinational logic and connections Cannot hold state, used in continuous assignments <code>reg</code> Sequential and combinational logic Holds state, used inside <code>always</code> blocks <code>integer</code> Signed 32-bit value for loops/counters Used in loops and non-synthesizable code <code>real</code> Floating-point value Used in non-synthesizable, behavioral code <code>time</code> 64-bit value to represent simulation time Used for timing and measuring delays in simulation <code>tri</code> Tri-state buffer signal Can take high-impedance (<code>Z</code>) values"},{"location":"DSD/verilog/#vectors-in-verilog","title":"Vectors in Verilog","text":"<p>Vectors in Verilog are used to represent multi-bit signals, which are crucial when dealing with buses, registers, or large numbers. They allow for grouping multiple bits into a single variable.</p> <p>A vector is declared by specifying the range of bits using <code>[MSB:LSB]</code>, where MSB is the most significant bit and LSB is the least significant bit.</p> <pre><code>wire [3:0] bus;   // 4-bit wide wire (vector)\nreg  [7:0] data;  // 8-bit register\n</code></pre> <p>Individual bits or a range of bits within a vector can be accessed as follows:</p> <pre><code>wire [7:0] data;\nassign bit3 = data[3];     // Accessing the 3rd bit of data\nassign lower_nibble = data[3:0];  // Accessing the lower 4 bits of data\n</code></pre> <p>You can assign values directly to vectors:</p> <pre><code>reg [3:0] result;\nresult = 4'b1010;  // Assigning binary value\nresult = 4'hA;     // Assigning hexadecimal value\n</code></pre> <p>A vector can have zero width when the MSB and LSB are the same, meaning it's a single-bit signal:</p> <pre><code>wire [0:0] single_bit;  // Equivalent to a scalar\n</code></pre> <p>By default, vectors are unsigned, but they can be declared as signed if needed:</p> <pre><code>signed reg [7:0] signed_data;  // Signed 8-bit register\n</code></pre> <p>In signed vectors, the most significant bit (MSB) is treated as the sign bit.</p>"},{"location":"DSD/verilog/#primitives","title":"Primitives","text":"<p>A Verilog primitive is a pre-defined logic element used in digital designs. These include basic gates like <code>and</code>, <code>or</code>, <code>nand</code>, and <code>xor</code>, with fixed functions that don't require module definitions.</p> <p>Output Declaration: In Verilog, when declaring a UDP, the output must always be listed first, followed by the input(s). This order is essential for the proper functioning of the UDP.</p> <p>Types of Primitives: - Combinational Primitives: e.g., <code>and</code>, <code>or</code>, <code>xor</code> - Sequential Primitives: Flip-flops, latches</p>"},{"location":"DSD/verilog/#user-defined-primitives-udps","title":"User Defined Primitives (UDPs)","text":"<p>UDPs are custom-defined logic, either combinational or sequential, declared using the <code>primitive</code> keyword. They use a truth table to define behavior.</p> <ul> <li>Example (Combinational UDP):     ```verilog<pre><code>primitive my_and (out, in1, in2);  \noutput out;  \ninput in1, in2;\n\n// The 'table' defines the behavior of this custom primitive.\n// Each row in the table specifies input combinations and the corresponding output.\n// For an AND gate, the output is 1 only when both inputs are 1.\ntable\n    0 0 : 0;  // If both inputs are 0, the output is 0.\n    1 1 : 1;  // If both inputs are 1, the output is 1.\n    // For simplicity, intermediate input states (like 0 1 or 1 0) are not explicitly defined here,\n    // but in a full AND gate implementation, these would typically output 0.\nendtable\n\nendprimitive\n</code></pre> <p>```</p> </li> </ul>"},{"location":"DSD/verilog/#verilog-operator-precedence","title":"Verilog Operator Precedence","text":"<p>In Verilog, operators follow a specific order of precedence. This determines how expressions are evaluated when there are multiple operators in the same expression. Below is the list of operators in order of precedence, from highest to lowest:</p> <ol> <li>Unary operators</li> <li><code>+</code>, <code>-</code> (unary plus and minus)</li> <li><code>!</code> (logical NOT)</li> <li><code>~</code> (bitwise NOT)</li> <li><code>&amp;</code>, <code>~&amp;</code> (reduction AND, NAND)</li> <li><code>|</code>, <code>~|</code> (reduction OR, NOR)</li> <li> <p><code>^</code>, <code>~^</code>, <code>^~</code> (reduction XOR, XNOR)</p> </li> <li> <p>Multiplicative operators</p> </li> <li> <p><code>*</code>, <code>/</code>, <code>%</code> (multiply, divide, modulus)</p> </li> <li> <p>Additive operators</p> </li> <li> <p><code>+</code>, <code>-</code> (addition, subtraction)</p> </li> <li> <p>Shift operators</p> </li> <li> <p><code>&lt;&lt;</code>, <code>&gt;&gt;</code> (logical shift left, right)</p> </li> <li> <p>Relational operators</p> </li> <li> <p><code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code> (less than, less than or equal, greater than, greater than or equal)</p> </li> <li> <p>Equality operators</p> </li> <li><code>==</code>, <code>!=</code> (logical equality, inequality)</li> <li> <p><code>===</code>, <code>!==</code> (case equality, case inequality)</p> </li> <li> <p>Bitwise operators</p> </li> <li> <p><code>&amp;</code>, <code>|</code>, <code>^</code>, <code>^~</code>, <code>~^</code> (AND, OR, XOR, XNOR)</p> </li> <li> <p>Logical operators</p> </li> <li><code>&amp;&amp;</code> (logical AND)</li> <li> <p><code>||</code> (logical OR)</p> </li> <li> <p>Conditional operator</p> </li> <li> <p><code>? :</code> (ternary operator)</p> </li> <li> <p>Assignment operators</p> <ul> <li><code>=</code>, <code>+=</code>, <code>-=</code>, <code>*=</code>, <code>/=</code>, <code>%=</code>, <code>&lt;&lt;=</code>, <code>&gt;&gt;=</code>, <code>&amp;=</code>, <code>|=</code>, <code>^=</code> (assignment and compound assignments)</li> </ul> </li> </ol>"},{"location":"DSD/verilog/#shifts","title":"Shifts","text":"<p>In Verilog, shift operations move bits of a value to the left or right. There are two types: logical shifts and arithmetic shifts.</p>"},{"location":"DSD/verilog/#1-logical-shifts","title":"1. Logical Shifts","text":"<p>Logical shifts move bits and fill the vacated positions with zeros.</p> <p>a. Logical Left Shift (<code>&lt;&lt;</code>) Shifts bits to the left by the specified amount, inserting zeros on the right. This is equivalent to multiplying by a power of 2.</p> <p>Syntax:</p> <pre><code>result = value &lt;&lt; shift_amount;\n</code></pre> <p>Example:</p> <pre><code>wire [3:0] value = 4'b1010; \nassign result = value &lt;&lt; 1;  // Result: 0100\n</code></pre> <p>b. Logical Right Shift (<code>&gt;&gt;</code>) Shifts bits to the right, inserting zeros on the left.</p> <p>Syntax:</p> <pre><code>result = value &gt;&gt; shift_amount;\n</code></pre> <p>Example:</p> <pre><code>wire [3:0] value = 4'b1010;\nassign result = value &gt;&gt; 1;  // Result: 0101\n</code></pre>"},{"location":"DSD/verilog/#2-arithmetic-shifts","title":"2. Arithmetic Shifts","text":"<p>Arithmetic shifts preserve the sign of signed numbers when shifting right.</p> <p>a. Arithmetic Right Shift (<code>&gt;&gt;&gt;</code>) Shifts bits to the right, preserving the sign by filling the leftmost bits with the sign bit (MSB).</p> <p>Syntax:</p> <pre><code>result = value &gt;&gt;&gt; shift_amount;\n</code></pre> <p>Example:</p> <pre><code>wire signed [3:0] value = -4;  // Binary: 1100 (two's complement)\nassign result = value &gt;&gt;&gt; 1;   // Result: 1110\n</code></pre>"},{"location":"DSD/verilog/#functions-and-tasks","title":"Functions and Tasks","text":"Aspect Functions Tasks Return Type Returns a single value. Can return multiple values via <code>output</code> ports. Time Control No timing control (<code>#</code>, <code>@</code>, <code>wait</code> not allowed). Supports timing control (can use <code>#</code>, <code>@</code>, <code>wait</code>). Arguments Only <code>input</code> arguments. Can have <code>input</code>, <code>output</code>, and <code>inout</code> arguments. Usage Used in expressions directly. Called as a separate statement. Execution Time Executes in zero simulation time. Takes simulation time to execute. Use Case Simple, combinational calculations. Complex tasks with delays or multiple outputs."},{"location":"DSD/verilog/#example-function","title":"Example Function:","text":"<pre><code>function [3:0] add;\n  input [3:0] a, b;\n  begin\n    add = a + b;\n  end\nendfunction\n</code></pre>"},{"location":"DSD/verilog/#example-task","title":"Example Task:","text":"<pre><code>task add_sub;\n  input [3:0] a, b;\n  output [3:0] sum, diff;\n  begin\n    sum = a + b;\n    diff = a - b;\n  end\nendtask\n</code></pre>"},{"location":"DSD/verilog/#example-task-with-event-control","title":"Example Task (With Event Control):","text":"<pre><code>task event_control_example;\n  input [3:0] a, b;\n  output reg [3:0] result;\n  begin\n    wait (a == b);   // Wait until a equals b\n    result = a + b;  // Perform addition\n  end\nendtask\n</code></pre>"},{"location":"DSD/verilog/#example-task-with-output-and-inout-arguments-called-as-a-separate-statement","title":"Example Task (with <code>output</code> and <code>inout</code> Arguments Called as a Separate Statement):","text":"<pre><code>module task_example;\n  reg [3:0] x, y, z, sum_result, diff_result;\n\n  initial begin\n    x = 4'b1010;              // Assign some values\n    y = 4'b0110;\n    z = 4'b0011;\n\n    // Call the task as a separate statement\n    arithmetic_operations(x, y, sum_result, z);  // `z` will be modified in-place as an inout\n  end\nendmodule\n\ntask arithmetic_operations;\n  input [3:0] a, b;           // Input arguments\n  output reg [3:0] sum;       // Output argument\n  inout [3:0] diff;           // Inout argument\n  begin\n    sum = a + b;              // Perform addition\n    diff = diff - a;          // Modify the inout value\n  end\nendtask\n</code></pre>"},{"location":"Physics/pages/","title":"Overview","text":""},{"location":"Physics/pages/#quanta-and-fields","title":"Quanta and Fields","text":"<p>This is a collection of notes I've taken while reading Quanta and Fields by Sean Carroll, much of the details are expanded on using other resources but the general framework was derived from Sean Carroll.</p> <ol> <li>The Wave Function</li> <li>Measurement</li> <li>Entanglement</li> <li>Fields</li> </ol>"},{"location":"Physics/pages/entanglement/","title":"Entanglement","text":"<p>From Sean Carroll's \"Quantum and Fields\"</p>"},{"location":"Physics/pages/entanglement/#key-concepts","title":"Key Concepts","text":""},{"location":"Physics/pages/entanglement/#1-particle-decay-and-momentum","title":"1. Particle Decay and Momentum","text":"<p>In quantum mechanics, particle decay is described by the wavefunction:</p> \\[|\\psi(t)\\rangle = \\alpha(t) |\\text{undecayed}\\rangle + \\beta(t) |\\text{decayed}\\rangle\\] <p>Here, \\(\\alpha(t)\\) represents the amplitude for the particle to remain undecayed, and \\(\\beta(t)\\) represents the amplitude for the particle to decay. The normalization condition ensures: $$ |\\alpha(t)|^2 + |\\beta(t)|^2 = 1 $$</p> <p>Over time, the probability of the particle remaining undecayed decreases, and the probability of decay increases. The survival probability at time \\(t\\) is \\(P_{\\text{undecayed}}(t) = |\\alpha(t)|^2\\), and the probability of the particle decaying is \\(P_{\\text{decayed}}(t) = 1 - |\\alpha(t)|^2\\).</p> <p>Momentum conservation plays a crucial role in particle decay. The total momentum before and after decay remains the same: $$ \\vec{p}{\\text{original}} = \\vec{p}{1} + \\vec{p}{2} $$ where \\(\\vec{p}_{\\text{original}}\\) is the momentum of the original particle, and \\(\\vec{p}_{1}\\) and \\(\\vec{p}_{2}\\) are the momenta of the decay products. This allows us to calculate the momentum of one decay product without directly observing it: $$ \\vec{p}{2} = \\vec{p}{\\text{original}} - \\vec{p}{1} $$</p> <p>Example: Decaying Boson Consider a stationary boson that decays into an electron-positron pair. Since the original boson has no initial momentum (\\(\\vec{p}_{\\text{original}} = 0\\)), the momentum of the electron and positron must be equal and opposite to conserve momentum: $$ \\vec{p}{\\text{positron}} = -\\vec{p}{\\text{electron}} $$ If we know the position of the original boson before decay, we can measure the momentum of one of the decay products (e.g., the positron) and use momentum conservation to determine the momentum of the other particle, even without directly measuring it. This is useful in particle physics experiments, where some particles (like neutrinos) are hard to detect directly. By measuring other decay products, we can infer properties of the undetected particles.</p>"},{"location":"Physics/pages/entanglement/#2-entanglement-one-state-many-parts","title":"2. Entanglement: One State, Many Parts","text":"<p>Entanglement describes a quantum phenomenon where the states of two or more particles become deeply interconnected. Unlike classical systems, where particles are independent, entangled particles share a combined quantum state. This means that measuring one particle\u2019s state instantly determines the state of the other, no matter the distance between them.</p> <p>The entangled state is typically written as: $$ |\\psi\\rangle = \\frac{1}{\\sqrt{2}} \\left( |\\uparrow\\rangle_1 |\\downarrow\\rangle_2 - |\\downarrow\\rangle_1 |\\uparrow\\rangle_2 \\right) $$ Here, \\(|\\uparrow\\rangle_1\\) and \\(|\\downarrow\\rangle_2\\) represent the spin states of two entangled particles. Measuring the spin of one particle gives immediate knowledge of the spin of the other.</p> <p>Building on the previous example of particle decay, we see how momentum conservation connects the decay products. Similarly, entangled particles are not independent; they are connected in such a way that measuring one particle gives us information about the other, even if they are far apart.</p> <p>In quantum mechanics, entanglement goes beyond momentum conservation. It links the properties of particles in a non-local way, forming a single quantum system that cannot be separated into individual parts. This phenomenon is fundamental to quantum technologies like quantum cryptography and quantum teleportation, which exploit these correlations to achieve tasks impossible with classical systems.</p>"},{"location":"Physics/pages/entanglement/#3-spooky-action-at-a-distance-and-the-epr-puzzle","title":"3. Spooky Action at a Distance and the EPR Puzzle","text":"<p>The term \"spooky action at a distance\" was coined by Einstein to criticize the non-local nature of quantum entanglement. It describes how entangled particles exhibit correlations that seem to violate the principle of locality, which states that objects are only directly influenced by their immediate surroundings.</p> <p>Key Features</p> <ul> <li>Entangled particles, even when separated by vast distances, are correlated in such a way that measuring one particle\u2019s state (e.g., its spin or polarization) immediately determines the state of the other. This happens instantaneously, seemingly defying the constraints of space and time.</li> <li>Importantly, this \"spooky action\" does not allow faster-than-light communication, thus preserving causality in quantum mechanics. While the information is transmitted instantaneously, it cannot be used to send information faster than light, which prevents paradoxes.  <ul> <li>For example, if Bob and Alice have an entangled pair and travel great distances from each other, when Bob measures his particle, he will immediately know the state of Alice\u2019s particle. However, Alice, unaware of Bob's measurement, still has the same odds of measurement outcomes as if Bob never measured his particle. The act of measuring Bob's particle doesn't change the fact that Alice's measurement will still have a probabilistic outcome based on her own measurements. Making the instantaneous information useless for communication between observers.</li> </ul> </li> </ul> <p>The EPR Puzzle</p> <p>Proposed by Einstein, Podolsky, and Rosen (EPR) in 1935, the EPR thought experiment challenged the completeness of quantum mechanics. EPR argued that if quantum mechanics were complete, it would have to explain physical reality through either:</p> <ol> <li>Local Realism: Particles have pre-existing properties (hidden variables) that determine their behavior before measurement.</li> <li>Non-Locality: The measurement of one particle instantaneously affects the state of another, regardless of the distance between them.</li> </ol> <p>EPR believed that non-locality was problematic because it implies that quantum mechanics allows for instantaneous influences between distant particles, which violates classical ideas of locality and causality. This made quantum mechanics seem incomplete because it relied on non-local effects without offering a fully deterministic explanation.</p> <p>Quantum mechanics rejects local realism and embraces non-locality through entanglement, where the state of entangled particles is not determined until one is measured, and the measurement of one affects the state of the other instantly.</p> <p>The EPR paradox led to experiments testing Bell's Theorem, which showed that quantum mechanics\u2019 predictions cannot be reproduced by any theory based on local hidden variables, confirming that non-locality is an intrinsic feature of quantum mechanics.</p>"},{"location":"Physics/pages/entanglement/#4-decoherence-theory-the-quantum-classical-divide","title":"4. Decoherence Theory: The Quantum-Classical Divide","text":"<p>Decoherence Theory argues the wave function never truly collapses. Instead, the appearance of collapse happens because the quantum system becomes entangled with its environment, making certain quantum behaviors (like interference) inaccessible to us as observers.</p> <p>Key Concepts</p> <ul> <li> <p>Interaction with the Environment:   Decoherence occurs when a quantum system interacts with its environment, such as measurement devices, air molecules, or photons. These interactions cause the quantum system to become entangled with the environment, making the superpositions of quantum states effectively unobservable.</p> </li> <li> <p>Entanglement with the Measurement Apparatus:   In quantum mechanics, when a measurement is made, the quantum system becomes entangled with the measuring device. This means that the state of the system and the state of the apparatus are linked. The act of measurement doesn't just collapse the system's state but causes both the system and the apparatus to evolve into a combined state.</p> </li> <li> <p>Why Big Things Appear Classical:   Macroscopic systems interact with their environment on a massive scale, causing rapid and irreversible decoherence. The entanglement between the system and its surroundings destroys quantum coherence, making superpositions unobservable and the system behave as though it has a single classical state.</p> </li> <li> <p>Loss of Superposition:   As the quantum system entangles with the environment or measuring device, its superposition states (e.g., being in two places at once) effectively disappear. The system appears to collapse into one definite classical state as the environment \"measures\" it.</p> </li> <li> <p>No Wavefunction Collapse:   Decoherence does not imply a traditional wavefunction collapse. Instead, it explains why certain outcomes appear classical: the system is entangled with the environment in such a way that it no longer displays quantum superposition in a measurable way.</p> </li> </ul> <p>Implications</p> <ul> <li> <p>Quantum to Classical Transition:   Decoherence helps explain why macroscopic systems behave classically. When a quantum system interacts with a large environment, it loses its quantum coherence and behaves as a classical system, even though the underlying dynamics remain quantum.</p> </li> <li> <p>Measurement Problem:   Decoherence helps solve the measurement problem by showing how measurement interactions between the system and the apparatus lead to classical outcomes. The entanglement between the system and measurement apparatus prevents the superposition from being observed in practice.</p> </li> <li> <p>Quantum Computing:   In quantum computing, decoherence is a significant challenge. Qubits are highly susceptible to decoherence from environmental interactions, which limits the time during which quantum information can be preserved. Reducing decoherence is critical for reliable quantum computing.</p> </li> </ul>"},{"location":"Physics/pages/entanglement/#5-foundations-of-quantum-mechanics","title":"5. Foundations of Quantum Mechanics","text":"<p>The foundations of quantum mechanics explore principles, interpretations, and unresolved questions at the core of the theory. Below are common theories:</p> <ol> <li> <p>Copenhagen Interpretation    Quantum systems are described by a wavefunction that collapses upon measurement. Measurement defines reality, but the collapse mechanism remains unexplained.</p> </li> <li> <p>Many-Worlds Interpretation    All possible outcomes occur in separate, branching universes. There is no wavefunction collapse, and the universe evolves deterministically.</p> </li> <li> <p>Pilot-Wave Theory (Bohmian Mechanics)    Particles follow definite trajectories guided by a \"pilot wave.\" This restores determinism but introduces non-local interactions.</p> </li> <li> <p>Quantum Bayesianism (QBism)    The wavefunction represents an observer\u2019s subjective knowledge. Measurement outcomes are probabilistic, avoiding objective collapse.</p> </li> <li> <p>Relational Quantum Mechanics    Properties of a quantum system exist only relative to an observer or another system. Reality is observer-dependent, avoiding universal collapse.</p> </li> <li> <p>Objective Collapse Theories    Wavefunction collapse is a real, physical process independent of observation. Examples include GRW theory, which aims to bridge quantum and classical behavior.</p> </li> </ol> <p>Each interpretation addresses the role of measurement, the nature of reality, and the origins of randomness, highlighting the ongoing debate in quantum mechanics.</p>"},{"location":"Physics/pages/entanglement/#takeaways","title":"Takeaways","text":"<ol> <li> <p>Wavefunctions and Probabilities    Quantum states are represented by wavefunctions, with probabilities derived from their squared amplitudes. Particle decay exemplifies how these probabilities evolve over time.</p> </li> <li> <p>Momentum Conservation in Quantum Systems    Momentum conservation in particle decay allows properties of unobserved decay products to be inferred from observed ones, a crucial principle in particle physics.</p> </li> <li> <p>Entanglement and Non-Locality    Entangled particles share a combined quantum state, meaning the measurement of one instantly determines the state of the other, regardless of distance. This underpins phenomena like \"spooky action at a distance.\"</p> </li> <li> <p>The EPR Puzzle and Bell\u2019s Theorem    The EPR paradox challenges quantum mechanics with concepts like local realism, but Bell\u2019s Theorem confirms quantum mechanics\u2019 intrinsic non-locality, ruling out local hidden variable theories.</p> </li> <li> <p>Decoherence and the Quantum-Classical Divide    Decoherence explains how quantum systems lose their coherence through interaction with the environment, transitioning to classical-like behavior without requiring wavefunction collapse.</p> </li> <li> <p>Interpretations of Quantum Mechanics    Competing interpretations\u2014such as Copenhagen, Many-Worlds, and Pilot-Wave\u2014offer differing perspectives on measurement, reality, and randomness, reflecting the unresolved nature of quantum foundations.</p> </li> </ol>"},{"location":"Physics/pages/entanglement/#resources","title":"Resources","text":"<ol> <li>Quanta and Fields by Sean Carroll</li> </ol> &lt;- Measurement Fields -&gt;"},{"location":"Physics/pages/fields/","title":"Fields","text":"<p>From Sean Carroll's \"Quanta and Fields\" </p>"},{"location":"Physics/pages/fields/#key-concepts","title":"Key Concepts","text":""},{"location":"Physics/pages/fields/#1-fields-as-the-fabric-of-reality","title":"1. Fields as the Fabric of Reality","text":"<p>Fields, according to our best current understanding, are among the most fundamental ingredients of nature. In quantum field theory (QFT), particles emerge as excitations or quanta of underlying fields, rather than as independent entities moving through space.  </p> <ul> <li> <p>Not Just in Spacetime\u2014They Are Spacetime   Fields are not simply objects within spacetime; they constitute the dynamical degrees of freedom that fill all of spacetime. Efforts in quantum gravity suggest there may be something even deeper underlying these fields, but such ideas remain speculative.</p> </li> <li> <p>Debates in Interpretation   Just as quantum mechanics spurs debates about measurement and reality, QFT raises profound questions about fields, vacuum energy, and the structure of spacetime. Theoretical work continues to refine our understanding.</p> </li> </ul>"},{"location":"Physics/pages/fields/#2-qft-basics-and-the-role-of-the-hamiltonian","title":"2. QFT Basics and the Role of the Hamiltonian","text":"<p>In quantum theory, energy is introduced through the Hamiltonian, the operator that drives time evolution in the Schr\u00f6dinger equation. In QFT, every field has its own Hamiltonian, which dictates how the field behaves and what energies are allowed.</p> <ul> <li> <p>Energy of a Field   A field\u2019s Hamiltonian encodes both kinetic and potential energy terms. When the field is quantized, these translate into possible excitations of the field\u2014what we call \u201cparticles.\u201d</p> </li> <li> <p>Interaction vs. Free Fields </p> </li> <li>Free Fields:     Fields that do not interact with other fields or with themselves. Their Hamiltonian typically looks like a sum of non-interacting modes\u2014think of them as the simplest \u201cno forces\u201d scenario.  </li> <li>Interacting Fields:     In reality, fields often interact, introducing complexities like scattering, bound states, and nonlinear effects. These interactions make the theory rich\u2014and much more challenging.</li> </ul> <p>A common example for the Hamiltonian density \\(\\mathcal{H}\\) of a free scalar field \\(\\phi(\\mathbf{x}, t)\\) is:</p> \\[ \\mathcal{H} \\;=\\; \\frac{1}{2}\\,\\pi^2(\\mathbf{x}, t)  \\;+\\;\\frac{1}{2}\\,(\\nabla \\phi(\\mathbf{x}, t))^2  \\;+\\;\\frac{1}{2}\\,m^2\\,\\phi^2(\\mathbf{x}, t), \\] <p>where \\(\\pi(\\mathbf{x}, t)\\) is the conjugate momentum to \\(\\phi(\\mathbf{x}, t)\\). Integrating this over all space gives the total Hamiltonian \\(H\\).</p>"},{"location":"Physics/pages/fields/#3-free-fields-and-mass","title":"3. Free Fields and Mass","text":"<p>Free fields have no interaction potential. They spread out over space instead of being localized to a particular region. Even if a free field is said to carry \u201cmass,\u201d it does not automatically localize the field itself. Instead, the mass term in the Hamiltonian affects how excitations (particles) of the field propagate or decay.</p> <ul> <li>Why 'Mass' Still Matters   In QFT, \u201cmass\u201d determines the dispersion relation of excitations\u2014how the energy relates to momentum:</li> </ul> <p>$$   E^2 \\;=\\; \\mathbf{p}^2c^2 \\;+\\; m^2c^4.   $$</p> <p>A massive particle\u2019s energy depends differently on momentum than a massless particle\u2019s (like a photon). Nevertheless, the field itself remains everywhere, with quanta that appear as localized excitations when observed.</p>"},{"location":"Physics/pages/fields/#4-modes-the-building-blocks-of-field-configurations","title":"4. Modes: The Building Blocks of Field Configurations","text":"<p>A powerful way to think about fields is by decomposing them into modes\u2014wave-like solutions that span all of space.</p> <ul> <li>Plane Waves   A free field can be represented as a superposition of plane waves with different wavevectors \\(\\mathbf{k}\\). Mathematically, one common expression is:</li> </ul> <p>$$   \\phi(\\mathbf{x}, t)    \\;=\\; \\int \\frac{d^3k}{(2\\pi)^3} \\,\\Big[\\,      a_{\\mathbf{k}}\\,e^{\\,i(\\mathbf{k}\\cdot\\mathbf{x} \\;-\\; \\omega_{\\mathbf{k}}\\,t)}      \\;+\\;      a_{\\mathbf{k}}^*\\,e^{-\\,i(\\mathbf{k}\\cdot\\mathbf{x} \\;-\\; \\omega_{\\mathbf{k}}\\,t)}   \\Big],   $$</p> <p>where \\(\\omega_{\\mathbf{k}} = \\sqrt{\\mathbf{k}^2 + m^2}\\) for a scalar field (in units where \\(c = 1\\)).</p> <ul> <li> <p>Fourier Transform and \\(\\mathbf{k}\\)-Space   Going from real space \\(\\mathbf{x}\\) to momentum space \\(\\mathbf{k}\\) via the Fourier transform simplifies the mathematics of wave propagation and lays the groundwork for quantization.  </p> </li> <li> <p>Energy of a Mode   For a free scalar field, each \\(\\mathbf{k}\\)-mode behaves like a harmonic oscillator with energy levels spaced by \\(\\hbar \\omega_{\\mathbf{k}}\\). This fact underlies the idea that a free quantum field is essentially an infinite collection of quantum harmonic oscillators.</p> </li> </ul>"},{"location":"Physics/pages/fields/#5-wavefunctions-of-fields","title":"5. Wavefunctions of Fields","text":"<p>When multiple particles or excitations are possible, we no longer have just a \u201cwavefunction of one particle\u2019s position.\u201d Instead, we talk about a wavefunction over field configurations:</p> <ul> <li> <p>Configuration Space   The \u201cposition\u201d variable in standard quantum mechanics is replaced by the entire configuration of the field at each point in space. Formally, you might write a wavefunctional \\(\\Psi[\\phi]\\), which assigns amplitudes to every possible shape (configuration) of the field \\(\\phi\\).</p> </li> <li> <p>No Single-Particle Restriction   This viewpoint naturally accommodates different particle numbers\u2014one, two, or many excitations\u2014without needing separate wavefunctions for each possible number of particles.</p> </li> </ul>"},{"location":"Physics/pages/fields/#6-particles-from-fields-preview","title":"6. Particles from Fields (Preview)","text":"<p>While this chapter focuses on fields themselves, the notion of \u201cparticle\u201d arises when a field is excited in quantized energy levels (the modes). QFT unifies the idea of wave-particle duality: both wave-like and particle-like pictures emerge from the same underlying field.</p> <ul> <li>Creation and Annihilation Operators (Preview)   In a rigorous formulation, each mode \\(\\mathbf{k}\\) is treated as a quantized harmonic oscillator. We introduce operators \\(\\hat{a}^\\dagger_{\\mathbf{k}}\\) (creation) and \\(\\hat{a}_{\\mathbf{k}}\\) (annihilation) that raise or lower the number of quanta in that mode. For example, you might see:</li> </ul> <p>$$   \\hat{\\phi}(\\mathbf{x})    \\;=\\; \\int \\frac{d^3k}{(2\\pi)^3} \\,\\frac{1}{\\sqrt{2\\,\\omega_{\\mathbf{k}}}}    \\Big(      \\hat{a}{\\mathbf{k}}\\,e^{\\,i\\,\\mathbf{k}\\cdot \\mathbf{x}}      \\;+\\;      \\hat{a}{\\mathbf{k}}^\\dagger\\,e^{-\\,i\\,\\mathbf{k}\\cdot \\mathbf{x}}   \\Big).   $$</p> <p>We\u2019ll see how these operators formalize the link between fields and particles in upcoming sections.</p>"},{"location":"Physics/pages/fields/#7-fields-of-the-world","title":"7. Fields of the World","text":"<p>Every known fundamental particle corresponds to a quantum field: electrons (electron field), photons (electromagnetic field), quarks (quark field), gluons (gluon field), and so on. Each field plays a role in the grand tapestry we call the Standard Model of particle physics.</p> <ul> <li>Speculative Frontiers   The quest for quantum gravity and the nature of spacetime continues to drive research. Whether fields themselves are emergent from deeper structures\u2014like strings, loops, or something else\u2014remains an active area of exploration.</li> </ul>"},{"location":"Physics/pages/fields/#takeaways","title":"Takeaways","text":"<ol> <li> <p>Fields Are Primary    In QFT, fields underpin the dynamics and properties of what we observe as \u201cparticles.\u201d The concept of an isolated particle traveling through space is replaced by the concept of excitations in a field permeating the entire universe.</p> </li> <li> <p>The Hamiltonian Defines Energy    The Hamiltonian for a field controls its evolution over time, just as in ordinary quantum mechanics. For free fields, it resembles an infinite collection of harmonic oscillators, each mode providing a quantized energy level.</p> </li> <li> <p>Modes and Superposition    By decomposing fields into plane-wave modes, we reveal how excitations form and propagate. The Fourier transform is central to this picture, illustrating how fields can be built up from an infinite set of simpler wave components.</p> </li> <li> <p>Wavefunctions Over Field Configurations    Instead of merely tracking the position of a single particle, QFT uses wavefunctionals that assign amplitudes to every conceivable field shape. This naturally incorporates situations with variable particle numbers.</p> </li> <li> <p>Linking Fields and Particles    \u201cParticles\u201d are best seen as discrete excitations of underlying fields, with creation and annihilation operators formalizing their quantum behavior. Future sections will explore how interactions shape these excitations, including scattering processes and bound states.</p> </li> <li> <p>Speculative Depths    While QFT is incredibly successful, many open questions remain\u2014especially about gravity and spacetime. We continue to explore whether fields are truly fundamental or themselves emergent from an even deeper theory.</p> </li> </ol>"},{"location":"Physics/pages/fields/#resources","title":"Resources","text":"<ol> <li>Quanta and Fields by Sean Carroll </li> <li>Peskin &amp; Schroeder, An Introduction to Quantum Field Theory </li> <li>David Tong, Lectures on Quantum Field Theory </li> </ol> &lt;- Entanglement Particles -&gt;"},{"location":"Physics/pages/measurement/","title":"Measurement","text":"<p>Based on Sean Carroll's \"Quantum and Fields\"</p>"},{"location":"Physics/pages/measurement/#key-concepts","title":"Key Concepts","text":""},{"location":"Physics/pages/measurement/#1-quantum-measurement","title":"1. Quantum Measurement","text":"<p>In quantum mechanics, measurement differs significantly from classical systems.</p> <ul> <li>Observables are represented by operators; the eigenvalues of these operators correspond to possible measurement outcomes.  </li> <li>Born Rule for measuring an eigenvalue \\( a_i \\):</li> </ul> <p>$$   P(a_i) = \\bigl|\\langle \\psi \\mid a_i\\rangle \\bigr|^2   $$</p> <p>Upon measurement, the wavefunction \\(\\psi\\) collapses to the corresponding eigenstate \\(\\lvert a_i\\rangle\\).</p>"},{"location":"Physics/pages/measurement/#2-wavefunction-collapse-and-quantum-indeterminism","title":"2. Wavefunction Collapse and Quantum Indeterminism","text":"<ul> <li>Quantum Indeterminism: Outcomes are probabilistic rather than deterministic.</li> <li>The wavefunction \\(\\psi\\) encodes probabilities for all possible measurement results.</li> <li>A measurement collapses \\(\\psi\\) to the observed eigenstate.</li> </ul>"},{"location":"Physics/pages/measurement/#3-wave-particle-duality-and-the-double-slit-experiment","title":"3. Wave-Particle Duality and the Double-Slit Experiment","text":"<ul> <li>Wave-Particle Duality: Particles (e.g., electrons, photons) exhibit both wave-like and particle-like properties.</li> <li>Double-Slit Experiment:  </li> <li>Without observation: Particles pass through both slits like waves, creating an interference pattern.  </li> <li>With observation: Particles act like discrete entities, and the interference pattern vanishes.</li> </ul> <p>Implications: - Demonstrates quantum superposition and indeterminism. - Highlights how measurement alters system behavior.</p>"},{"location":"Physics/pages/measurement/#4-the-reality-problem","title":"4. The Reality Problem","text":"<p>Key Question: Does the wavefunction represent physical reality or a mere tool for probability?</p> <ul> <li>Copenhagen Interpretation: Wavefunction collapses upon measurement.  </li> <li>Many-Worlds Interpretation: No collapse; all outcomes occur in parallel branches.</li> </ul>"},{"location":"Physics/pages/measurement/#5-hilbert-space","title":"5. Hilbert Space","text":"<ul> <li>Hilbert space provides the mathematical framework for quantum states, with each state represented as a vector in this space.</li> <li>Operators (e.g., position, momentum, spin) act on these vectors to predict outcomes.</li> </ul> <p>Key Properties: - Potentially infinite-dimensional. - Inner product \\(\\langle \\psi_1 \\mid \\psi_2\\rangle\\) defines probabilities and orthogonality.</p> <p>Differences from Classical Space-Time: - Nature: Classical space-time describes physical geometry; Hilbert space is an abstract state space. - Dimensionality: Space-time has 3+1 dimensions; Hilbert space can be infinite-dimensional. - Purpose: Space-time locates objects physically; Hilbert space underpins quantum predictions. - Structure: Hilbert space is linear (enabling superposition), whereas space-time is not necessarily linear in that sense.</p>"},{"location":"Physics/pages/measurement/#6-qubits","title":"6. Qubits","text":"<p>A qubit is the quantum analog of a classical bit:</p> \\[ |\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle \\quad\\text{with}\\quad |\\alpha|^2 + |\\beta|^2 = 1. \\] <ul> <li>Measurement collapses the qubit to \\(\\lvert 0\\rangle\\) or \\(\\lvert 1\\rangle\\) with probabilities \\(|\\alpha|^2\\) and \\(|\\beta|^2\\).</li> </ul>"},{"location":"Physics/pages/measurement/#7-operators-and-observables","title":"7. Operators and Observables","text":"<ul> <li>Operators correspond to measurable quantities.  </li> <li>Commutation Relations:</li> </ul> <p>$$   [\\hat{A}, \\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}.   $$</p> <ul> <li>If \\([\\hat{A}, \\hat{B}] = 0\\), they are compatible (can be measured simultaneously).  </li> <li>If \\([\\hat{A}, \\hat{B}] \\neq 0\\), they are incompatible.</li> </ul>"},{"location":"Physics/pages/measurement/#8-uncertainty-principle","title":"8. Uncertainty Principle","text":"<p>The Heisenberg Uncertainty Principle limits simultaneous knowledge of certain pairs of observables (e.g., position \\(x\\) and momentum \\(p\\)).</p> \\[ \\Delta x \\,\\Delta p \\,\\ge \\,\\frac{\\hbar}{2} \\] <ul> <li>\\(\\Delta x\\): Uncertainty in position  </li> <li>\\(\\Delta p\\): Uncertainty in momentum</li> </ul> <p>Explanation: - Arises from the wave nature of quantum systems (Fourier transform relationship). - Localizing a wavefunction in position space broadens it in momentum space, and vice versa. - For an illustrative video, see 3Blue1Brown\u2019s explanation.</p>"},{"location":"Physics/pages/measurement/#9-momentum-and-measurement","title":"9. Momentum and Measurement","text":"<ul> <li>Momentum is represented by the operator:</li> </ul> <p>$$   \\hat{p} = -\\,i\\hbar \\,\\frac{\\partial}{\\partial x}.   $$</p> <ul> <li>Measuring momentum collapses the wavefunction into a momentum eigenstate.</li> </ul>"},{"location":"Physics/pages/measurement/#important-examples","title":"Important Examples","text":"<p>Spin Measurement - Measuring spin along any axis collapses the state to \\(|+\\rangle\\) or \\(|-\\rangle\\). - Spin measurements along different axes (e.g., \\(x, y, z\\)) are incompatible.</p> <p>Double-Slit Experiment - Demonstrates quantum superposition and the significance of measurement.</p> <p>Position and Momentum - Conjugate variables governed by \\(\\Delta x \\,\\Delta p \\,\\ge \\,\\hbar/2\\).</p>"},{"location":"Physics/pages/measurement/#takeaways","title":"Takeaways","text":"<ol> <li>Measurement is central to quantum mechanics, introducing probabilities and wavefunction collapse.  </li> <li>Observables are encoded as operators, whose eigenvalues/eigenstates determine outcomes.  </li> <li>The uncertainty principle and wave-particle duality emphasize the unique nature of quantum systems.  </li> <li>The double-slit experiment highlights indeterminism and the measurement problem.  </li> <li>Hilbert space provides the core mathematical structure of quantum mechanics.</li> </ol>"},{"location":"Physics/pages/measurement/#resources","title":"Resources","text":"<ol> <li>Quanta and Fields by Sean Carroll </li> <li>3Blue1Brown's video on the uncertainty principle</li> </ol> <p>\u2190 Wave Function | Entanglement \u2192</p>"},{"location":"Physics/pages/waveFunction/","title":"Schr\u00f6dinger equation","text":"<p>The Schr\u00f6dinger equation is the cornerstone of quantum mechanics. It describes how the wave function of a quantum system evolves over time:</p> \\[ i \\hbar \\frac{\\partial \\psi}{\\partial t} = \\hat{H} \\psi \\] <ul> <li><code>i</code> is the imaginary unit (\u221a-1),</li> <li><code>\u210f</code> is the reduced Planck\u2019s constant,</li> <li><code>\u03c8(x, t)</code> is the wave function, representing the quantum state of the system,</li> <li><code>\u0124</code> is the Hamiltonian operator, representing the total energy of the system (kinetic + potential).</li> </ul>"},{"location":"Physics/pages/waveFunction/#the-wave-function","title":"The Wave Function","text":"<p>The Wave Function, <code>\u03c8(x, t)</code>, is a complex-valued function that encapsulates all the information about a quantum system. It serves as a mathematical representation of the particle's quantum state, containing information about properties such as position, momentum, and energy.</p> <p>The absolute value squared of the wave function, <code>|\u03c8(x, t)|^2</code>, is interpreted as the probability density of finding a particle at position <code>x</code> and time <code>t</code>. This interpretation was introduced by Max Born and is central to the probabilistic nature of quantum mechanics.</p> <p>Key points about the absolute value:</p> <ul> <li>Probability Density: <code>|\u03c8(x, t)|^2</code> represents the likelihood of locating a particle in a specific region of space. For example, in one dimension, the probability of finding the particle between positions <code>a</code> and <code>b</code> is given by:   $$   P(a \\leq x \\leq b) = \\int_a^b |\u03c8(x, t)|^2 dx   $$  </li> <li>Normalization: The total probability of finding the particle in all space must be 1. This imposes the normalization condition:   $$   \\int_{-\\infty}^{\\infty} |\u03c8(x, t)|^2 dx = 1   $$</li> <li>Interpretation: While the wave function itself is a complex number, only its absolute value squared has a physical interpretation.</li> </ul>"},{"location":"Physics/pages/waveFunction/#the-hamiltonian","title":"The Hamiltonian","text":"<p>The Hamiltonian operator, <code>\u0124</code>, defines the total energy of a quantum system and can take different forms depending on the system being studied. Some common examples include:</p> <ol> <li>Free Particle:    $$    \\hat{H} = -\\frac{\\hbar^2}{2m} \\nabla^2    $$</li> <li> <p>Represents a particle with no potential energy, only kinetic energy.</p> </li> <li> <p>Particle in a Potential:    $$    \\hat{H} = -\\frac{\\hbar^2}{2m} \\nabla^2 + V(x)    $$</p> </li> <li> <p>Includes a potential energy term, <code>V(x)</code>.</p> </li> <li> <p>Harmonic Oscillator:    $$    \\hat{H} = -\\frac{\\hbar^2}{2m} \\nabla^2 + \\frac{1}{2} m \\omega^2 x^2    $$</p> </li> <li> <p>Describes systems like vibrating molecules or quantum springs.</p> </li> <li> <p>Hydrogen Atom:    $$    \\hat{H} = -\\frac{\\hbar^2}{2m} \\nabla^2 - \\frac{e^2}{4 \\pi \\epsilon_0 r}    $$</p> </li> <li>Models the interaction between an electron and a proton.</li> </ol> <p>The form of the Hamiltonian determines the dynamics of the quantum system and governs the evolution of the wave function over time.</p>"},{"location":"Physics/pages/waveFunction/#a-brief-history-of-the-development-of-the-wave-function","title":"A Brief History of the Development of the Wave Function","text":"<p>Light as a Wave</p> <ul> <li>Christiaan Huygens (1678): Proposed the wave theory of light, suggesting that light travels as waves, contrary to Isaac Newton's particle theory.</li> <li>Thomas Young (1801): Demonstrated light's wave nature through the double-slit experiment, revealing interference patterns.</li> <li>Augustin-Jean Fresnel (1816): Expanded on Huygens' principle, mathematically explaining diffraction and interference.</li> </ul> <p>Electromagnetism and Light</p> <ul> <li>Michael Faraday (1831): Introduced the concept of electromagnetic fields, showing the interplay between electricity and magnetism.</li> <li>James Clerk Maxwell (1861\u20131865): Unified electricity, magnetism, and optics with Maxwell's equations, proving light to be an electromagnetic wave.</li> </ul> <p>The Atom and Subatomic Discoveries</p> <ul> <li>John Dalton (1803): Proposed that matter is composed of indivisible atoms.</li> <li>Dmitri Mendeleev (1869): Developed the periodic table, emphasizing the discrete properties of elements.</li> <li>J.J. Thomson (1897): Discovered the electron, introducing subatomic particles to atomic theory.</li> <li>Ernest Rutherford (1911): Conducted the gold foil experiment, leading to the nuclear model of the atom with a dense nucleus surrounded by electrons. This model raised questions about electron stability and atomic spectra.</li> </ul> <p>Blackbody Radiation and Quantization</p> <ul> <li>Max Planck (1900): Solved the blackbody radiation problem by introducing energy quantization: \\(E = h \\nu\\) , where <code>h</code> is Planck's constant and <code>v</code> is the frequency.</li> <li>Albert Einstein (1905): Explained the photoelectric effect using quantized light particles (photons), supporting the wave-particle duality of light.</li> </ul> <p>Atomic Spectra and Discrete Energy Levels</p> <ul> <li>Niels Bohr (1913): Introduced the Bohr model, explaining hydrogen's spectral lines via quantized electron orbits.</li> </ul> <p>Wave-Particle Duality</p> <ul> <li> <p>Louis de Broglie (1924): Proposed that matter exhibits wave-like behavior, with a wavelength given by:   $$   \\lambda = \\frac{h}{p}   $$   where <code>p</code> is the momentum.</p> </li> <li> <p>Albert Einstein (1905): Demonstrated the particle-like behavior of light through the photoelectric effect, showing that light is made up of discrete packets of energy called photons.</p> </li> <li> <p>Werner Heisenberg (1927): Introduced the uncertainty principle, which states that certain pairs of physical properties, such as position and momentum, cannot be simultaneously measured with arbitrary precision.</p> </li> </ul> <p>Schr\u00f6dinger\u2019s Equation</p> <ul> <li>Erwin Schr\u00f6dinger (1926): Developed the wave equation to describe quantum systems. The wave function \\(\\psi(x, t)\\) represents the quantum state of a particle and evolves according to:   $$   i \\hbar \\frac{\\partial \\psi}{\\partial t} = \\hat{H} \\psi   $$   where \\(\\hat{H}\\) is the Hamiltonian operator. Schr\u00f6dinger's work explained atomic phenomena such as electron orbitals.</li> </ul> <p>Interpretation of the Wave Function</p> <ul> <li>Max Born (1926): Interpreted \\(\\psi\\) as a probability amplitude, with \\(|\\psi|^2\\) giving the probability density of finding a particle in a given state. This marked a departure from deterministic classical mechanics.</li> </ul> <p>Complementarity and Uncertainty</p> <ul> <li>Werner Heisenberg (1927): Formulated the uncertainty principle, limiting the simultaneous measurement of properties like position and momentum.</li> <li>Niels Bohr: Advocated for complementarity, asserting the dual wave-particle nature of quantum systems.</li> </ul>"},{"location":"Physics/pages/waveFunction/#blackbody-thermal-radiation-problem-and-its-solution","title":"Blackbody (Thermal) Radiation Problem and Its Solution","text":"<p>The Blackbody Radiation Problem</p> <p>Blackbody radiation refers to the electromagnetic radiation emitted by an idealized object that absorbs all incident radiation, regardless of wavelength or angle. This object, called a blackbody, emits radiation solely based on its temperature. By the late 19th century, experimental studies of blackbody radiation revealed discrepancies that classical physics could not explain.</p> <p>Classical Predictions</p> <ul> <li>Rayleigh-Jeans Law: Classical physics, using the equipartition theorem, predicted that the intensity of blackbody radiation at short wavelengths (high frequencies) would grow infinitely, leading to what became known as the ultraviolet catastrophe. Mathematically, the intensity \\( I(\\nu, T) \\) was given by: $$ I(\\nu, T) = \\frac{8 \\pi \\nu^2}{c^3} k_B T $$</li> </ul> <p>Where: - \u03bd: Frequency of radiation - c: Speed of light - k_B: Boltzmann constant - T: Temperature of the blackbody</p> <p>This equation worked well at long wavelengths (low frequencies) but diverged at high frequencies, predicting an infinite energy output, which was physically impossible.</p> <p>Experimental Results</p> <p>Experimental data showed that the radiation intensity increased with frequency, peaked at a certain value, and then declined at higher frequencies. Classical theories failed to reproduce this behavior, prompting the need for a new explanation.</p> <p></p> <p>Source: LibreTexts Chemistry - Blackbody Radiation Cannot Be Explained Classically</p> <p>The Solution by Max Planck</p> <p>In 1900, Max Planck proposed a revolutionary solution that marked the birth of quantum theory. Planck introduced the idea that energy is not continuous but quantized. He suggested that electromagnetic radiation is emitted or absorbed in discrete packets, called quanta, with energy: $$ E = h \\nu $$ where: - h: Planck's constant (\\(6.626 \\times 10^{-34} \\, \\text{Js}\\)) - v: Frequency of radiation</p> <p>Planck\u2019s Law</p> <p>Using this assumption, Planck derived an equation that accurately described the observed blackbody radiation spectrum: $$ I(\\nu, T) = \\frac{8 \\pi h \\nu^3}{c^3} \\frac{1}{e^{\\frac{h \\nu}{k_B T}} - 1} $$ This equation successfully matched experimental data at all wavelengths, resolving the ultraviolet catastrophe.</p> <p>Implications of Planck's Solution</p> <ol> <li>Quantization of Energy: Planck's hypothesis introduced the concept of quantized energy levels, fundamentally challenging the continuous energy assumptions of classical physics.</li> <li>Wave-Particle Duality: The idea of energy quanta set the stage for the later development of wave-particle duality, where light and matter exhibit both wave-like and particle-like properties.</li> <li>Foundation of Quantum Mechanics: Planck\u2019s work paved the way for subsequent discoveries in quantum theory, including the photoelectric effect (Einstein), matter waves (de Broglie), and the wave function (Schr\u00f6dinger).</li> </ol>"},{"location":"Physics/pages/waveFunction/#particles-acting-like-waves-and-waves-acting-like-particles","title":"Particles Acting Like Waves and Waves Acting Like Particles","text":"<p>The development of quantum mechanics was heavily influenced by the realization that particles can exhibit wave-like behavior, and waves can exhibit particle-like behavior. This duality challenged the traditional boundaries between particles and waves in classical physics and became a cornerstone of quantum mechanics.</p>"},{"location":"Physics/pages/waveFunction/#waves-acting-like-particles","title":"Waves Acting Like Particles","text":"<p>Photoelectric Effect</p> <ul> <li>Discovery: The photoelectric effect demonstrated that light behaves like discrete packets of energy, called photons, rather than a continuous wave.</li> <li>Albert Einstein (1905): Explained that when light of sufficient frequency strikes a metal surface, it ejects electrons. This effect could not be explained by the wave theory of light, which predicted that energy would depend on the intensity of the light rather than its frequency.</li> <li>Key Equation: Einstein's explanation relied on the quantization of light energy:   $$   E = h \\nu   $$   where:</li> <li><code>E</code> is the energy of a photon,</li> <li><code>h</code> is Planck's constant, and</li> <li><code>\u03bd</code> is the frequency of light.</li> <li>Significance: This discovery provided direct evidence of the particle nature of light and introduced the concept of photons, supporting the idea of wave-particle duality.</li> </ul> <p>Compton Scattering</p> <ul> <li>Arthur Compton (1923): Showed that X-rays scattered off electrons exhibit changes in wavelength consistent with treating X-rays as particles (photons) colliding with electrons. This reinforced the particle-like behavior of waves.</li> </ul>"},{"location":"Physics/pages/waveFunction/#particles-acting-like-waves","title":"Particles Acting Like Waves","text":"<p>de Broglie's Hypothesis</p> <ul> <li>Louis de Broglie (1924): Proposed that matter, like light, exhibits wave-like properties. He introduced the concept of matter waves, with the wavelength of a particle given by:   $$   \\lambda = \\frac{h}{p}   $$   where:</li> <li><code>\u03bb</code> is the wavelength of the particle,</li> <li><code>h</code> is Planck's constant, and</li> <li><code>p</code> is the momentum of the particle.</li> <li>Implications: This hypothesis suggested that all particles, including electrons, have associated wave properties, which was later confirmed experimentally.</li> </ul> <p>Electron Diffractio</p> <ul> <li>Davisson-Germer Experiment (1927): Demonstrated the wave-like behavior of electrons by showing that they produce diffraction patterns when scattered off a crystal. This experiment provided direct evidence of de Broglie's matter waves.</li> </ul>"},{"location":"Physics/pages/waveFunction/#wave-particle-duality","title":"Wave-Particle Duality","text":"<p>Wave-particle duality is the principle that particles and waves exhibit both wave-like and particle-like properties depending on the experiment: - For Light: Light behaves as a wave in phenomena like interference and diffraction, but as particles (photons) in phenomena like the photoelectric effect. - For Matter: Particles like electrons behave as localized entities in some contexts but exhibit wave-like behavior, such as interference and diffraction, in others.</p>"},{"location":"Physics/pages/waveFunction/#impact-on-quantum-mechanics","title":"Impact on Quantum Mechanics","text":"<ol> <li>Quantum Theory of Radiation: The particle-like behavior of light and wave-like behavior of particles necessitated a new theory for describing phenomena at small scales.</li> <li>Wave Function: Schr\u00f6dinger\u2019s wave equation formalized the concept of wave-particle duality, describing particles as wave functions that evolve in time.</li> <li>Heisenberg's Uncertainty Principle: The duality introduced fundamental limits to simultaneously measuring properties like position and momentum:    $$    \\Delta x \\cdot \\Delta p \\geq \\frac{\\hbar}{2}    $$    where:</li> <li><code>\u0394x</code> is the uncertainty in position,</li> <li><code>\u0394p</code> is the uncertainty in momentum, and</li> <li><code>\u210f</code> is the reduced Planck's constant.</li> </ol>"},{"location":"Physics/pages/waveFunction/#resources","title":"Resources","text":"<ol> <li>LibreTexts Chemistry - Blackbody Radiation Cannot Be Explained Classically</li> <li>Quanta and Fields by Sean Carroll</li> </ol> &lt;- Overview Measurement -&gt;"},{"location":"Protocols/pages/","title":"Overview","text":"<ol> <li>Controller Area Network (CAN)</li> </ol>"},{"location":"Protocols/pages/can/","title":"Controller Area Network (CAN)","text":"<p>I put a decent amount of work in the wikipedia page for CAN, while it still needs work, use/update that.</p>"},{"location":"Signals/pages/","title":"Overview","text":"<ol> <li>Intro to DSP<ul> <li>Digital Signal Processing (DSP) basics</li> </ul> </li> <li>FCC Electromagnetic Spectrum<ul> <li>Technical details and resources for working with specific frequency bands</li> </ul> </li> </ol>"},{"location":"Signals/pages/#resources","title":"Resources","text":"<ol> <li>Digital Signal Processing and Software Defined Radio: Theory and Construction of the T41-EP Software Defined Transceiver by Albert F Peter and Dr. Jack Purdum</li> <li>Common Microcontroller Software Interface Standard<ul> <li>Open source embedded DSP library</li> </ul> </li> <li>FCC Electromagnetic Spectrum</li> </ol>"},{"location":"Signals/pages/introDsp/","title":"Introduction to Digital Signal Processing (DSP)","text":""},{"location":"Signals/pages/introDsp/#what-is-dsp","title":"What is DSP?","text":"<p>Digital Signal Processing (DSP) involves analyzing, manipulating, and synthesizing signals using digital techniques. Rather than working directly with continuous-time signals (as in analog systems), DSP first converts analog signals into a sequence of discrete samples.</p> <ul> <li>Sampling: Converting a continuous-time signal \\( x(t) \\) into a discrete-time sequence \\( x[n] \\) by taking periodic snapshots at intervals \\( T_s \\).  </li> <li>Quantization: Mapping each sample to a finite set of integer levels.  </li> </ul> <p>Thus, a continuous waveform is represented in a digital form suitable for storage, transmission, or further numerical processing.</p>"},{"location":"Signals/pages/introDsp/#comparing-analog-and-digital","title":"Comparing Analog and Digital","text":"<p>Analog systems handle continuously varying voltages, currents, or other physical quantities. Digital systems, on the other hand, work with discrete numerical values (bits).</p> <p>Analog Systems   - Continuous in time and amplitude.   - Subject to noise accumulation over long distances.   - Hardware-based filters (resistors, capacitors, inductors) adjust frequency response.</p> <p>Superheterodyne AM Receiver Block and Circuit Diagrams </p> <p></p> <p>To gain idea of the incresed hardware complexity associated with anaolog system, above is the Block and circuit diagram of a Superheterodyne AM Receiver. See refrences for a deeper dive. Compare it to the SDR below.</p> <p>Digital Systems   - Discrete in time and amplitude.   - Noise-resistant (regeneration of binary signals can reduce error).   - Highly flexible: Filters, transforms, and algorithms can be reprogrammed or updated.  </p> <p></p> <p>Unlike the analog system above, the SDR only needs hardware to tune the signal down to an intermediate frequency (IF) then it is sampled and handled via software.</p>"},{"location":"Signals/pages/introDsp/#in-phase-i-and-quadrature-q-signals","title":"In-Phase (I) and Quadrature (Q) Signals","text":"<p>When a signal is transmitted or received at a particular carrier frequency \\( f_c \\), it can be decomposed into two orthogonal components:</p> <ol> <li>An in-phase component, denoted \\( I \\), often aligned with a cosine wave.  </li> <li>A quadrature component, denoted \\( Q \\), often aligned with a sine wave.</li> </ol> <p>Mathematically, a bandpass signal \\( s(t) \\) at frequency \\( f_c \\) can be represented as:</p> \\[ s(t) = I(t)\\cos(2\\pi f_c t) - Q(t)\\sin(2\\pi f_c t). \\] <p>Why \"In-Phase\" and \"Quadrature\"? - The two components are out of phase by \\( 90^\\circ \\) (i.e., \\(\\pi/2\\) radians). - They form an orthogonal basis, allowing them to be processed independently and recombined without interference.</p>"},{"location":"Signals/pages/introDsp/#qsd-and-qse","title":"QSD and QSE","text":"<ul> <li>Quadrature Sampling Detector (QSD):   A technique for down-converting a high-frequency signal to baseband by simultaneously sampling in-phase and quadrature components.  </li> <li>Quadrature Sampling Exciter (QSE):   The reverse operation\u2014up-converting baseband I/Q signals to a higher, desired carrier frequency.</li> </ul> <p>These approaches exploit the fact that many modern communication standards use I/Q modulation for efficient data encoding.</p>"},{"location":"Signals/pages/introDsp/#resources","title":"Resources","text":"<ol> <li>Digital Signal Processing and Software Defined Radio: Theory and Construction of the T41-EP Software Defined Transceiver by Albert F Peter and Dr. Jack Purdum</li> <li>Common Microcontroller Software Interface Standard</li> <li>Superheterodyne AM Receiver - Working with Block Diagram and Schematics by Aleksander Kopyto</li> </ol> <p>\u2190 Previous | Next \u2192</p>"},{"location":"Signals/pages/spectrum/","title":"FCC Electromagnetic Spectrum","text":"<p>Based off the FCC Electromagnetic Spectrum, this page covers technical details for working with specific frequency bands.</p>"},{"location":"Signals/pages/spectrum/#960-1164mhz-aeronautical-radionavigational-service-arns-aeronautical-mobile-route-service-amrs","title":"960-1164MHz Aeronautical Radionavigational Service (ARNs) &amp; Aeronautical Mobile (Route) Service (AM(R)S)","text":""},{"location":"Signals/pages/spectrum/#1090mhz-automatic-dependent-surveillancebroadcast","title":"1090MHz Automatic Dependent Surveillance\u2013Broadcast","text":"<ul> <li>Data: Broadcasted messages include flight details such as position, altitude, and speed.</li> <li>Usage: Decode these signals and visualize the real-time locations of aircraft on a map.</li> <li>RTL-SDR decoder (has a basic visualization tool): dump1090</li> <li>For plotting track lines use: Virtual Radar Server</li> </ul>"},{"location":"Software/pages/","title":"Overview","text":"<ol> <li>Software Development Flow</li> <li>C Programming Language</li> </ol>"},{"location":"Software/pages/c/","title":"C Programming Language","text":"<p>C is one of the most widely used programming languages, particularly in systems programming and embedded development.</p>"},{"location":"Software/pages/c/#pros-and-cons-of-c","title":"Pros and Cons of C","text":""},{"location":"Software/pages/c/#pros","title":"Pros","text":"<ul> <li> <p>Efficiency: C is close to hardware and offers excellent performance, making it ideal for resource-constrained systems like embedded devices.</p> </li> <li> <p>Portability: C code can run on a wide variety of platforms with minimal changes, as long as it adheres to standard conventions.</p> </li> <li> <p>Control Over Hardware: C provides low-level access to memory through pointers and direct manipulation, which is essential in embedded systems.</p> </li> <li> <p>Extensive Libraries and Tools: A rich ecosystem of libraries, compilers, and debuggers is available, supporting a variety of platforms.</p> </li> <li> <p>Structured Programming: Encourages modular code organization with functions and reusable code structures.</p> </li> </ul>"},{"location":"Software/pages/c/#cons","title":"Cons","text":"<ul> <li> <p>No Built-in Safety: Lacks features like memory safety (e.g., bounds checking) and type safety, making bugs like buffer overflows and segmentation faults common. </p> <ul> <li>Alternative Languages: Rust (provides memory safety guarantees for embedded systems), Ada (designed for safety-critical embedded applications).</li> </ul> </li> <li> <p>Manual Memory Management: Requires careful handling of memory allocation and deallocation using <code>malloc</code> and <code>free</code>. </p> <ul> <li>Alternative Languages: Rust (ownership system for memory safety), Zig (manual memory control with safety features).</li> </ul> </li> <li> <p>Limited Abstraction: Compared to modern languages, C provides fewer abstractions, which can lead to verbose or error-prone code.</p> <ul> <li>Alternative Languages: C++ (offers embedded-compatible abstractions), Ada (rich abstractions with embedded focus).</li> </ul> </li> <li> <p>Difficult Debugging: Debugging C programs, especially those with pointer or memory issues, can be challenging. </p> <ul> <li>Alternative Languages: Rust (compile-time safety checks reduce runtime bugs), Embedded Python (simplifies debugging for certain embedded use cases).</li> </ul> </li> </ul>"},{"location":"Software/pages/c/#define-vs-declare","title":"Define vs. Declare","text":"<p>In C, the terms define and declare are fundamental but often confused. Here's a breakdown:</p>"},{"location":"Software/pages/c/#declaration","title":"Declaration","text":"<p>A declaration tells the compiler about the type and name of a variable, function, or object without allocating memory or providing implementation. It acts as a \"promise\" that the defined entity will exist elsewhere.</p> <p>Examples:</p> <p><code>c   extern int x;   // Declares an integer 'x' defined elsewhere   int myFunc();   // Declares a function 'myFunc' returning an int</code></p> <p>Key Points:</p> <ul> <li>Declarations are often found in header files to inform other files about functions or variables they can use.</li> <li>They do not allocate memory (for variables) or provide a body (for functions).</li> </ul> <p>When to Use:</p> <ul> <li>Use declarations in header files to allow other source files to reference external variables or functions without duplicating their definitions.</li> </ul>"},{"location":"Software/pages/c/#definition","title":"Definition","text":"<p>A definition allocates memory (for variables) or provides the implementation (for functions). It is the actual \"creation\" of the variable or function.</p> <p>Examples:</p> <p><code>c   int x = 42;       // Defines and initializes 'x' (allocates memory)   int myFunc() {    // Defines the function 'myFunc'       return 42;   }</code></p> <p>Key Points:</p> <ul> <li>A definition is necessary for the program to work, as it provides the actual resource or implementation.</li> <li>Definitions are generally found in source files.</li> </ul> <p>When to Use:</p> <ul> <li>Use definitions in source files to allocate memory for variables or implement the functionality of declared functions.</li> </ul> <p>Summary of Differences</p> Aspect Declaration Definition Purpose Announces the existence of an entity Creates and allocates the entity Memory Usage Does not allocate memory Allocates memory Functionality No implementation Provides implementation Location Typically in header files Typically in source files"},{"location":"Software/pages/developmentProcess/","title":"Software Development Flow (Embedded Systems)","text":"<p>This page details a typical build and deployment process in embedded software development, including the roles of linker files and incremental compilation. While many examples use the GNU toolchain, the concepts below apply to a wide variety of compilers and IDEs (e.g., ARM Compiler, Keil, IAR, XC8, etc.) across vendors like STM32, Cypress (Infineon), and Microchip.</p>"},{"location":"Software/pages/developmentProcess/#overview-of-the-build-process","title":"Overview of the Build Process","text":"<ol> <li> <p>Preprocessing </p> <ul> <li>The preprocessor handles directives like <code>#include</code> and <code>#define</code> in your C/C++ source files.  </li> <li>Produces expanded source files (often named <code>.i</code> or <code>.ii</code>).</li> </ul> </li> <li> <p>Compilation </p> <ul> <li>Converts the preprocessed source code into assembly (internally or explicitly).  </li> <li>Produces object files (<code>.o</code>, <code>.obj</code>) containing machine code for each module.</li> </ul> </li> <li> <p>Assembly (if separate) </p> <ul> <li>If you have raw assembly code or if your build toolchain treats assembly as a distinct step, the assembler converts <code>.s</code> files into object files.</li> </ul> </li> <li> <p>Linking </p> <ul> <li>Combines all object files into a single executable (or firmware image) using linker files (also called linker scripts, scatter files, or linker command files, depending on the toolchain).  </li> <li>Resolves symbol references (e.g., function names, global variables) and places code/data in specific memory sections.</li> </ul> </li> <li> <p>Hex/Bin Generation </p> <ul> <li>Many embedded tools produce a <code>.hex</code>, <code>.bin</code>, or similar file to be loaded into the microcontroller\u2019s non-volatile memory (Flash).</li> </ul> </li> <li> <p>Flashing/Programming </p> <ul> <li>The final image is written to your device\u2019s memory (e.g., Flash) using a hardware programmer/debugger (e.g., ST-Link for STM32, PSoC Programmer for Cypress, MPLAB IPE for Microchip, or generic JTAG/SWD).</li> </ul> </li> <li> <p>Debugging </p> <ul> <li>Tools like GDB, Arm Debugger, IAR C-SPY, or Keil \u00b5Vision allow you to step through code, set breakpoints, and inspect memory/registers on the target hardware.</li> </ul> </li> </ol>"},{"location":"Software/pages/developmentProcess/#linker-files-linker-scripts-scatter-files","title":"Linker Files (Linker Scripts / Scatter Files)","text":"<p>In embedded systems, linker configuration files are crucial for defining how compiled code and data map into the microcontroller\u2019s memory. Different toolchains have different naming conventions:</p> <ul> <li>GNU ld: <code>linker.ld</code> or <code>.ld</code> scripts  </li> <li>Arm Compiler (Keil): Scatter files (<code>.sct</code>)  </li> <li>IAR: <code>.icf</code> (IAR Linker Configuration File)  </li> <li>Microchip: Linker scripts for XC compilers (e.g., <code>.gld</code>)</li> </ul>"},{"location":"Software/pages/developmentProcess/#memory-regions","title":"Memory Regions","text":"<p>Most linker files specify the sizes and addresses of Flash (program memory) and RAM (data memory). For example:</p> <pre><code>MEMORY\n{\n  FLASH (rx) : ORIGIN = 0x08000000, LENGTH = 256K\n  RAM  (rwx) : ORIGIN = 0x20000000, LENGTH = 64K\n}\n</code></pre>"},{"location":"Software/pages/developmentProcess/#sections","title":"Sections","text":"<p>Code and data sections (e.g., .text, .data, .bss, .rodata) are placed in the corresponding memory regions. For instance:</p> <pre><code>SECTIONS\n{\n  .text : {\n     *(.text)\n     *(.text*)\n     ...\n  } &gt; FLASH\n\n  .data : {\n     *(.data)\n     *(.data*)\n     ...\n  } &gt; RAM AT &gt; FLASH\n  ...\n}\n</code></pre> <p>By customizing these files, you control where code and variables reside\u2014critical for ensuring the program fits within the microcontroller\u2019s memory constraints and meets performance requirements (e.g., running time-critical code from RAM).</p>"},{"location":"Software/pages/developmentProcess/#incremental-compilation","title":"Incremental Compilation","text":"<p>Incremental compilation is a build strategy where only changed source files (and their dependents) are recompiled, rather than recompiling the entire codebase. This concept applies across a variety of toolchains and IDEs:</p> <p>How It Works</p> <ol> <li>The build system checks file timestamps or hashes (e.g., Make, CMake, vendor IDE projects).  </li> <li>If only a single <code>.c</code> file is modified, only that file (and any files depending on it) are recompiled.</li> </ol> <p>Why It\u2019s Important for Embedded</p> <ul> <li> <p>Faster Iterations: Large projects (e.g., STM32, PSoC, or Microchip-based) can be slow to compile from scratch.   Incremental builds let you test changes more rapidly.</p> </li> <li> <p>Efficiency: Reduces CPU usage and the time you spend waiting in the edit-compile-flash-debug cycle.</p> </li> </ul> <p>Potential Pitfalls</p> <ul> <li> <p>Dependency Tracking:  Misconfigured or missing dependencies can result in stale object files.</p> </li> <li> <p>Linker Script Changes:  Altering memory layouts or section placements often requires a full rebuild to ensure everything is placed correctly.</p> </li> </ul>"},{"location":"Software/pages/developmentProcess/#best-practices-for-embedded-builds","title":"Best Practices for Embedded Builds","text":"<ol> <li> <p>Keep Code Modular</p> <ul> <li>Break your application into multiple modules, each with its own responsibility.  </li> <li>This helps the build system isolate changes and recompile only what\u2019s necessary.</li> </ul> </li> <li> <p>Use a Robust Build System</p> <ul> <li>CMake, Meson, Makefiles, or IDE-based solutions (Keil, IAR Embedded Workbench, MPLAB X) all support incremental builds.  </li> <li>Ensure your system is correctly configured so unchanged modules are skipped.</li> </ul> </li> <li> <p>Maintain a Clean Linker Configuration</p> <ul> <li>Document memory regions and section mappings clearly.  </li> <li>Update if you switch MCUs (e.g., going from an STM32F4 to STM32F7, or from PSoC 4 to PSoC 6).</li> </ul> </li> <li> <p>Enable Compiler Optimizations Wisely</p> <ul> <li>For Debug: Use lower optimization (e.g., <code>-O0</code>, <code>-Og</code>) to preserve debugging symbols and structure.  </li> <li>For Release: Use higher optimization (e.g., <code>-O2</code>, <code>-Os</code>) or vendor-specific flags for performance/size gains.</li> </ul> </li> <li> <p>Test Early and Often</p> <ul> <li>Automated tests, static analysis, and continuous integration help catch bugs before they hit hardware.  </li> <li>Regular flashing and real-device tests ensure performance and reliability in actual operating conditions.</li> </ul> </li> </ol>"},{"location":"Software/pages/developmentProcess/#references","title":"References","text":"<ul> <li>Arm Compiler Documentation </li> <li>Keil \u00b5Vision (Arm MDK) </li> <li>IAR Embedded Workbench </li> <li>Microchip MPLAB X IDE </li> <li>GNU Linker (ld) Documentation </li> </ul> <p>\u2190 Previous Section | Next Section \u2192</p>"},{"location":"ai/pages/","title":"Artificial Intelligence","text":""},{"location":"ai/pages/#overview","title":"Overview","text":"<p>This section contains notes and resources related to Artificial Intelligence (AI), including hardware, programming, and parallel computing concepts. It primarily draws from my UTSA AI Hardware and Programming course but applies to broader AI hardware topics.</p>"},{"location":"ai/pages/#topics-covered","title":"Topics Covered","text":"<ul> <li>Hardware<ul> <li>Covers fundamentals relevant to AI, but not limited to AI.</li> <li>Memory Hierarchy Design</li> <li>Parallel Computing</li> <li>Cache-Aware Matrix Multiplication &amp; GPU Performance Analysis</li> </ul> </li> <li>Software:<ul> <li>CUDA C\\C++</li> </ul> </li> <li>Convolutional Neural Networks<ul> <li>Concepts and implementation details for training and running neural networks efficiently.</li> <li>1-D CNN Examples </li> </ul> </li> <li>Nvidia's Jetson Nano<ul> <li>Notes and experiences using Nvidia\u2019s Jetson Nano for AI and edge computing applications.</li> </ul> </li> </ul>"},{"location":"ai/pages/#resources","title":"Resources","text":"<ul> <li>Notes are mainy derived from Dr. Chen Pan's Hardware and programming course at UTSA. Instructor Email: chen.pan@utsa.edu</li> <li>Computer Architecture: A Quantitative Approach 6th Edition</li> </ul>"},{"location":"ai/pages/1d_cnn/","title":"1d cnn","text":""},{"location":"ai/pages/1d_cnn/#introduction-to-1d-convolutional-neural-networks-cnns","title":"Introduction to 1D Convolutional Neural Networks (CNNs)","text":""},{"location":"ai/pages/1d_cnn/#what-is-a-1d-cnn","title":"What is a 1D CNN?","text":"<p>A 1D Convolutional Neural Network (CNN) is a type of deep learning model designed to analyze sequential or time-series data. Unlike 2D CNNs, which process images, 1D CNNs extract patterns from 1D signals, such as:</p> <ul> <li>Sensor readings  </li> <li>Audio waveforms  </li> <li>Stock market data  </li> </ul>"},{"location":"ai/pages/1d_cnn/#why-use-a-1d-cnn","title":"Why Use a 1D CNN?","text":"<p>\u2705 Automatically detects patterns in sequences \u2705 Requires less computational power than RNNs/LSTMs \u2705 Works well for structured numerical sequences </p>"},{"location":"ai/pages/1d_cnn/#how-does-a-1d-cnn-work","title":"How Does a 1D CNN Work?","text":"<p>A 1D CNN processes sequential data using convolutional layers that apply filters across the input data. This allows the model to detect local patterns and relationships.</p>"},{"location":"ai/pages/1d_cnn/#key-components-of-a-1d-cnn","title":"Key Components of a 1D CNN","text":"<p>1\ufe0f\u20e3 Convolutional Layers (<code>Conv1D</code>)    - Extract patterns from input sequences    - Uses filters (kernels) to detect features  </p> <p>2\ufe0f\u20e3 Pooling Layers (<code>MaxPooling1D</code>)    - Downsamples the extracted features    - Reduces computation and prevents overfitting  </p> <p>3\ufe0f\u20e3 Flatten Layer (<code>Flatten</code>)    - Converts extracted features into a 1D vector for classification  </p> <p>4\ufe0f\u20e3 Fully Connected (<code>Dense</code>) Layers    - Learns the relationships between extracted features  </p> <p>5\ufe0f\u20e3 Output Layer    - Produces the final prediction  </p>"},{"location":"ai/pages/1d_cnn/#understanding-the-windowing-concept","title":"Understanding the Windowing Concept","text":"<p>Since sequential data involves time-dependent relationships, a single data point is not enough to make predictions. Instead, we use a window of past values as input.</p>"},{"location":"ai/pages/1d_cnn/#what-is-a-window","title":"What is a Window?","text":"<p>A window is a segment of the input sequence containing multiple past values. Instead of using one value, the model analyzes a small section of the sequence to detect patterns.</p>"},{"location":"ai/pages/1d_cnn/#when-to-use-a-window","title":"When to Use a Window?","text":"<p>\u2705 Use a window when:</p> <ul> <li>Data has sequential dependencies (e.g., time-series, sensor data, speech signals).</li> <li>You need to detect trends or short-term patterns over multiple time steps.</li> <li>Predictions depend on recent past values, not just the current value.</li> </ul> <p>\u274c Do not use a window when:</p> <ul> <li>Each data point is independent (e.g., static images, tabular data without a time factor).</li> <li>There is no meaningful temporal relationship between values.</li> <li>The model is expected to classify individual samples, not patterns over time.</li> </ul>"},{"location":"ai/pages/1d_cnn/#example-windowed-input","title":"Example: Windowed Input","text":"<p>If we use a window size of 5, our data might look like this:</p> Time Step Input Window Target t=1 [1, 2, 3, 4, 5] 6 t=2 [2, 3, 4, 5, 6] 7 t=3 [3, 4, 5, 6, 7] 8 <p>\u2705 The model learns patterns across the window instead of treating each value independently.</p>"},{"location":"ai/pages/1d_cnn/#1d-cnn-non-windowed-example","title":"1D CNN non-Windowed Example","text":"<p>This example takes in <code>.csv</code> EMG sensor data like this:</p> <pre><code>    timestamp   label   emg_value\n1   1739915829.7830825  NOISE   4\n2   1739915829.7873728  NOISE   3\n3   1739915829.7873728  NOISE   13\n14885   1739915840.9299445  COLLECTING  2\n14886   1739915840.930449   COLLECTING  3\n14887   1739915840.930449   COLLECTING  0\n</code></pre> <p>then proceesses and trains a 1-D CNN to predict, based on EMG data, if a desired movement (collecting) is occuring or not (noise).</p> <p>Key points:</p> <ul> <li>Increaseing the number of epochs increasing training passes and therefore accuracy but requires more training power/time</li> </ul> <pre><code>############################################\n# Preprocessing\n#\n# This model is a basic classification 1D CNN, it predicts based on the one specific memory input (EMG) at a time.\n# It does not take in consideration patterns, it is a Memoryless model.\n################\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nimport tensorflow as tf  # Deep learning framework\nfrom tensorflow import keras  # Keras API for model building\nfrom tensorflow.keras import layers  # Layers for the neural network\nimport matplotlib.pyplot as plt  # For visualizing training progress\n\n\n# Load the dataset\nfile_path = \"emg_data.csv\"  # Ensure this file is in your working directory\ndf = pd.read_csv(file_path)\n\n# Convert categorical labels to numerical values\nlabel_encoder = LabelEncoder()\ndf[\"label\"] = label_encoder.fit_transform(df[\"label\"])  # Example: NOISE -&gt; 0, MOVEMENT -&gt; 1 (sorts alphabetically)\n\n# Normalize the EMG values from 0 to 1\nscaler = MinMaxScaler()\ndf[\"emg_value\"] = scaler.fit_transform(df[[\"emg_value\"]])\n\n# Prepare features (X) and labels (y) for training a 1D Convolutional Neural Network (CNN).\nX = df[\"emg_value\"].values.reshape(-1, 1, 1)  # df[\"emg_value\"].values: Extracts the EMG signal values from the DataFrame.\n                                              # Convolutional Neural Networks (CNNs) expect a 3D input shape for time-series data: (batch_size, time_steps, features)\n                                              # -1 \u2192 Automatically infers the number of samples\n                                              # 1 \u2192 Each sample has one time step (since this is a single EMG value per row).\n                                              # 1 \u2192 There is only one feature per time step (the EMG value)\ny = df[\"label\"].values  # Labels (0 or 1) # Extracts the label column as a 1D NumPy array.\n                                          # It contains binary classification labels (0 for NOISE, 1 for MOVEMENT).\n\n# Split into train/test sets (80% train, 20% test)\n# X, y \u2192 The input features (X) and labels (y).\n# test_size=0.2 \u2192 Reserves 20% of the data for testing.\n# random_state=42 \u2192 Ensures the same split every time (for reproducibility).\n# stratify=y \u2192 Ensures that both the training and test sets maintain the same class distribution.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\n# This prints the number of samples in the training and test sets.\nprint(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\nprint(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n\n# Save the processed dataset (optional, this data is saved in memory within the notebook)\n# pd.DataFrame({\"emg_value\": X_train.flatten(), \"label\": y_train}).to_csv(\"train_data.csv\", index=False)\n# pd.DataFrame({\"emg_value\": X_test.flatten(), \"label\": y_test}).to_csv(\"test_data.csv\", index=False)\n\n\n##########################################################################################\n# Define a 1D Convolutional Neural Network (CNN) for binary classification\n###########################################################\nmodel = keras.Sequential([\n    # First Convolutional Layer\n    layers.Conv1D(filters=16, kernel_size=1, activation='relu', input_shape=(1, 1)),\n    # Conv1D applies a filter over 1D input (EMG values). Here, we use:\n    # - 16 filters (feature detectors)\n    # - Kernel size = 1 (since each EMG value is a single point)\n    # - ReLU activation function to introduce non-linearity\n    # - Input shape = (1, 1), meaning each sample consists of 1 EMG value\n\n    # First Max Pooling Layer\n    layers.MaxPooling1D(pool_size=1),\n    # MaxPooling downsamples the data, reducing overfitting and computation cost.\n\n    # Second Convolutional Layer\n    layers.Conv1D(filters=32, kernel_size=1, activation='relu'),\n    # - 32 filters to extract more complex features\n    # - Kernel size remains 1 since we are working with single values\n    # - ReLU activation again\n\n    # Second Max Pooling Layer\n    layers.MaxPooling1D(pool_size=1),\n    # Further downsampling the features extracted\n\n    # Flatten Layer\n    layers.Flatten(),\n    # Converts the 1D feature maps into a single vector for classification.\n\n    # Fully Connected (Dense) Layer\n    layers.Dense(64, activation='relu'),\n    # - 64 neurons for learning complex patterns in EMG values\n\n    # Dropout Layer (Regularization)\n    layers.Dropout(0.5),\n    # - Randomly disables 50% of neurons during training to prevent overfitting.\n\n    # Output Layer for Binary Classification\n    layers.Dense(1, activation='sigmoid')\n    # - 1 neuron output (since we have a binary classification)\n    # - Sigmoid activation outputs a probability (0 to 1), which is interpreted as:\n    #   - Close to 0 \u2192 Likely NOISE\n    #   - Close to 1 \u2192 Likely MOVEMENT\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',  # Adam optimizer for adaptive learning rate\n              loss='binary_crossentropy',  # Loss function for binary classification\n              metrics=['accuracy'])  # Track accuracy during training\n\n# Display model architecture summary\nmodel.summary()\n\n##########################################################################\n####  Train the CNN\n##################\n# Train the CNN model using training data\nhistory = model.fit(X_train,  # Training features (EMG values)\n                    y_train,  # Corresponding labels (0 or 1)\n                    epochs=10,  # Number of passes through the dataset\n                    batch_size=32,  # Number of samples processed before model updates weights\n                    validation_data=(X_test, y_test))  # Evaluate model on test data during training\n\n\n# Evaluate the trained model on the test dataset\ntest_loss, test_acc = model.evaluate(X_test, y_test)\n\n\n##########################################################################\n#### Evaluate the Model Performance\n#####################################\n# Print test accuracy (percentage of correct classifications)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n\n# Plot Training vs Validation Accuracy\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')  # X-axis: Epoch number\nplt.ylabel('Accuracy')  # Y-axis: Accuracy percentage\nplt.legend()  # Show legend\nplt.title('CNN Training Accuracy Over Epochs')  # Title of the plot\nplt.show()  # Display the plot\n\n# Plot Training vs Validation Loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')  # X-axis: Epoch number\nplt.ylabel('Loss')  # Y-axis: Loss value\nplt.title('CNN Training Loss Over Epochs')  # Title of the plot\nplt.show()  # Display the plot\n</code></pre>"},{"location":"ai/pages/1d_cnn/#1d-cnn-windowed-example","title":"1D CNN Windowed Example","text":"<p>This example processes the same data as the non-windowed example above, see the example for details.</p> <p>Key points:</p> <ul> <li>Changing <code>time_steps = XX</code> changes how many previous EMG values to use.</li> </ul> <pre><code># To pick up on signal patterns of sampled data, this model\n# processes a window (sequence) of EMG values instead of just one at a time.\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n#######################################################################\n# Load and Preprocess the Data\n####################################\n# Load the dataset\nfile_path = \"emg_data.csv\"  # Ensure this file is in your working directory\ndf = pd.read_csv(file_path)\n\n# Convert categorical labels to numerical values\nlabel_encoder = LabelEncoder()\ndf[\"label\"] = label_encoder.fit_transform(df[\"label\"])  # NOISE \u2192 0, MOVEMENT \u2192 1\n\n# Normalize the EMG values from 0 to 1\nscaler = MinMaxScaler()\ndf[\"emg_value\"] = scaler.fit_transform(df[[\"emg_value\"]])\n\n# Convert data to NumPy arrays\nX = df[\"emg_value\"].values\ny = df[\"label\"].values  # Labels (0 or 1)\n\n#######################################################################\n# Reshape Data to Include a Sequence of EMG Values\n####################################\n\n# Define how many previous EMG values to use in each input window\ntime_steps = 50  # Instead of using 1 EMG value, use a sequence of 10\n\n# Create sequences of EMG values\nX_seq = np.array([X[i:i+time_steps] for i in range(len(X) - time_steps)])\ny_seq = y[time_steps:]  # Align labels with the sequences\n\n# Reshape X_seq to fit CNN input requirements (samples, time_steps, features)\nX_seq = X_seq.reshape(X_seq.shape[0], time_steps, 1)\n\n# Split into training and testing sets again\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq)\n\n# Print new shapes\nprint(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n\n#############################################################\n# Develop CNN Architecture to Process Sequences\n###############################\nmodel = keras.Sequential([\n    # First Convolutional Layer - Detects patterns in time series data\n    layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(time_steps, 1)),\n    layers.MaxPooling1D(pool_size=2),\n\n    # Second Convolutional Layer - Captures more complex patterns\n    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n    layers.MaxPooling1D(pool_size=2),\n\n    # Flatten the 1D feature maps into a vector for classification\n    layers.Flatten(),\n\n    # Fully Connected Layer for decision making\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n\n    # Output Layer (Binary Classification)\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n\n################################################################\n# Train the CNN Model\n#######################\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n\n################################################################\n# Evaluate the Model\n#######################\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n################################################################\n# Visualize Training Performance\n#######################\n# Plot Training vs Validation Accuracy\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('CNN Training Accuracy Over Epochs')\nplt.show()\n\n# Plot Training vs Validation Loss\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('CNN Training Loss Over Epochs')\nplt.show()\n</code></pre>"},{"location":"ai/pages/1d_cnn/#final-thoughts","title":"Final Thoughts","text":"<p>1D CNNs are powerful tools for analyzing sequential data. They efficiently capture patterns over time using convolutional layers, making them useful for signal processing, forecasting, and classification tasks.</p> <p>Would you like to explore CNN + LSTM models for even better sequence learning? \ud83d\ude80</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/","title":"Cache-Aware Matrix Multiplication &amp; GPU Performance Analysis","text":"<p>This document explains a set of homework problems that analyze matrix multiplication performance in two scenarios: one on a CPU with a cache and one on a GPU. It includes both the problem statements and detailed reasoning behind each answer.</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#overview","title":"Overview","text":"<p>The homework explores:</p> <ul> <li>CPU-side matrix multiplication with memory and cache constraints.</li> <li>GPU-based matrix multiplication with thread and cache considerations.</li> </ul> <p>The problems examine how data layout, cache locality, and architectural parameters affect performance.</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#part-i-cpu-matrix-multiplication-with-cache","title":"Part I: CPU \u2013 Matrix Multiplication with Cache","text":"<pre><code>You experiment with a computer having a one-level data cache (128 Bytes) and a main memory \n(1K Bytes). You exclusively focus on data accesses instead of instruction access. The latencies (in \nCPU cycles) of the different kinds of accesses are as follows:  \nCache hit: 1 cycle; Cache miss: 110 cycles; Main memory access with cache disabled: 80 cycles; \nNow, considering the following matrix multiplication C=A\u00d7B, please answer the following questions \n(please show detailed steps)\n</code></pre>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#system-specifications","title":"System Specifications","text":"<ul> <li>Cache: One-level data cache with 128 Bytes.</li> <li>Main Memory: 1K Bytes.</li> <li>Focus: Data accesses (instruction accesses are ignored).</li> <li>Access Latencies:</li> <li>Cache hit: 1 cycle</li> <li>Cache miss: 110 cycles</li> <li>Main memory access (with cache disabled): 80 cycles</li> </ul>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#matrix-multiplication-problem","title":"Matrix Multiplication Problem","text":"<p>Given matrices:</p> <p>Matrix A:</p> \\[ A = \\begin{bmatrix} x_{0,0} &amp; x_{0,1} &amp; \\cdots &amp; x_{0,l} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{m,0} &amp; x_{m,1} &amp; \\cdots &amp; x_{m,l} \\end{bmatrix} \\] <p>Matrix B:</p> \\[ B = \\begin{bmatrix} y_{0,0} &amp; y_{0,1} &amp; \\cdots &amp; y_{0,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ y_{l,0} &amp; y_{l,1} &amp; \\cdots &amp; y_{l,n} \\end{bmatrix} \\] <p>The product is given by:</p> \\[ C = A \\times B. \\]"},{"location":"ai/pages/cpu-vs-gpu-matrix/#questions-and-explanations","title":"Questions and Explanations","text":""},{"location":"ai/pages/cpu-vs-gpu-matrix/#q1-what-is-the-dimension-of-the-matrix-c","title":"Q1. What is the dimension of the matrix \\( C \\)?","text":"<ul> <li>Explanation:   Since \\( A \\) is \\((m+1) \\times (l+1)\\) and \\( B \\) is \\((l+1) \\times (n+1)\\), the resulting matrix \\( C \\) has dimensions:   \\( (m+1) \\times (n+1) \\).</li> </ul>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q2-memory-constraints-maximum-l","title":"Q2. Memory Constraints &amp; Maximum \\( l \\)","text":"<pre><code>Raw question: For multiplication using ALU, assume you pre-load both A and B into the main memory \nfrom the storage and reserve space for C in the main memory to speed up the performance. \nIf pre-loading A and B takes 50% of the main memory space and reserved space for C takes 12.5% of \nthe main memory. All elements in A, B, and C have the same bitwidth. Assuming we know the \nvalue of m and (m+n) is a constant, then what is the maximum value of l?\nAnswer: l = 2(m+1) - 1 = 2m + 1.\n</code></pre> <ul> <li> <p>Problem Setup: </p> <ul> <li>Pre-loading \\( A \\) and \\( B \\) into main memory uses 50% of the memory.</li> <li>Reserving space for \\( C \\) uses 12.5% of the memory.</li> <li>All elements in \\( A \\), \\( B \\), and \\( C \\) have the same bitwidth.</li> <li>Assume \\( m+n \\) is constant.</li> </ul> </li> <li> <p>Explanation:</p> </li> </ul> <p>Since pre-loading A and B uses 50% of the memory while reserving space for \\( C \\) uses 12.5%, the ratio of memory allocated for (A+B) to \\( C \\) is 50:12.5, or 4:1. This means the total number of elements in A and B is four times the number of elements in \\( C \\). Given that every element has the same size:</p> <ul> <li>Matrix A has \\((m+1)(l+1)\\) elements.</li> <li>Matrix B has \\((l+1)(n+1)\\) elements.</li> <li>Matrix \\( C \\) has \\((m+1)(n+1)\\) elements.</li> </ul> <p>So, the relationship becomes:</p> <p>$$   (m+1)(l+1) + (l+1)(n+1) = 4\\,(m+1)(n+1).   $$</p> <p>Notice that both terms on the left share the factor \\((l+1)\\), so we can factor it out:</p> <p>$$   (l+1)\\Big[(m+1) + (n+1)\\Big] = (l+1)(m+n+2).   $$</p> <p>Using a calculator to solve for \\( l+1 \\), we get:</p> <p>$$   l+1 = \\frac{4\\,(m+1)(n+1)}{m+n+2}.   $$</p> <p>Since we assume \\( m+n \\) is constant and we're looking to maximize \\( l \\), the product \\((m+1)(n+1)\\) is maximized when \\( m = n \\). Setting \\( m = n \\) simplifies the expression (because \\( m+n+2 \\) becomes \\( 2(m+1) \\)), and we eventually find that:</p> <p>$$   l+1 = 2(m+1),   $$   which means:   $$   l = 2(m+1) - 1 = 2m+1.   $$</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q3-dimensions-with-16-bit-integers","title":"Q3. Dimensions with 16-bit Integers","text":"<pre><code>Raw question: Following the result from 2), If x is a 16-bit integer, what are the dimensions of A, B, and C?\nAnswer: (m=n=7; l=15)\n</code></pre> <p>Explanation:</p> <ul> <li>The problem states that 12.5% of the 1K bytes is reserved for matrix \\( C \\). Since each 16\u2011bit element is 2 bytes, \\( C \\) gets \\(128 \\div 2 = 64\\) elements.</li> <li>If \\( C \\) is square (which is optimal when \\( m \\) and \\( n \\) are equal), then:</li> </ul> <p>$$   (m+1)(n+1) = 64.   $$</p> <p>The simplest square factorization is \\(8 \\times 8\\), so \\( m+1 = 8 \\) and \\( n+1 = 8 \\), which gives \\( m = n = 7 \\).</p> <ul> <li>From Q2, we derived that \\( l = 2(m+1) - 1 \\). Plugging \\( m = 7 \\) into this gives:</li> </ul> <p>$$   l = 2 \\times 8 - 1 = 16 - 1 = 15.   $$</p> <p>Thus, we have:</p> <ul> <li>m = n = 7</li> <li>l = 15</li> </ul> <p>This means:</p> <ul> <li>Matrix A is \\(8 \\times 16\\),</li> <li>Matrix B is \\(16 \\times 8\\),</li> <li>Matrix C is \\(8 \\times 8\\).</li> </ul>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q4-total-memory-access-time","title":"Q4. Total Memory Access Time","text":"<pre><code>Raw question: Following the result from 3), assume that the cache is a fully associative cache with \nthe least recently used (LRU) replacement policy and the result can be directly written back to the main memory \nwithout sacrificing the memory read. What is the total memory access time (total cycles) for the matrix multiplication \nif we strictly follow the instructions below? Assume that we are using a 32B cache line.\n\nfor (int i=0; i&lt;=m; i++)\n{\n    for (int j=0; j&lt;=n; j++)\n    {\n         C(i,j) = A(i,:) * BT(:,j)\n    }\n}\nAnswer: Assume 32B cache line: 8\u00d7(1(A) + 8\u00d78(B)) = 65\u00d78 = 520 misses.\n</code></pre> <p>Explanation:</p> <p>For our matrix multiplication (with \\( m=7 \\), so 8 rows):</p> <ul> <li> <p>Matrix A:   Each row of \\( A \\) has 16 elements. Since each element is 2 bytes, one row occupies \\( 16 \\times 2 = 32 \\) bytes, exactly one cache line.   \u2192 Result: Loading one row of \\( A \\) causes 1 cache miss per row.</p> </li> <li> <p>Matrix B:   In the inner loop, elements from \\( B \\) (or its transposed version) are accessed in a less contiguous manner.   For each row of \\( C \\), the multiplication requires accessing an 8\u00d78 block from \\( B \\).   In the worst-case scenario, each element of this block might be in a different cache line, which would yield 64 cache misses per row.</p> </li> <li> <p>Total for each row of \\( C \\):   1 miss (for \\( A \\)) + 64 misses (for \\( B \\)) = 65 misses per row.</p> </li> <li> <p>With 8 rows, the total number of cache misses is:</p> </li> </ul> <p>$$   65 \\text{ misses/row} \\times 8 \\text{ rows} = 520 \\text{ misses}.   $$</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q5-leveraging-cache-locality","title":"Q5. Leveraging Cache Locality","text":"<pre><code>Raw question: How can you modify the approach to leverage the locality of the data in cache for improvement? \nHow much can you improve with your method? (Use Tiling, transpose, or other techniques as long as you can show the improvement.)\n</code></pre> <p>Explanation:</p> <p>Technique:   Use tiling (blocking) and/or transpose \\( B \\) to improve spatial and temporal locality.</p> <p>How it helps: </p> <ul> <li>Tiling/Blocking:     Break the matrices into smaller sub-blocks that fit entirely in the cache. Once a sub-block is loaded, you can reuse the data multiple times for computations, significantly reducing cache misses.</li> <li>Transposing \\( B \\):     This ensures that the memory accesses for \\( B \\) are more contiguous, further reducing the number of cache misses.</li> </ul> <p>Improvement:   Depending on the chosen block size, such techniques can reduce cache misses by an order of magnitude, making the matrix multiplication much more efficient.</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q6-average-memory-access-time-random-access","title":"Q6. Average Memory Access Time (Random Access)","text":"<pre><code>Raw question: After the computation, if we randomly access elements from A, B, and C continuously, what would the average memory access time (cycles per access) be?\n(Hit rate = 128/(512+128) \u2192 110\u00d74/5 + 1/5 = 88.2 cycles)\n</code></pre> <p>Explanation:</p> <ul> <li> <p>Calculation of Hit Rate:   The cache holds 128 Bytes out of a total of 512 + 128 = 640 Bytes that are being accessed.   So, the hit rate is \\( 128/640 = 20\\% \\) and the miss rate is \\( 80\\% \\).</p> </li> <li> <p>Average Memory Access Time:   With a hit taking 1 cycle and a miss taking 110 cycles, the average access time is:</p> </li> </ul> <p>$$   \\text{Average} = 0.8 \\times 110 + 0.2 \\times 1 \\approx 88.2 \\text{ cycles}.   $$</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q7-conclusion-on-cache-usefulness","title":"Q7. Conclusion on Cache Usefulness","text":"<pre><code>Raw question: Based on the observation from Q6 and the memory access time given before, what can you conclude?\n(88.2 &gt; 80 \u2192 cache is useless if there is no locality)\n</code></pre> <p>Explanation: The average access time when randomly accessing memory (88.2 cycles) is higher than the 80 cycles required for a main memory access when the cache is disabled.</p> <p>Conclusion: Without proper data locality, the cache not only fails to provide benefits but may actually hurt performance. This shows that effective use of the cache (via locality improvements) is critical for performance gains.</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#part-2-gpu-matrix-multiplication","title":"Part 2: GPU Matrix Multiplication","text":"<p>In this section, we analyze matrix multiplication on a GPU. The problem assumes that all elements in matrices A, B, and C are 2-byte values. We use the following GPU characteristics to answer the questions:</p> <ul> <li>Element Bitwidth: 2 Bytes.</li> <li> <p>GPU Specifications:</p> <ul> <li>2 Stream Multiprocessors (SM).</li> <li>Each SM has 2 warp schedulers.</li> <li>Each warp scheduler manages 4 warps.</li> <li>Each warp contains 32 threads.</li> </ul> </li> </ul>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#questions-and-explanations_1","title":"Questions and Explanations","text":""},{"location":"ai/pages/cpu-vs-gpu-matrix/#q1-thread-requirement","title":"Q1. Thread Requirement","text":"<pre><code>Raw question: If you use one thread to compute each element in the matrix C, how many threads should you request in your program code?\nAnswer: (m+1)(n+1)\n</code></pre> <p>Explanation: Since each element of C is computed by a single thread, the total number of threads equals the number of elements in C, which is \\((m+1)(n+1)\\).</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q2-maximizing-gpu-resource-utilization","title":"Q2. Maximizing GPU Resource Utilization","text":"<pre><code>Raw question: Assume that the GPU has 2 Stream Multiprocessors (SM), each SM has 2 warp schedulers, each warp scheduler is in charge of 4 warps, and each warp contains 32 threads. What are the possible dimensions of C that can maximize GPU resource utilization? Please provide at least four different dimensions.\nAnswer (professor\u2019s interpretation): 2 \u00d7 2 \u00d7 32 = 128 \u2192 (i.e. 128\u00d7... threads)\n</code></pre> <p>Explanation:</p> <p>Even though the GPU specifications mention that each warp scheduler manages 4 warps, in practice only one warp per scheduler is active at any given cycle. This means that each warp scheduler effectively handles only 32 threads concurrently.</p> <p>Thus, the total number of threads that can be scheduled at the same time is:</p> \\[ 2 \\text{ SMs} \\times 2 \\text{ warp schedulers per SM} \\times 32 \\text{ threads per scheduler} = 128 \\text{ threads.} \\] <p>To maximize GPU utilization, the total number of threads (i.e., the total elements in matrix C) should be a multiple of 128. For example, dimensions of C such that \\((m+1)(n+1)\\) equals 128, 256, 384, etc., would be optimal.</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q3-minimum-dimension-for-square-matrix-c","title":"Q3. Minimum Dimension for Square Matrix \\( C \\)","text":"<pre><code>Raw question: Following the settings in Q2, if m = n, what is the minimum dimension of C?\nAnswer: (m+1)(n+1) = 256 \u2192 m = n = 15.\n</code></pre> <p>Explanation: For a square matrix \\( C \\), the total number of elements is given by \\((m+1)^2\\) (since there are \\(m+1\\) rows and \\(m+1\\) columns). To align with a convenient GPU thread count, we want the number of elements to be 256. Setting up the equation:</p> \\[ (m+1)^2 = 256, \\] <p>we take the square root of both sides to obtain:</p> \\[ m+1 = 16. \\] <p>This immediately gives:</p> \\[ m = 15. \\] <p>Since \\( m = n \\) for a square matrix, we also have \\( n = 15 \\). Thus, matrix \\( C \\) is \\( 16 \\times 16 \\) (since \\(m+1 = 16\\) and \\(n+1 = 16\\)), which provides exactly 256 elements, matching the desired GPU thread count.</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q4-minimum-l1-cache-size-per-sm","title":"Q4. Minimum L1 Cache Size per SM","text":"<pre><code>Raw question: Following the result from Q3, assume that each SM has its own L1 and L2 caches and the L2 cache is large enough to preload all necessary data. Let\u2019s further assume that m = n = l. If each L1 cache only allows up to 80% of its space for data buffering, what is the minimum size of L1 that can accommodate all necessary data for each warp operation without causing any runtime L1 cache miss? (Hint: Consider both inputs and outputs.)\nAnswer: 960 Bytes.\n</code></pre> <p>Explanation:</p> <p>For each warp operation, the algorithm must have the necessary data buffered in the cache. Here\u2019s how we determine the numbers:</p> <ol> <li> <p>Matrix C:    A warp consists of 32 threads, and each thread computes one output element of matrix \\( C \\). To support both reading and writing (or double-buffering) for \\( C \\), we need space for:    $$    2 \\times 32 = 64 \\text{ elements}.    $$</p> </li> <li> <p>Matrix A:    Similarly, each thread uses one element from matrix \\( A \\) as an input. Doubling this for buffering gives:    $$    2 \\times 32 = 64 \\text{ elements}.    $$</p> </li> <li> <p>Matrix B:    The tiling strategy for matrix \\( B \\) requires a block of data sized \\( 16 \\times 16 \\) (from the square matrix assumption when \\( m = n = l \\)), which equals:    $$    256 \\text{ elements}.    $$</p> </li> </ol> <p>Adding these together, the total number of elements that must be buffered for one warp operation is:</p> \\[ 64 \\text{ (for \\( C \\))} + 64 \\text{ (for \\( A \\))} + 256 \\text{ (for \\( B \\))} = 384 \\text{ elements}. \\] <p>Since each element is 2 bytes, the total data size is: $$ 384 \\times 2 = 768 \\text{ bytes}. $$</p> <p>However, only 80% of the L1 cache is available for data buffering. To ensure that 768 bytes fit into just 80% of the cache, we need the L1 cache to be at least: $$ \\frac{768 \\text{ bytes}}{0.8} = 960 \\text{ bytes}. $$</p> <p>Thus, the minimum L1 cache size required for each warp operation is 960 bytes.</p>"},{"location":"ai/pages/cpu-vs-gpu-matrix/#q5-delay-due-to-compulsory-cache-misses","title":"Q5. Delay Due to Compulsory Cache Misses","text":"<pre><code>Raw question: Following the result from Q4, assume the cache has four ports that can concurrently support four data transfers between L2 and L1. The cache line is 32B and an L1 cache miss takes 20 cycles. What is the delay of compulsory misses for the matrix multiplication C = A\u00d7B?\nAnswer: For (m+1)(n+1) = 256, the compulsory miss with a single port is estimated as:\n16\u00d716 (matrix dimension) \u00d7 (2B/32B) \u00d7 (1 (for B) + \u00bd (for A) + \u00bd (for C)) \u00d7 20 cycles = 640 cycles. With four ports, the delay is 640/4 = 160 cycles.\n</code></pre> <p>Explanation:</p> <ol> <li> <p>Matrix Dimension:    With \\((m+1)(n+1)=256\\), matrix \\(C\\) is \\(16 \\times 16\\).</p> </li> <li> <p>Cache Line Usage:    Each element is 2 bytes and each cache line is 32 bytes, so one cache line holds \\(32/2=16\\) elements.    Therefore, the fraction of a cache line used per element is \\(\\frac{2 \\text{ B}}{32 \\text{ B}}\\).</p> </li> <li> <p>Weighted Data Requirements: </p> <p>For matrix \\(B\\), every element is loaded fresh (weight 1).  </p> <p>For matrices \\(A\\) and \\(C\\), only about half of the required data is loaded on a compulsory miss (weight \\(1/2\\) each). Thus, the combined weight is:  </p> \\[ 1 \\;(\\text{for } B) + \\frac{1}{2} \\;(\\text{for } A) + \\frac{1}{2} \\;(\\text{for } C) = 2. \\] </li> <li> <p>Calculating Misses:    Multiply the total number of elements in \\(C\\) (which is \\(16 \\times 16\\)) by the fraction of a cache line per element and then by the weight:    $$    16 \\times 16 \\times \\left(\\frac{2 \\text{ B}}{32 \\text{ B}}\\right) \\times 2.    $$    Here, \\(16 \\times 16 = 256\\) elements, and \\(\\frac{2}{32} = \\frac{1}{16}\\), so:    $$    256 \\times \\frac{1}{16} \\times 2 = 16 \\times 2 = 32 \\text{ cache line misses}.    $$</p> </li> <li> <p>Cycle Delay:    Each cache miss costs 20 cycles, so with one port, the delay is:    $$    32 \\times 20 = 640 \\text{ cycles}.    $$</p> </li> <li> <p>Parallel Cache Ports:    With 4 cache ports working concurrently, the delay is divided by 4:    $$    \\frac{640}{4} = 160 \\text{ cycles}.    $$</p> </li> </ol> <p>Thus, the delay due to compulsory cache misses is 160 cycles.</p>"},{"location":"ai/pages/cuda/","title":"CUDA C/C++ Programming Notes","text":""},{"location":"ai/pages/cuda/#overview","title":"Overview","text":"<p>What is CUDA? CUDA is a parallel computing architecture developed by NVIDIA that allows developers to leverage GPU acceleration for general-purpose computing, not just graphics.</p> <p>Key points:</p> <ul> <li>Exposes GPU parallelism for general-purpose computing while retaining high performance.</li> <li>Based on industry-standard C/C++ with a small set of extensions, like <code>Malloc() -&gt; cudaMalloc()</code> and <code>Memcpy() -&gt; cudaMemcpy()</code></li> <li>Provides straightforward APIs to manage devices, memory, and kernels.</li> <li>Enables writing and launching CUDA kernels, managing GPU memory, and handling synchronization between the CPU (host) and GPU (device).</li> </ul> <p>Difference between host and device:</p> <ul> <li>Host: CPU</li> <li>Device: GPU</li> </ul> <p>Using <code>__global__</code> to declare a function as device code:</p> <ul> <li>Executes on the device (GPU)</li> <li>Called from the host (CPU)</li> </ul>"},{"location":"ai/pages/cuda/#heterogeneous-computing","title":"Heterogeneous Computing","text":""},{"location":"ai/pages/cuda/#host-and-device-relationship","title":"Host and Device relationship","text":"<ul> <li>Host: The CPU and its associated memory; executes serial portions of code.</li> <li>Device: The GPU and its dedicated memory; executes parallel portions of code efficiently.</li> </ul>"},{"location":"ai/pages/cuda/#simple-processing-flow","title":"Simple Processing Flow","text":""},{"location":"ai/pages/cuda/#hello-world","title":"Hello World","text":""},{"location":"ai/pages/cuda/#host-cpu-example","title":"Host (CPU) Example","text":"<p>A simple C program running entirely on the CPU:</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n    printf(\"Hello World!\\n\");\n    return 0;\n}\n</code></pre> <ul> <li>Standard C code executed by the CPU.</li> <li>Compiled using a standard C compiler like <code>gcc</code>.</li> <li>Outputs <code>Hello World!</code> directly.</li> </ul>"},{"location":"ai/pages/cuda/#cuda-device-gpu-example","title":"CUDA Device (GPU) Example","text":"<p>A basic CUDA C program that launches a kernel (device function) on the GPU:</p> <pre><code>#include &lt;stdio.h&gt;\n\n// Device kernel (does nothing here)\n__global__ void mykernel(void) {\n}\n\nint main(void) {\n    // Launch kernel on the GPU\n    mykernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;();\n\n    // Host code continues\n    printf(\"Hello World!\\n\");\n    return 0;\n}\n</code></pre> <ul> <li><code>__global__</code> keyword specifies a CUDA kernel (runs on GPU, called from CPU).</li> <li>Triple angle brackets <code>&lt;&lt;&lt; &gt;&gt;&gt;</code> indicate kernel launch parameters (1 block, 1 thread).</li> <li>Requires NVIDIA's CUDA compiler (<code>nvcc</code>) to compile.</li> <li>Kernel launch does nothing visible here; real work happens inside device code.</li> <li>Still prints <code>Hello World!</code> from the CPU (host).</li> </ul>"},{"location":"ai/pages/cuda/#addition-on-the-device","title":"Addition on the Device","text":"<p>A basic CUDA C program performing addition on the GPU:</p> <pre><code>#include &lt;stdio.h&gt;\n\n// Device kernel for addition\n__global__ void add(int *a, int *b, int *c) {\n    *c = *a + *b;\n}\n\nint main(void) {\n    int a = 2, b = 7, c;\n    int *d_a, *d_b, *d_c;\n\n    // Allocate memory on GPU\n    cudaMalloc((void **)&amp;d_a, sizeof(int));\n    cudaMalloc((void **)&amp;d_b, sizeof(int));\n    cudaMalloc((void **)&amp;d_c, sizeof(int));\n\n    // Copy values to GPU\n    cudaMemcpy(d_a, &amp;a, sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, &amp;b, sizeof(int), cudaMemcpyHostToDevice);\n\n    // Launch kernel on GPU\n    add&lt;&lt;&lt;1, 1&gt;&gt;&gt;(d_a, d_b, d_c);\n\n    // Copy result back to CPU\n    cudaMemcpy(&amp;c, d_c, sizeof(int), cudaMemcpyDeviceToHost);\n\n    // Free GPU memory\n    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n\n    printf(\"Sum: %d\\n\", c);\n    return 0;\n}\n</code></pre> <ul> <li><code>__global__</code> defines a kernel function to execute on the GPU.</li> <li>Uses CUDA-specific functions for memory management (<code>cudaMalloc</code>, <code>cudaMemcpy</code>, <code>cudaFree</code>).</li> <li>Kernel launch syntax (<code>&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code>) specifies execution configuration: 1 block, 1 thread.</li> <li>Data must be explicitly transferred between host (CPU) and device (GPU).</li> <li>Compiled with NVIDIA's CUDA compiler (<code>nvcc</code>).</li> </ul>"},{"location":"ai/pages/cuda/#vector-addition-on-the-device","title":"Vector Addition on the Device","text":""},{"location":"ai/pages/cuda/#device-kernel","title":"Device Kernel","text":"<pre><code>__global__ void add(int *a, int *b, int *c) {\n    c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x];\n}\n</code></pre> <ul> <li>Each block handles one element of the addition.</li> <li>Blocks run in parallel on the GPU.</li> </ul> <p>Example of Parallel Execution:</p> <ul> <li>Block 0: <code>c[0] = a[0] + b[0];</code></li> <li>Block 1: <code>c[1] = a[1] + b[1];</code></li> <li>Block 2: <code>c[2] = a[2] + b[2];</code></li> <li>Block 3: <code>c[3] = a[3] + b[3];</code></li> </ul>"},{"location":"ai/pages/cuda/#host-code-example","title":"Host Code Example","text":"<pre><code>#define N 512\n\nint main(void) {\n    int *a, *b, *c; // host copies of a, b, c\n    int *d_a, *d_b, *d_c; // device copies of a, b, c\n    int size = N * sizeof(int);\n\n    // Allocate space for device copies\n    cudaMalloc((void **)&amp;d_a, size);\n    cudaMalloc((void **)&amp;d_b, size);\n    cudaMalloc((void **)&amp;d_c, size);\n\n    // Allocate space for host copies and setup input values\n    a = (int *)malloc(size); random_ints(a, N);\n    b = (int *)malloc(size); random_ints(b, N);\n    c = (int *)malloc(size);\n\n    // Copy inputs to device\n    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n\n    // Launch add() kernel on GPU with N blocks\n    add&lt;&lt;&lt;N, 1&gt;&gt;&gt;(d_a, d_b, d_c);\n\n    // Copy result back to host\n    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n\n    // Cleanup\n    free(a); free(b); free(c);\n    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n\n    return 0;\n}\n</code></pre> <ul> <li>Allocates and manages memory explicitly.</li> <li>Each GPU block computes one addition in parallel.</li> <li>Data transferred between host and GPU explicitly.</li> </ul> <p>Below is a new section on Indexing that integrates the concepts covered on pages 37\u201348:</p>"},{"location":"ai/pages/cuda/#indexing","title":"Indexing","text":"<p>Efficient indexing is key to mapping threads to data elements in CUDA. In CUDA\u2019s parallel programming model, each thread in a grid must be assigned a unique portion of the data. This section outlines the common strategies and formulas used for indexing in CUDA kernels.</p>"},{"location":"ai/pages/cuda/#basic-indexing-concepts","title":"Basic Indexing Concepts","text":"<p>CUDA organizes threads into blocks, and blocks form a grid. Each thread within a block is uniquely identified by <code>threadIdx</code>, while each block in the grid is identified by <code>blockIdx</code>.</p> <p></p> <p>Built-in Variables: </p> <ul> <li><code>threadIdx.x</code>: The thread\u2019s index within its block (in the x-dimension).  </li> <li><code>blockIdx.x</code>: The block\u2019s index within the grid (in the x-dimension).  </li> <li><code>blockDim.x</code>: The number of threads in a block (in the x-dimension). In the example above <code>blockDim.x = 8</code></li> </ul> <p>The Indexing Formula: When processing one-dimensional arrays, the unique global index for each thread is calculated using:</p> <pre><code>int index = threadIdx.x + blockIdx.x * blockDim.x;\n</code></pre>"},{"location":"ai/pages/cuda/#practical-example","title":"Practical Example","text":"<p>Consider a kernel launch where each block contains 8 threads. For a thread with:</p> <ul> <li><code>threadIdx.x = 5</code> </li> <li><code>blockIdx.x = 2</code> </li> <li><code>M = blockDim.x = 8</code></li> </ul> <p></p> <p>This means that the thread processes the 22nd element (using zero-based indexing) of the array.</p>"},{"location":"ai/pages/cuda/#multi-dimensional-indexing","title":"Multi-Dimensional Indexing","text":"<p>For higher-dimensional data, CUDA supports three-dimensional configurations:</p> <p>2D Indexing: When working with images or matrices, you may compute a unique index using both x and y dimensions:</p> <pre><code>int col = threadIdx.x + blockIdx.x * blockDim.x;\nint row = threadIdx.y + blockIdx.y * blockDim.y;\nint index = row * width + col; // 'width' is the number of columns\n</code></pre> <p>3D Indexing: </p> <p>For volume data, a similar approach extends to three dimensions using <code>threadIdx.z</code>, <code>blockIdx.z</code>, and <code>blockDim.z</code>.</p>"},{"location":"ai/pages/cuda/#handling-arbitrary-vector-sizes","title":"Handling Arbitrary Vector Sizes","text":"<p>Often, the total number of data elements is not an exact multiple of the block size. To safely process all elements without accessing out-of-bound memory, include a bounds check in your kernel:</p> <pre><code>__global__ void add(int *a, int *b, int *c, int n) {\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index &lt; n) {\n        c[index] = a[index] + b[index];\n    }\n}\n</code></pre> <ul> <li>Kernel Launch:   The number of blocks is computed to cover all elements:</li> </ul> <pre><code>  add&lt;&lt;&lt;(n + blockDim.x - 1) / blockDim.x, blockDim.x&gt;&gt;&gt;(d_a, d_b, d_c, n);\n</code></pre> <p>Below is a new section that covers shared memory, 1D stencils, and the use of __syncthreads() in CUDA. You can integrate this section into your notes as follows:</p>"},{"location":"ai/pages/cuda/#shared-memory-1d-stencil-and-__syncthreads","title":"Shared Memory, 1D Stencil, and __syncthreads()","text":""},{"location":"ai/pages/cuda/#overview_1","title":"Overview","text":"<p>CUDA provides a fast, user-managed on-chip memory called shared memory that all threads within a block can access. This section demonstrates how to leverage shared memory for a 1D stencil operation and explains the role of the synchronization primitive <code>__syncthreads()</code>.</p>"},{"location":"ai/pages/cuda/#shared-memory-basics","title":"Shared Memory Basics","text":"<ul> <li> <p>Definition:   Shared memory is declared with the <code>__shared__</code> keyword. It is a limited, high-speed memory region accessible by all threads within the same block.</p> </li> <li> <p>Purpose:   It is used to cache data from global memory to reduce redundant memory accesses, thereby improving performance\u2014especially in operations that reuse data, such as stencil computations.</p> </li> </ul>"},{"location":"ai/pages/cuda/#the-1d-stencil-operation","title":"The 1D Stencil Operation","text":"<p>A stencil operation processes each element of an input array by combining it with its neighboring elements. For example, with a radius of 3, each output element is computed by summing 7 consecutive elements from the input array.</p> <p></p>"},{"location":"ai/pages/cuda/#example-1d-stencil-kernel","title":"Example: 1D Stencil Kernel","text":"<pre><code>#define BLOCK_SIZE 16\n#define RADIUS 3\n\n__global__ void stencil_1d(int *in, int *out) {\n    // Declare shared memory with additional space for halo elements\n    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n\n    // Calculate global and local indices\n    int gindex = threadIdx.x + blockIdx.x * blockDim.x;\n    int lindex = threadIdx.x + RADIUS;\n\n    // Load the main data element into shared memory\n    temp[lindex] = in[gindex];\n\n    // Load halo elements (neighbors) needed for the stencil\n    if (threadIdx.x &lt; RADIUS) {\n        temp[lindex - RADIUS] = in[gindex - RADIUS];\n        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n    }\n\n    // Synchronize to ensure all shared memory loads are complete\n    __syncthreads();\n\n    // Apply the stencil: sum the element and its neighbors\n    int result = 0;\n    for (int offset = -RADIUS; offset &lt;= RADIUS; offset++) {\n        result += temp[lindex + offset];\n    }\n\n    // Write the result back to global memory\n    out[gindex] = result;\n}\n</code></pre>"},{"location":"ai/pages/cuda/#how-the-kernel-works","title":"How the Kernel Works","text":"<ul> <li>Data Loading:   Each thread loads one data element from global memory into a designated location in the shared memory array (<code>temp</code>). Threads with an index less than <code>RADIUS</code> also load extra \"halo\" elements from the boundaries. These halo elements provide the necessary neighboring data for the stencil computation.</li> </ul> <ul> <li> <p>Synchronization with __syncthreads():   The call to <code>__syncthreads()</code> is essential because it ensures that all threads have finished loading their data into shared memory before any thread begins processing. This barrier prevents data races, ensuring that every thread sees the correct, complete data.</p> </li> <li> <p>Stencil Computation:   After synchronization, each thread applies the stencil by summing the element at its local index (<code>lindex</code>) along with its surrounding elements within the radius. The result is then written back to global memory.</p> </li> </ul>"},{"location":"ai/pages/cuda/#the-role-of-__syncthreads","title":"The Role of __syncthreads()","text":"<ul> <li> <p>Barrier for Threads: <code>__syncthreads()</code> acts as a barrier where all threads in a block must reach before any can proceed. This is crucial when threads depend on data loaded by other threads.</p> </li> <li> <p>Preventing Data Hazards:   It prevents race conditions by ensuring that shared memory writes are completed before any thread begins reading the data for computation.</p> </li> <li> <p>Uniform Execution Requirement:   All threads within a block must execute the same <code>__syncthreads()</code> call (i.e., it should not be placed inside a condition that may evaluate differently across threads) to avoid deadlocks.</p> </li> </ul>"},{"location":"ai/pages/cuda/#practical-considerations","title":"Practical Considerations","text":"<ul> <li> <p>Handling Boundaries:   When the total number of elements does not neatly divide into blocks, additional boundary checks may be necessary to prevent out-of-bounds memory access.</p> </li> <li> <p>Performance Optimization:   Efficient use of shared memory can significantly reduce global memory accesses, especially in stencil operations where the same data may be reused by multiple threads.</p> </li> </ul> <p>Below is a new section that covers managing the device, based on the content from the provided CUDA slides (ppt pages 60+):</p>"},{"location":"ai/pages/cuda/#managing-the-device","title":"Managing the Device","text":""},{"location":"ai/pages/cuda/#overview_2","title":"Overview","text":"<p>CUDA applications must manage GPU devices effectively to ensure optimal performance, especially in systems with multiple GPUs. This section describes how to query available devices, select the appropriate device for execution, retrieve device properties, and perform advanced operations like peer-to-peer memory copies.</p>"},{"location":"ai/pages/cuda/#querying-and-selecting-devices","title":"Querying and Selecting Devices","text":"<ul> <li> <p>Querying Devices:   Use <code>cudaGetDeviceCount(int *count)</code> to determine the number of CUDA-capable GPUs in your system. This helps in dynamically adjusting your application to the available hardware.</p> </li> <li> <p>Selecting a Device:   Once you know the available devices, use <code>cudaSetDevice(int device)</code> to select the GPU on which your kernels will run. This is essential in multi-GPU systems where you might want to distribute workloads.</p> </li> <li> <p>Getting the Current Device:   Use <code>cudaGetDevice(int *device)</code> to find out which device is currently active.</p> </li> <li> <p>Retrieving Device Properties:   To get detailed information about a GPU\u2014such as its memory size, number of registers, clock rate, and compute capability\u2014use:</p> </li> </ul> <p><code>c   cudaDeviceProp prop;   cudaGetDeviceProperties(&amp;prop, device);</code></p> <p>This information can be used to optimize your code for the specific hardware.</p>"},{"location":"ai/pages/cuda/#advanced-device-management","title":"Advanced Device Management","text":"<ul> <li> <p>Multi-threading and Device Sharing:   Multiple host threads can share a single GPU, and a single thread can manage multiple GPUs by switching between them using successive calls to <code>cudaSetDevice</code>.</p> </li> <li> <p>Peer-to-Peer Memory Copies:   CUDA supports direct memory copies between GPUs with <code>cudaMemcpy(...)</code>, provided that the operating system and the GPUs support peer-to-peer transfers. This can further enhance performance by bypassing the host memory for inter-device communication.</p> </li> </ul>"},{"location":"ai/pages/cuda/#synchronization-and-error-handling","title":"Synchronization and Error Handling","text":"<ul> <li> <p>Synchronizing Host and Device:   Kernel launches are asynchronous by default. Use <code>cudaDeviceSynchronize()</code> to block the host until all preceding CUDA calls have completed. This is crucial before copying results back to the host or when precise timing is required.</p> </li> <li> <p>Error Reporting:   Every CUDA API call returns an error code. After making an API call, check for errors using:</p> </li> </ul> <p><code>c   cudaError_t err = cudaGetLastError();   if (err != cudaSuccess) {       printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));   }</code></p> <p>This practice helps in diagnosing issues related to device management and kernel execution.</p>"},{"location":"ai/pages/cuda/#conclusion","title":"Conclusion","text":"<p>Managing the device in CUDA involves more than simply launching kernels. By querying device capabilities, selecting the appropriate GPU, and handling synchronization and errors properly, you ensure that your application fully leverages the available hardware resources for optimal performance.</p>"},{"location":"ai/pages/hardware/","title":"AI Hardware Overview","text":""},{"location":"ai/pages/hardware/#instructor","title":"Instructor","text":"<p>Notes are mainy derived from Dr. Chen Pan's Hardware and programming course at UTSA.</p> <ul> <li>Instructor Email: chen.pan@utsa.edu</li> </ul>"},{"location":"ai/pages/hardware/#resources","title":"Resources","text":"<ul> <li>Computer Architecture: A Quantitative Approach 6th Edition</li> </ul>"},{"location":"ai/pages/hardware/#overview","title":"Overview","text":"<ul> <li>Understanding microarchitecture of various AI hardware, parallel computing, and deep learning</li> <li>Low level (C\\C++) programming for executing AI algorithms</li> <li>Bridge the gap between hardware and software in AI.</li> </ul>"},{"location":"ai/pages/hardware/#computational-limits-in-ai","title":"Computational Limits in AI","text":"<p>AI differs from traditional sequential code in that it relies heavily on parallelism and processing large datasets, making operations like matrix multiplication central to tasks like training and inference. While sequential code (found in most classical microcontrollers/microcomputers) executes one instruction at a time, AI performs millions of computations simultaneously, requiring hardware like GPUs or TPUs for efficient parallel processing. AI also demands high memory bandwidth, low latency, and specialized accelerators to handle its computational intensity, exceeding the capabilities of traditional architectures designed for sequential tasks.</p>"},{"location":"ai/pages/hardware/#the-von-neumann-bottleneck","title":"The Von Neumann bottleneck","text":"<p>The Von Neumann architecture is a foundational computing model that organizes a computer into three main components: a CPU (central processing unit), memory (for storing data and instructions), and I/O devices. It operates by fetching instructions from memory, decoding them, and executing them sequentially, which is known as the fetch-decode-execute cycle. While the Von Neumann architecture underpins most general-purpose computers, its inherent bottleneck\u2014limited bandwidth between the CPU and memory (the \"Von Neumann bottleneck\")\u2014poses challenges for AI workloads, which often require high-speed processing of massive datasets. To address this, specialized AI hardware, such as GPUs, TPUs, and neuromorphic chips, is designed to parallelize computations and overcome memory bottlenecks, enabling efficient training and inference for complex AI models. Thus, while AI builds on the principles of the Von Neumann architecture, it increasingly relies on adaptations and specialized architectures optimized for its unique demands.</p> <p></p>"},{"location":"ai/pages/hardware/#the-memory-performace-gap","title":"The Memory Performace Gap","text":"<p>The memory performance gap is the mismatch between CPU speed and slower memory access, creating a bottleneck for AI workloads that rely on fast data processing. AI hardware mitigates this with high-bandwidth memory (HBM), larger caches, and optimized architectures like GPUs and TPUs, which improve memory throughput and enable efficient parallel processing of large datasets.</p> <p> Source: researchgate</p>"},{"location":"ai/pages/hardware/#memory-bound-vs-compute-bound","title":"Memory-bound vs Compute-bound","text":"<p>Memory-Bound tasks are limited by the speed of data transfer between memory and the processor, causing the CPU or GPU to wait for data. Examples include fetching large datasets or model weights in AI.</p> <p>Compute-Bound tasks are limited by the processor's computational power, with the CPU or GPU fully utilized for calculations. Examples include matrix multiplications in deep learning.</p> <p>In AI, memory-bound tasks benefit from faster memory or bandwidth, while compute-bound tasks require more powerful processors or accelerators.</p>"},{"location":"ai/pages/hardware/#the-power-gap","title":"The Power Gap","text":"<p>Battery technology is not keeping up to power requirments of embedded systems. To combat this, energy harvesting alternatives like solar, RF, heat, or kinetic energy harvesting are being explored.</p>"},{"location":"ai/pages/hardware/#performance-trends-bandwidth-over-latency","title":"Performance Trends: Bandwidth Over Latency","text":"<p>Modern AI and computing workloads are increasingly bandwidth-bound rather than latency-bound. While bandwidth improvements have scaled exponentially (400\u201332,000\u00d7), latency improvements have been much slower (8\u201391\u00d7). This discrepancy requires efficient data movement strategies in hardware design.</p> <p>To address this, modern AI architectures emphasize:</p> <ul> <li>Parallelism: Increasing the number of processing units (e.g., GPUs, TPUs, multi-core CPUs).</li> <li>Memory Hierarchies: Using multi-level caches and HBM to reduce memory access delays.</li> <li>Interconnect Optimization: Using high-speed data buses, PCIe, and NVLink to improve bandwidth efficiency.</li> <li>Data Locality Strategies: Prefetching and reusing data efficiently to minimize slow memory accesses.</li> </ul> <p></p>"},{"location":"ai/pages/hardware/#cost-vs-energy-efficiency-in-ai-hardware","title":"Cost vs. Energy Efficiency in AI Hardware","text":"<p>In large-scale AI computing, energy efficiency is now as important as performance. As operational costs (power and cooling) exceed hardware costs in cloud data centers, AI accelerators must balance power efficiency with computational throughput. Key trends include:</p> <ul> <li>Energy-efficient accelerators: TPUs and domain-specific processors reduce redundant computation.</li> <li>Dark Silicon Awareness: Modern chips often have unused transistors due to power constraints, requiring careful workload distribution.</li> <li>Performance per Watt: AI hardware is increasingly evaluated based on tasks per joule rather than raw FLOPS.</li> </ul>"},{"location":"ai/pages/hardware/#computer-architecture-basics","title":"Computer Architecture Basics","text":""},{"location":"ai/pages/hardware/#instruction-sets","title":"Instruction Sets","text":"<p>RISC (Reduced Instruction Set Computing) and CISC (Complex Instruction Set Computing) are two CPU architecture approaches that impact instruction execution, power efficiency, and complexity.</p> Feature RISC (Reduced Instruction Set Computing) CISC (Complex Instruction Set Computing) Instruction Set Small, fixed-length, simple Large, variable-length, complex Execution One or few cycles per instruction Some instructions take multiple cycles Memory Access Load/Store model (separate memory instructions) Memory operations embedded in instructions Code Size More instructions but simpler Fewer instructions but more complex Pipelining Easier to implement More difficult due to complex decoding Power Efficiency More efficient, ideal for mobile and embedded devices Higher power consumption, used in desktops and servers Examples ARM, RISC-V, PowerPC x86 (Intel, AMD), IBM System/360"},{"location":"ai/pages/hardware/#old-vs-real-computer-architecture","title":"Old vs. Real Computer Architecture","text":"<p>Computing architectures can be categorized into old (theoretical) models and real (practical) implementations. Old architectures define foundational principles, while real architectures build upon them with modern enhancements.</p> Architecture Type Description Examples Old Architectures Conceptual models shaping computing principles, often with limitations in speed and memory efficiency. Von Neumann (shared memory for data/instructions), Harvard (separate memory paths), Stack-based (operations via stack), Accumulator-based (single register for operations) Real Architectures Practical implementations optimizing speed, parallelism, and efficiency for real-world applications. RISC (ARM, RISC-V), CISC (x86), VLIW (Intel Itanium), Superscalar (modern x86, ARM Cortex), Multi-core (Intel Core, AMD Ryzen) <p>Modern processors often blend Von Neumann\u2019s model with elements of Harvard architecture to enhance performance. Superscalar and multi-core architectures leverage parallel execution for increased computing power, making them dominant in general-purpose computing, AI acceleration, and cloud systems.</p>"},{"location":"ai/pages/hardware/#parallel-architecture","title":"Parallel Architecture","text":"<p>Parallel architecture is a computer system design that enables multiple processors or computing units to execute tasks simultaneously, improving speed and efficiency. It is essential for AI, big data, and modern computing, as workloads like deep learning, graphics processing, and cloud computing require massive parallelism to handle complex computations efficiently. With the slowdown of Moore\u2019s Law, parallel processing is the key to scaling performance while maintaining energy efficiency. For a deep dive on this subject travel to Parallel architecture</p>"},{"location":"ai/pages/hardware/#quantitative-principles-of-computer-design","title":"Quantitative Principles of Computer Design","text":""},{"location":"ai/pages/hardware/#take-advantage-of-parallelism","title":"Take Advantage of Parallelism","text":"<p>Parallelism is a key strategy for improving computing performance. It can be leveraged at multiple levels:</p> <ul> <li>System-Level Parallelism: Servers use multiple processors and storage devices to handle workloads efficiently.</li> <li>Instruction-Level Parallelism (ILP): Techniques such as pipelining improve execution speed by overlapping instruction execution.</li> <li>Data-Level Parallelism (DLP): Vector processing and SIMD instructions enable faster computations in AI and multimedia applications.</li> </ul>"},{"location":"ai/pages/hardware/#principle-of-locality","title":"Principle of Locality","text":"<p>Programs tend to reuse recently accessed data and instructions. This is classified into:</p> <ul> <li>Temporal Locality: Frequently used data is likely to be accessed soon.</li> <li>Spatial Locality: Data stored near recently accessed addresses is likely to be used. Optimizing memory hierarchies and cache design takes advantage of these principles to improve performance.</li> </ul>"},{"location":"ai/pages/hardware/#focus-on-the-common-case","title":"Focus on the Common Case","text":"<p>Prioritizing optimizations for the most frequent operations leads to greater overall performance gains. Examples include:</p> <ul> <li>Optimizing instruction fetch and decode units (used more often than multipliers).</li> <li>Prioritizing memory accesses over rare operations to enhance speed.</li> </ul>"},{"location":"ai/pages/hardware/#amdahls-law","title":"Amdahl\u2019s Law","text":"<p>Amdahl\u2019s Law quantifies the potential speedup of a system by optimizing a portion of its execution. The formula:</p> \\[ Speedup = \\frac{1}{(1 - Fraction_{enhanced}) + \\frac{Fraction_{enhanced}}{Speedup_{enhanced}}} \\] <p>Illustrates diminishing returns\u2014speedup is limited by the fraction of time the enhancement can be utilized.</p>"},{"location":"ai/pages/hardware/#the-processor-performance-equation","title":"The Processor Performance Equation","text":"<p>Performance depends on three key factors:</p> \\[ CPU\\ Time = Instruction\\ Count \\times Cycles\\ per\\ Instruction \\times Clock\\ Cycle\\ Time \\] <ul> <li>Clock Cycle Time: Determined by hardware and fabrication technology.</li> <li>Cycles per Instruction (CPI): Affected by instruction set and microarchitecture.</li> <li>Instruction Count: Influenced by compiler optimizations and ISA efficiency.</li> </ul> <p>Optimizing across these dimensions ensures efficient and high-performance computing systems.</p>"},{"location":"ai/pages/hardware/#memory-hierarchy-design","title":"Memory Hierarchy Design","text":"<p>Computers use a memory hierarchy to balance speed and cost, with fast, small caches close to the processor and slower, larger memory further away. Since accessing main memory is slow, cache optimization techniques like loop blocking, prefetching, and associativity improvements help keep frequently used data nearby, reducing delays. Understanding how caches work and how to optimize memory access is essential for improving overall system performance, especially in tasks like matrix multiplication, AI workloads, and high-performance computing. For a deep dive visit: Memory Hierarchy Design</p>"},{"location":"ai/pages/jetson/","title":"Jetson Nano","text":"<p>The Jetson Nano is a powerful yet compact AI computing platform designed for edge AI projects. It provides the performance and flexibility needed for real-world AI applications while remaining accessible to hobbyists, students, and developers alike. Below, you'll find essential resources to get started with the Jetson Nano:</p>"},{"location":"ai/pages/jetson/#resources","title":"Resources","text":"<ul> <li>Jetson Nano Datasheet: Comprehensive technical specifications for the Jetson Nano.</li> <li>Jetson Nano Developer Kit: Official page for setup guides, tutorials, and detailed documentation.</li> <li>NVIDIA Jetson Projects: Explore a collection of projects built by the Jetson community for inspiration and guidance.</li> <li>YOLO Object Detection</li> </ul>"},{"location":"ai/pages/jetson/#ssh-into-jetson-nano-via-ubuntu-wsl","title":"SSH into Jetson Nano via Ubuntu WSL","text":"<ol> <li> <p>Identify the Jetson Nano's IP Address    On the Jetson Nano, run: <code>bash    hostname -I</code>    Note the IP address.</p> </li> <li> <p>Check Connection from WSL    Open the Ubuntu WSL terminal on your Windows machine and ping the Jetson Nano to confirm connectivity: <code>bash    ping &lt;Jetson-IP&gt;</code>    Replace <code>&lt;Jetson-IP&gt;</code> with the IP address of the Jetson Nano.</p> </li> <li> <p>SSH into the Jetson Nano    Use the <code>ssh</code> command in your WSL terminal: <code>bash    ssh jetson@&lt;Jetson-IP&gt;</code> </p> <ul> <li>Replace <code>&lt;Jetson-IP&gt;</code> with the IP address of the Jetson Nano.  </li> <li>The default password is <code>jetson</code>.  </li> </ul> </li> <li> <p>Save SSH Fingerprint    During the first connection, you may be prompted to confirm the SSH fingerprint. Type <code>yes</code> and press Enter to save it for future connections.</p> </li> </ol>"},{"location":"ai/pages/jetson/#image-processing-hello-ai-world","title":"Image Processing (Hello AI World)","text":"<p>Following notes are based off Jetson Nano's AI introductionary project called Hello AI world.</p>"},{"location":"ai/pages/jetson/#imagenet","title":"ImageNet","text":"<p>ImageNet is a large-scale dataset containing millions of labeled images across thousands of categories. It is widely used for training and benchmarking computer vision models and was instrumental in the rise of deep learning, starting with the success of convolutional neural networks (CNNs) like AlexNet in 2012.</p> <p>Why It Matters to the Jetson Nano:</p> <p>The Jetson Nano, a low-power AI development board for edge computing, benefits from ImageNet in several ways:</p> <ol> <li>Pretrained Models: Deploy CNNs like ResNet or MobileNet for tasks such as object detection, image classification, and feature extraction.  </li> <li>Transfer Learning: Fine-tune pretrained models on custom datasets with accelerated training and inference.  </li> <li>Benchmarking: Use ImageNet to evaluate the Nano\u2019s performance in handling computer vision workloads.  </li> <li>Edge AI Applications: Enable real-time object recognition for robotics, IoT devices, and autonomous systems.  </li> </ol> <p>Learning Material: Jetson ImagNet Project</p>"},{"location":"ai/pages/jetson/#tensorrt","title":"TensorRT","text":"<p>TensorRT is an NVIDIA library for optimizing and accelerating deep learning inference on GPUs. It supports model optimization (e.g., precision calibration, layer fusion), multiple precisions (FP32, FP16, INT8), and works with models from frameworks like TensorFlow, PyTorch, and ONNX. TensorRT is ideal for high-throughput, low-latency applications in real-time AI tasks such as object detection, NLP, and autonomous systems.</p> <p>Example project using GoogleNet image recognition netowrk with TensorRT found here</p>"},{"location":"ai/pages/jetson/#sigmoid-and-softmax-in-classification","title":"Sigmoid and Softmax in Classification","text":"<p>Softmax is used for single-class classification, where each instance belongs to exactly one class. It converts logits into a probability distribution using the formula:</p> \\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\] <p>This ensures that probabilities across all classes sum to 1. Softmax is suitable for tasks where classes are mutually exclusive, like predicting whether an image contains a \"cat,\" \"dog,\" or \"bird\" (but only one of them).</p> <p>Sigmoid is used for multi-label classification, where each instance can belong to multiple classes. It applies the function:</p> \\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\] <p>to each output independently, producing probabilities between 0 and 1. For example, in an image containing both \"cat\" and \"dog,\" sigmoid allows both classes to be true since they are not mutually exclusive. The probabilities for each class do not sum to 1.</p> <p>Here's a Sigmoid example for printing out the top 5 classifications of a jpg image:</p> <pre><code>#include &lt;jetson-inference/imageNet.h&gt;\n#include &lt;jetson-utils/loadImage.h&gt;\n#include &lt;vector&gt;\n\nint main(int argc, char** argv)\n{\n    // Ensure an image filename is provided as a command line argument\n    if (argc &lt; 2)\n    {\n        printf(\"my-recognition: expected image filename as argument\\n\");\n        printf(\"example usage:  ./my-recognition my_image.jpg\\n\");\n        return 0;\n    }\n\n    // Retrieve the image filename from command line arguments\n    const char* imgFilename = argv[1];\n\n    // Variables for the image data pointer and dimensions\n    uchar3* imgPtr = NULL;   // shared CPU/GPU pointer to image\n    int imgWidth   = 0;      // width of the image (in pixels)\n    int imgHeight  = 0;      // height of the image (in pixels)\n\n    // Load the image from disk as uchar3 RGB (24 bits per pixel)\n    if (!loadImage(imgFilename, &amp;imgPtr, &amp;imgWidth, &amp;imgHeight))\n    {\n        printf(\"failed to load image '%s'\\n\", imgFilename);\n        return 0;\n    }\n\n    // Load the GoogleNet image recognition network with TensorRT\n    imageNet* net = imageNet::Create(\"googlenet\");\n\n    // Ensure the network model loaded properly\n    if (!net)\n    {\n        printf(\"failed to load image recognition network\\n\");\n        return 0;\n    }\n\n    // Store classifications and specify the number of top results (topK)\n    imageNet::Classifications classifications; // vector of (classID, confidence)\n    const int topK = 5; // Modify this for more or fewer top results\n\n    // Classify the image and retrieve multiple results\n    if (net-&gt;Classify(imgPtr, imgWidth, imgHeight, classifications, topK) &lt; 0)\n    {\n        printf(\"failed to classify image\\n\");\n        delete net;\n        return 0;\n    }\n\n    // Print out the classification results\n    printf(\"Classification results (top %d):\\n\", topK);\n    for (size_t n = 0; n &lt; classifications.size(); n++)\n    {\n        const uint32_t classID = classifications[n].first;\n        const char* classLabel = net-&gt;GetClassLabel(classID);\n        const float confidence = classifications[n].second * 100.0f;\n\n        printf(\" %2.5f%% class #%u (%s)\\n\", confidence, classID, classLabel);\n    }\n\n    // Free the network's resources before shutting down\n    delete net;\n    return 0;\n}\n</code></pre> <p>example project here: Multi-Label Classification for Image Tagging</p>"},{"location":"ai/pages/jetson/#object-detection-with-detectnet","title":"Object Detection with DetectNet","text":"<p>DetectNet is a deep learning framework designed for object detection tasks, optimized for NVIDIA Jetson devices. It identifies objects in an image and provides their locations using bounding boxes.</p> <p>Key Features:</p> <ol> <li>Input: Processes an image through a pre-trained neural network (e.g., SSD or Faster R-CNN).</li> <li>Output: Returns: Bounding boxes, Object classes, and Confidence scores</li> <li>Applications: Pedestrian detection, Vehicle detection, Custom object detection (after fine-tuning)</li> </ol> <p>DetectNet utilizes TensorRT for high-speed inference and supports training on custom datasets to detect specific object classes.</p> <p>Reference for images</p> <p>Reference for video</p>"},{"location":"ai/pages/jetson/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Semantic segmentation classifies every pixel in an image into categories (e.g., road, car, sky), offering detailed scene understanding. Unlike object detection, which provides bounding boxes, semantic segmentation delivers pixel-level precision. Two prominent architectures are Fully Convolutional Networks (FCNs) and SegNet.</p> <p>Fully Convolutional Networks (FCNs)</p> <ul> <li>Architecture: Replace fully connected layers with convolutional layers to preserve spatial relationships.</li> <li>Upsampling: Use transposed convolutions (deconvolution) to restore spatial resolution.</li> <li>Skip Connections: Combine high-level features with low-level details for better accuracy.</li> <li>Use Case: High-accuracy, detailed segmentation tasks, though computationally intensive.</li> </ul> <p>SegNet</p> <ul> <li> <p>Encoder-Decoder Structure:</p> <ol> <li>Encoder: Extracts features with convolution and pooling layers, reducing spatial resolution.</li> <li>Decoder: Upsamples using saved pooling indices from the encoder, maintaining spatial accuracy while reducing memory and computation.</li> </ol> </li> <li> <p>Optimized for Efficiency: Designed for lightweight, real-time applications.</p> </li> <li>Use Case: Real-time tasks on resource-constrained devices, like Jetson Nano.</li> </ul> <p>Applications</p> <ol> <li> <p>Urban Environments (Cityscapes)    Segment urban scenes to identify roads, buildings, pedestrians, and vehicles for autonomous driving and smart city systems.  </p> <ul> <li>Dataset: Cityscapes </li> </ul> </li> <li> <p>Off-Road Navigation (DeepScene)    Classify forest trails, vegetation, and off-road elements for robotic path-following in outdoor environments.  </p> <ul> <li>Dataset: DeepScene </li> </ul> </li> <li> <p>Multi-Human Parsing (MHP)    Segment people into detailed body parts like arms, legs, and clothing for pose estimation or fashion analytics.  </p> <ul> <li>Dataset: Multi-Human Parsing </li> </ul> </li> <li> <p>Object Segmentation (Pascal VOC)    Classify and segment various objects, including people, animals, and vehicles, for general-purpose object recognition.  </p> <ul> <li>Dataset: Pascal VOC </li> </ul> </li> <li> <p>Indoor Scene Understanding (SUN RGB-D)    Recognize furniture, appliances, and spaces in office and home environments for robotics and augmented reality.  </p> <ul> <li>Dataset: SUN RGB-D </li> </ul> </li> </ol> <p>Nividia Reference</p>"},{"location":"ai/pages/jetson/#setting-up-yolo-on-jetson-nano-with-docker","title":"Setting Up YOLO on Jetson Nano with Docker","text":""},{"location":"ai/pages/jetson/#install-the-jetpack-docker-image","title":"Install the JetPack Docker Image","text":"<p>To get started, visit the Ultralytics NVIDIA Jetson Guide and follow the instructions to install the Ultralytics YOLO11 JetPack 4 Docker environment .</p>"},{"location":"ai/pages/jetson/#running-the-docker-environment","title":"Running the Docker Environment","text":"<p>Once the container is downloaded, run it with:</p> <pre><code>sudo docker run -it --ipc=host --runtime=nvidia ultralytics/ultralytics:latest-jetson-jetpack4\n</code></pre> <p>After running this command, you should see a prompt similar to:</p> <pre><code>root@&lt;container_id&gt;:/ultralytics#\n</code></pre> <p>This means you are inside the YOLO Docker environment.</p>"},{"location":"ai/pages/jetson/#basic-docker-commands","title":"Basic Docker Commands","text":"<p>Check Running Containers</p> <pre><code>sudo docker ps\n</code></pre> <p>Check All Containers (Including Stopped Ones)</p> <pre><code>sudo docker ps -a\n</code></pre> <p>Restart a Stopped Container</p> <pre><code>sudo docker start &lt;CONTAINER_ID&gt;\n</code></pre> <p>Enter a Running Container</p> <pre><code>sudo docker exec -it &lt;CONTAINER_ID&gt; bash\n</code></pre> <p>Exit the Container</p> <pre><code>exit\n</code></pre> <p>Copy Files From a Stopped Container to Your Jetson Nano</p> <pre><code>sudo docker cp &lt;CONTAINER_ID&gt;:/ultralytics/runs/detect/predict ~/yolo_results\n</code></pre>"},{"location":"ai/pages/jetson/#running-yolo-inside-the-docker-container","title":"Running YOLO Inside the Docker Container","text":"<p>Verify YOLO Installation</p> <pre><code>python3 -c \"import ultralytics; print(ultralytics.__version__)\"\n</code></pre> <p>Run a YOLO Detection on a Sample Image</p> <pre><code>yolo detect predict model=yolo11n.pt source=https://ultralytics.com/images/zidane.jpg\n</code></pre> <p>Expected output:</p> <pre><code>Results saved to /ultralytics/runs/detect/predict\n</code></pre> <p>Check GPU Availability</p> <pre><code>python3 -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>If it prints <code>True</code>, YOLO is using CUDA.</p> <p>Run YOLO on a Custom Image Make sure the image is accessible inside the container and run:</p> <pre><code>yolo detect predict model=yolo11n.pt source=/path/to/your/image.jpg\n</code></pre>"},{"location":"ai/pages/jetson/#accessing-results","title":"Accessing Results","text":"<p>YOLO saves detected images inside <code>/ultralytics/runs/detect/predict</code>. To access these files:</p> <ol> <li>Re-enter the container:</li> </ol> <p><code>bash    sudo docker exec -it &lt;CONTAINER_ID&gt; bash</code></p> <ol> <li>Navigate to the results folder:</li> </ol> <p><code>bash    cd /ultralytics/runs/detect/predict    ls</code></p> <ol> <li>Copy results to your Jetson Nano:</li> </ol> <p><code>bash    sudo docker cp &lt;CONTAINER_ID&gt;:/ultralytics/runs/detect/predict ~/yolo_results</code></p> <p>Now you can access them in <code>~/yolo_results</code>.</p>"},{"location":"ai/pages/jetson/#training-yolo-untested","title":"Training YOLO (untested)","text":"<p>If you want to train YOLO on your own dataset, use:</p> <pre><code>yolo train model=yolo11n.pt data=your_dataset.yaml epochs=50 imgsz=640\n</code></pre>"},{"location":"ai/pages/jetson/#cleaning-up-docker-containers","title":"Cleaning Up Docker Containers","text":"<p>If you have too many stopped containers, you can remove them with:</p> <pre><code>sudo docker container prune\n</code></pre> <p>This deletes all stopped containers to free up space.</p>"},{"location":"ai/pages/memory/","title":"Memory Hierarchy Design","text":""},{"location":"ai/pages/memory/#memory-hierarchy-basics","title":"Memory Hierarchy Basics","text":"<p>Modern computing systems rely on memory hierarchies to bridge the gap between fast processors and slower main memory. When a requested word is not found in a cache (cache miss), the system fetches it from a lower-level memory, often bringing in an entire block due to spatial locality. Each cache block includes a tag to identify its memory address.</p>"},{"location":"ai/pages/memory/#cache-placement-strategies","title":"Cache Placement Strategies:","text":"<ol> <li>Direct-Mapped Cache \u2013 Each block maps to exactly one location.</li> <li>Fully Associative Cache \u2013 Blocks can be placed anywhere in the cache.</li> <li>Set-Associative Cache \u2013 Blocks map to a limited number of locations (n-way set associativity).</li> </ol>"},{"location":"ai/pages/memory/#cache-write-strategies","title":"Cache Write Strategies:","text":"<ul> <li>Write-Through \u2013 Data is written to both the cache and main memory simultaneously.</li> <li>Write-Back \u2013 Data is written to the cache first and updated in main memory later, improving efficiency.</li> </ul>"},{"location":"ai/pages/memory/#cache-miss-classification-3-cs-model","title":"Cache Miss Classification (3 Cs Model):","text":"<ol> <li>Compulsory Misses \u2013 Occur when data is accessed for the first time.</li> <li>Capacity Misses \u2013 Happen when the cache cannot hold all necessary data.</li> <li>Conflict Misses \u2013 Result from poor cache placement, causing unnecessary evictions.</li> </ol>"},{"location":"ai/pages/memory/#optimizing-cache-performance","title":"Optimizing Cache Performance:","text":"<ul> <li>Larger Block Sizes reduce compulsory misses but may increase miss penalties.</li> <li>Increasing Cache Size minimizes capacity misses at the cost of increased access time.</li> <li>Higher Associativity reduces conflict misses but can increase complexity and power consumption.</li> <li>Multilevel Caches balance speed and capacity (e.g., L1, L2, L3 caches).</li> </ul>"},{"location":"ai/pages/memory/#average-memory-access-time-amat","title":"Average Memory Access Time (AMAT):","text":"<p>A key metric for evaluating memory hierarchy performance:</p> \\[ \\text{AMAT} = \\text{Hit Time} + (\\text{Miss Rate} \\times \\text{Miss Penalty}) \\] <p>Optimizations such as multithreading and latency hiding help mitigate cache misses by overlapping memory accesses with computation.</p>"},{"location":"ai/pages/memory/#types-of-memory-in-computer-architecture","title":"Types of Memory in Computer Architecture","text":"<p>Memory in a computer system is structured into primary, secondary, and virtual memory, each serving different roles in performance and storage hierarchy.</p>"},{"location":"ai/pages/memory/#a-primary-memory-volatile","title":"A. Primary Memory (Volatile)","text":"<p>This memory type directly supports the CPU, ensuring fast access for ongoing computations.</p> Memory Type Description Examples Registers Small storage inside the CPU for immediate execution. Program Counter (PC), Accumulator Cache Memory Stores frequently accessed data for faster CPU access. L1, L2, L3 cache RAM (Random Access Memory) Stores currently running programs and data. DDR4, DDR5"},{"location":"ai/pages/memory/#b-secondary-memory-non-volatile","title":"B. Secondary Memory (Non-Volatile)","text":"<p>Used for long-term data storage and retains data even when the power is off.</p> Memory Type Description Examples HDD (Hard Disk Drive) Magnetic storage, slower but cost-effective. Traditional laptop HDDs SSD (Solid State Drive) Flash-based storage, much faster than HDDs. NVMe SSD, SATA SSD Flash Memory Non-volatile, used in portable devices and SSDs. USB drives, SD cards"},{"location":"ai/pages/memory/#c-virtual-memory","title":"C. Virtual Memory","text":"Memory Type Description Examples Swap Space Uses disk storage as an extension of RAM. Linux Swap, Windows Pagefile Paging Divides memory into fixed-size pages for management. 4KB page size in OS <p>Virtual memory extends RAM capacity by using a portion of disk storage as an extension, allowing the system to handle larger workloads than physical memory alone. It functions by treating physical memory as a cache for secondary storage, moving data between the two as needed. This process relies on paging, where memory is divided into fixed-size pages, and a Translation Lookaside Buffer (TLB) caches page table entries to speed up address translation.  </p> <p>Beyond extending memory, virtual memory also enhances process isolation and security by providing each program with its own virtual address space, preventing direct access to other processes\u2019 memory. Virtual Machines (VMs) take this further by running entire operating systems within isolated environments, managed by a Virtual Machine Monitor (VMM) or hypervisor. VMs improve security, system reliability, and hardware utilization, making them essential for cloud computing and server virtualization.  </p> <p>Modern processors optimize virtual memory and VM performance through hardware-assisted virtualization, such as Intel VT-x and AMD-V, which reduce the overhead of managing multiple virtual address spaces. Additionally, techniques like nested page tables and direct memory access (DMA) virtualization enhance efficiency in handling virtualized workloads.</p> <p></p> <p>Figure above shows a process memory layout, which is managed by virtual memory. It allows the CPU to access a continuous address space, even if data is stored across RAM (volatile) and NVM (non-volatile). Source: Dr. Chen Pan UTSA</p>"},{"location":"ai/pages/memory/#memory-access-methods","title":"Memory Access Methods","text":"<p>Different memory technologies use specific access methods to retrieve data efficiently.</p> Access Method Description Examples Sequential Access Reads data in a linear order. Magnetic tape storage Direct Access Jumps directly to memory locations. HDDs, SSDs Random Access Any memory cell can be accessed instantly. RAM, Cache"},{"location":"ai/pages/memory/#memory-technology","title":"Memory Technology","text":""},{"location":"ai/pages/memory/#1-basic-memory-technologies","title":"1. Basic Memory Technologies","text":"<ul> <li>SRAM (Static RAM) \u2013 Fastest, used in CPU caches (L1, L2, L3), but expensive.</li> <li>DRAM (Dynamic RAM) \u2013 Main memory, slower than SRAM but cheaper and denser.</li> <li>Flash Memory \u2013 A type of EEPROM, Non-volatile, used in SSDs and embedded systems.</li> </ul> <p>In DRAM, the information is stored in a capacitor that holds a charge representing a bit of data. However, reading the data from the capacitor causes it to discharge, and since the charge leaks over time, the data is lost when read and must be rewritten. This is why DRAM requires periodic refreshing to maintain the data. In contrast, SRAM stores data in a flip-flop made of transistors, which does not lose its information when read. SRAM does not require refreshing and is faster than DRAM as a result.</p> <p> A six-transistor (6T) CMOS SRAM cell. WL: word line. BL: bit line. Realize how complex a single cell is compared to a DRAM cell below. Source: Public Domain, https://commons.wikimedia.org/w/index.php?curid=5771850</p> <p> A 4X4 DRAM array, realize how a single DRAM cell has only one transistor and one capacitor. Source: Dynamic random-access memory Wikipedia by J\u00fcrgenZ</p>"},{"location":"ai/pages/memory/#2-advanced-dram-technologies","title":"2. Advanced DRAM Technologies","text":"<ul> <li>SDRAM (Synchronous DRAM): The foundation of modern memory, synchronized with the system clock for predictable, efficient data access.</li> <li>DDR (Double Data Rate) SDRAM: Transfers data on both rising and falling clock edges, doubling bandwidth over SDRAM. Newer generations (DDR2\u2013DDR5) improve speed, efficiency, and capacity. Essential for CPUs in desktops, laptops, and servers, balancing speed and cost.</li> <li>GDDR (Graphics DRAM):  Optimized for GPUs with wider buses (32-bit), higher clock rates, and direct board soldering for 2\u20135\u00d7 DDR3 bandwidth. Critical for gaming, AI, and high-speed graphics tasks needing rapid memory access.</li> <li>HBM (High Bandwidth Memory): Stacks DRAM layers with an ultra-wide bus for extreme bandwidth and low power consumption. Used in AI, supercomputers, and high-end GPUs where traditional memory isn\u2019t fast enough.</li> </ul> <p>Source: J. L. Hennessy and D. A. Patterson, Computer Architecture: A Quantitative Approach, 6th ed. Morgan Kaufmann, 2017.</p>"},{"location":"ai/pages/memory/#3-memory-technology-optimizations","title":"3. Memory Technology Optimizations","text":"<p>To improve performance, modern computers use a memory hierarchy with the following strategies:</p> <ol> <li>Larger Cache Sizes \u2013 Increases data retention, reducing main memory accesses.</li> <li>Higher Associativity \u2013 Reduces cache misses by allowing more flexible data placement.</li> <li>Multilevel Caches (L1, L2, L3) \u2013 Keeps frequently used data closer to the processor.</li> <li>Burst Transfers and Prefetching \u2013 Reduces DRAM latency by pre-loading data.</li> <li>Banked &amp; Interleaved DRAM \u2013 Enables parallel memory accesses, increasing efficiency.</li> <li>Stacked Memory (HBM, 3D DRAM) \u2013 Reduces travel distances for lower latency and higher bandwidth.</li> </ol>"},{"location":"ai/pages/memory/#memory-reliability-and-error-correction","title":"Memory Reliability and Error Correction","text":"<p>Memory failures are mitigated through Error Correction Codes (ECC) and redundancy techniques.</p> Error Handling Method Description Parity Checking Detects single-bit errors but cannot correct them. ECC (Error Correction Code) Detects and fixes single-bit errors. Chipkill Technology Used in data centers; recovers from complete memory chip failure. <p>In large-scale computing (e.g., cloud servers, data centers), Chipkill and ECC memory are essential to ensure high data integrity and system stability.</p>"},{"location":"ai/pages/memory/#compiler-optimizations-to-reduce-cache-miss-rate","title":"Compiler Optimizations to Reduce Cache Miss Rate","text":"<p>Efficient cache utilization can significantly improve performance without requiring hardware modifications. The following compiler optimizations reduce cache miss rates by improving spatial and temporal locality.</p>"},{"location":"ai/pages/memory/#1-loop-interchange-improves-spatial-locality","title":"1. Loop Interchange (Improves Spatial Locality)","text":"<p>Nested loops may access data in an inefficient order, leading to cache misses. Loop interchange reorders loops so that memory accesses follow a sequential pattern, maximizing cache utilization.</p> <p>Example:</p> <pre><code>/* Before - Poor spatial locality (column-major order) */\nfor (j = 0; j &lt; 100; j++)\n    for (i = 0; i &lt; 5000; i++)\n        x[i][j] = 2 * x[i][j];\n\n/* After - Optimized for row-major order */\nfor (i = 0; i &lt; 5000; i++)\n    for (j = 0; j &lt; 100; j++)\n        x[i][j] = 2 * x[i][j];\n</code></pre> <ul> <li>The original version jumps through memory with a stride of 100, causing frequent cache misses.</li> <li>The optimized version accesses adjacent elements, improving spatial locality.</li> </ul>"},{"location":"ai/pages/memory/#2-blocking-improves-temporal-locality","title":"2. Blocking (Improves Temporal Locality)","text":"<p>Blocking reduces cache misses by ensuring frequently accessed data remains in the cache before being replaced.</p> <p>Example: Na\u00efve Matrix Multiplication (High Cache Misses)</p> <pre><code>/* Standard matrix multiplication */\nfor (i = 0; i &lt; N; i++)\n    for (j = 0; j &lt; N; j++) {\n        r = 0;\n        for (k = 0; k &lt; N; k++)\n            r += y[i][k] * z[k][j];\n        x[i][j] = r;\n    }\n</code></pre> <ul> <li>The above implementation results in poor cache efficiency because:</li> <li>It reads an entire N \u00d7 N matrix <code>z</code> for every row of <code>x</code> and <code>y</code>.</li> <li>Cache capacity is exceeded, leading to frequent capacity misses.</li> </ul> <p>Optimized Matrix Multiplication Using Blocking</p> <pre><code>/* Optimized using blocking */\nfor (jj = 0; jj &lt; N; jj += B)\n    for (kk = 0; kk &lt; N; kk += B)\n        for (i = 0; i &lt; N; i++)\n            for (j = jj; j &lt; min(jj + B, N); j++) {\n                r = 0;\n                for (k = kk; k &lt; min(kk + B, N); k++)\n                    r += y[i][k] * z[k][j];\n                x[i][j] += r;\n            }\n</code></pre> <ul> <li>Why is this better?</li> <li>Instead of working on full rows/columns, we process smaller blocks that fit in the cache, reducing memory accesses.</li> <li>This leverages both spatial and temporal locality.</li> </ul>"},{"location":"ai/pages/memory/#prioritization-for-matrix-multiplication-analysis","title":"Prioritization for Matrix Multiplication Analysis","text":"<p>To analyze the given matrix multiplication problem efficiently, we should prioritize the following steps:</p> <ol> <li> <p>Determine Matrix Dimensions </p> <ul> <li>Identify the size of <code>C = A \u00d7 B</code>, ensuring <code>C</code> has dimensions <code>(m \u00d7 n)</code>.</li> <li>Establish constraints for <code>l</code> based on memory availability.</li> </ul> </li> <li> <p>Memory Allocation Constraints </p> <ul> <li>Calculate the maximum possible <code>l</code> using memory constraints.</li> <li>Convert results to dimensions for <code>A</code>, <code>B</code>, and <code>C</code> considering 16-bit integer storage.</li> </ul> </li> <li> <p>Compute Memory Access Time </p> <ul> <li>Analyze cache behavior given fully associative cache with 32B cache lines.</li> <li>Compute cache hit/miss penalties using Least Recently Used (LRU) replacement.</li> <li>Derive total cycles required for the matrix multiplication.</li> </ul> </li> <li> <p>Optimize for Cache Efficiency </p> <ul> <li>Identify poor cache utilization patterns in the given loop.</li> <li>Implement loop interchange or blocking to improve locality.</li> <li>Calculate performance improvement by reducing cache misses.</li> </ul> </li> <li> <p>Analyze Post-Computational Random Access </p> <ul> <li>Compute the average memory access time when randomly accessing <code>A</code>, <code>B</code>, and <code>C</code>.</li> <li>Compare this with structured access times to draw conclusions on cache efficiency.</li> </ul> </li> <li> <p>Final Conclusion </p> <ul> <li>Summarize findings on how memory hierarchy impacts performance.</li> <li>Justify why locality-aware optimizations significantly improve execution speed.</li> </ul> </li> </ol> <p>By following this structured prioritization, we can effectively minimize memory latency and improve computational efficiency for matrix multiplication.</p>"},{"location":"ai/pages/neural/","title":"Neural","text":""},{"location":"ai/pages/neural/#computational-view-of-a-dnn","title":"Computational View of a DNN","text":""},{"location":"ai/pages/neural/#its-just-matrix-math","title":"Its Just Matrix Math","text":"<p>In deep neural networks (DNN), matrix math is the foundation for propagating data through layers, as depicted in the diagram. Each layer starts with input features (\\( \\mathbf{x}_1, \\mathbf{x}_2, \\dots \\)) that are combined with a weight matrix (\\( \\mathbf{W} \\)) to compute scores:</p> \\[ \\mathbf{z} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b} \\] <p>The scores are then passed through an activation function \\( f(\\cdot) \\), such as the one shown in the diagram, to introduce non-linearity:</p> \\[ \\mathbf{y} = f(\\mathbf{z}) \\] <p>The result, \\( \\mathbf{y} \\), represents the output of hidden units in each layer, which then serve as inputs to the next layer. This process is repeated across multiple layers, from the first layer (weights, scores, and activation functions) through the hidden layers to the final layer, which produces the output units.</p> <p>Each hidden layer transforms its inputs into a new representation using the same sequence: weights, scores, activation functions, and outputs. These operations are computationally intensive but highly parallelizable, making GPUs and TPUs essential for efficiently handling the large-scale computations in deep learning.</p>"},{"location":"ai/pages/neural/#training-the-dnn","title":"Training the DNN","text":"<p>Training is the process of learning the optimal weights and biases for a DNN. It involves:</p> <ol> <li>Forward Pass: Input features are propagated through the network to compute predictions.</li> <li>Loss Calculation: The error between predictions and targets is measured using a loss function.</li> <li>Backpropagation: Gradients of the loss with respect to the weights are calculated.</li> <li>Weight Update: Weights are adjusted using an optimizer (e.g., SGD) to minimize the error.</li> </ol> <p>This process repeats over many iterations (epochs) until the model converges.</p>"},{"location":"ai/pages/neural/#executing-the-dnn-inference","title":"Executing the DNN (Inference)","text":"<p>Inference uses the trained weights to make predictions. It involves:</p> <ol> <li>Forward Pass Only: Input features propagate through the network to compute outputs.</li> <li>Prediction: The final output represents the model's decision.</li> </ol> <p>Unlike training, inference is faster and less computationally intensive as it skips backpropagation and weight updates.</p> <p>Key Differences</p> Aspect Training Inference Purpose Learn weights. Make predictions. Operations Forward pass + backpropagation. Forward pass only. Resource Needs High (GPUs/TPUs). Low, optimized for speed."},{"location":"ai/pages/parallelism/","title":"Parallel Computing","text":"<p>Parallel architecture refers to a computer system design that allows multiple processors or computing units to execute tasks concurrently. This approach improves performance, efficiency, and scalability by leveraging parallelism at several levels.</p> <p>Parallelism in computing is the practice of executing multiple computations simultaneously by dividing work into smaller sub-tasks that can be processed independently. Modern systems use various forms of parallelism to meet the demands of complex and data\u2010intensive applications.</p>"},{"location":"ai/pages/parallelism/#flynns-taxonomy-of-parallel-architectures","title":"Flynn\u2019s Taxonomy of Parallel Architectures","text":"<p>Flynn\u2019s Taxonomy classifies computer architectures based on how instructions and data are processed.</p> <p></p> Category Description Usage Examples SISD (Single Instruction, Single Data) Traditional sequential processing: one instruction operates on one data element at a time. Early CPUs, simple microcontrollers SIMD (Single Instruction, Multiple Data) A single instruction is simultaneously applied to multiple data elements. GPUs, vector processors, deep learning accelerators MISD (Multiple Instruction, Single Data) Multiple instructions process the same data stream. This model is rarely used in practice. Fault-tolerant systems in critical applications (e.g., redundant military systems) MIMD (Multiple Instruction, Multiple Data) Different processors execute different instructions on separate data streams. Multi-core CPUs, distributed computing, cloud systems <p>Modern systems often blend characteristics of SIMD and MIMD, creating hybrid architectures that balance flexibility with high throughput. For example, GPUs integrate both data-level and thread-level parallelism to handle a variety of workloads.</p>"},{"location":"ai/pages/parallelism/#types-of-parallelism","title":"Types of Parallelism","text":"<p>Parallelism can be categorized into different types based on how tasks are executed. Two fundamental types are temporal parallelism (pipelining) and spatial parallelism.  </p>"},{"location":"ai/pages/parallelism/#temporal-parallelism-pipelining","title":"Temporal Parallelism (Pipelining)","text":"<p>Temporal parallelism divides a task into sequential stages that can be overlapped, improving overall throughput. A great way to understand this is through a cookie-baking process:  </p> <ul> <li>Without parallelism: You roll out the dough (5 min) and then bake it (15 min), taking 20 minutes per batch.  </li> <li>With pipelining: While the first batch is baking, you start preparing the next one. This doesn\u2019t change the time required for a single batch but allows continuous production, reducing the wait time between finished batches.  </li> </ul> <p>This analogy mirrors how CPUs execute instructions in a pipeline, where different stages (fetch, decode, execute, etc.) are processing different instructions simultaneously. Just like a baker needs separate trays to avoid mixing batches, processors use pipeline registers to store intermediate data. While the total time for the first output remains the same, subsequent results come out much faster, boosting overall performance.  </p> <p> Source: Dr. Chen Pan (UTSA)</p>"},{"location":"ai/pages/parallelism/#spatial-parallelism","title":"Spatial Parallelism","text":"<p>Spatial parallelism, on the other hand, involves executing tasks simultaneously using separate hardware units. Instead of working in stages, tasks run in parallel from the start.  </p> <p>Imagine Ben has a helper and an extra oven. Now, two batches can be processed at the same time\u2014effectively doubling throughput while keeping the time per batch unchanged.  </p> <p>This type of parallelism is commonly seen in:</p> <ul> <li>Multithreaded systems: Where independent threads run on separate cores.</li> <li>GPU architectures: Which execute many threads in parallel using a SIMT (Single Instruction, Multiple Threads) model.</li> </ul>"},{"location":"ai/pages/parallelism/#types-of-processing","title":"Types of Processing","text":"<p>Processing refers to how a CPU executes instructions.</p>"},{"location":"ai/pages/parallelism/#scalar-processing","title":"Scalar Processing","text":"<p>A scalar CPU executes one instruction at a time. This category includes pipelined processors, which improve instruction throughput by overlapping the execution of multiple instructions through different stages of processing. However, despite the efficiency gains from pipelining, a scalar processor fundamentally handles only one operation per cycle.</p>"},{"location":"ai/pages/parallelism/#vector-processing","title":"Vector Processing","text":"<p>A vector CPU also executes one instruction at a time, but it operates on vector data instead of single scalar values. This allows the processor to perform computations on multiple elements simultaneously. For example, the instruction:</p> <pre><code>X[0:7] + Y[0:7]\n</code></pre> <p>performs element-wise addition on two vectors of eight elements in a single operation. On a scalar processor, this would require eight separate instructions to achieve the same result.</p>"},{"location":"ai/pages/parallelism/#superscalar-processing","title":"Superscalar Processing","text":"<p>A superscalar CPU is capable of executing multiple unrelated instructions simultaneously. It achieves this by having multiple execution units that can work in parallel. For example, a superscalar processor could execute the following two instructions concurrently:</p> <pre><code>ADD X + Y\nMUL W * Z\n</code></pre> <p>This parallelism increases overall instruction throughput and efficiency, making superscalar processors well-suited for high-performance computing applications.</p>"},{"location":"ai/pages/parallelism/#understanding-instruction-level-parallelism-ilp","title":"Understanding Instruction-Level Parallelism (ILP)","text":""},{"location":"ai/pages/parallelism/#what-is-ilp","title":"What is ILP?","text":"<p>Instruction-Level Parallelism (ILP) refers to the extent to which multiple instructions in a program can be executed simultaneously. ILP is an attribute of the program, meaning it depends on the nature of the instructions, the Instruction Set Architecture (ISA), and how well the compiler organizes the code for parallel execution.</p>"},{"location":"ai/pages/parallelism/#ilp-vs-ipc","title":"ILP vs. IPC","text":"<p>While ILP describes the potential parallelism within a program, Instructions Per Cycle (IPC) measures the actual instruction throughput of a processor. The key differences are:</p> <ul> <li>ILP is theoretical: It represents the maximum parallelism that can be extracted from a program.</li> <li>IPC is practical: It depends on the actual hardware implementation and various runtime factors.</li> <li>ILP sets an upper bound on IPC, but the achievable IPC is limited by hardware constraints such as instruction latencies, cache behavior, and processor execution capabilities.</li> </ul>"},{"location":"ai/pages/parallelism/#hazards-and-dependencies","title":"Hazards and Dependencies","text":"<p>To effectively exploit ILP, we must understand various hazards (situations that prevent parallel execution). The three main types of dependencies that cause hazards are:</p>"},{"location":"ai/pages/parallelism/#1-data-dependencies","title":"1. Data Dependencies","text":"<p>Data dependencies occur when an instruction depends on the result of a previous instruction. There are three types:</p> <ul> <li>True Dependency (Read After Write, RAW): The second instruction needs data produced by the first instruction.</li> </ul> <pre><code>ADD R1, R2, R3   # R1 = R2 + R3  \nSUB R4, R1, R5   # R4 = R1 - R5 (Depends on ADD result)\n</code></pre> <ul> <li>Anti Dependency (Write After Read, WAR): The second instruction writes to a register before the first instruction reads it.</li> </ul> <pre><code>SUB R1, R2, R3   # Reads R2\nADD R2, R4, R5   # Writes to R2 before SUB reads it\n</code></pre> <ul> <li>Output Dependency (Write After Write, WAW): Two instructions write to the same register.</li> </ul> <pre><code>MUL R1, R2, R3   # Writes to R1\nADD R1, R4, R5   # Also writes to R1, causing a conflict\n</code></pre>"},{"location":"ai/pages/parallelism/#2-control-dependencies","title":"2. Control Dependencies","text":"<p>Control dependencies occur when an instruction\u2019s execution depends on the outcome of a previous branch instruction.</p> <pre><code>BEQ R1, R2, LABEL  # If R1 == R2, jump to LABEL\nADD R3, R4, R5     # Only executed if branch is not taken\n</code></pre> <p>Processors use branch prediction and speculative execution to mitigate control dependencies, but incorrect predictions lead to performance penalties due to pipeline flushes.</p>"},{"location":"ai/pages/parallelism/#3-memory-dependencies","title":"3. Memory Dependencies","text":"<p>Memory dependencies arise when multiple instructions access memory, potentially causing conflicts:</p> <ul> <li>Load After Store (Read After Write, RAW):</li> </ul> <pre><code>STORE R1, 0(R2)  # Store R1 at memory address in R2\nLOAD R3, 0(R2)   # Read from the same memory location\n</code></pre> <p>The processor must ensure the store completes before the load.</p> <ul> <li>Store After Load (Write After Read, WAR):</li> </ul> <pre><code>LOAD R1, 0(R2)   # Load value from memory\nSTORE R3, 0(R2)  # Store a new value at the same address\n</code></pre> <p>If reordered improperly, the store might overwrite a value before it is used.</p> <ul> <li>Store After Store (Write After Write, WAW):</li> </ul> <pre><code>STORE R1, 0(R2)  # Write value to memory\nSTORE R3, 0(R2)  # Write a different value to the same address\n</code></pre> <p>The second store must execute in the correct order to maintain program correctness.</p>"},{"location":"ai/pages/parallelism/#why-ilp-matters","title":"Why ILP Matters","text":"<p>ILP is fundamental to modern processor design as it enables higher performance without increasing clock speeds. Exploiting ILP efficiently can lead to significant improvements in throughput, power efficiency, and execution speed. However, real-world execution is constrained by hardware limitations and hazards such as data, control, and memory dependencies. </p> <p>Understanding ILP also ties into broader topics like superscalar architectures, out-of-order execution, speculative execution, and multi-threading. Techniques like register renaming, branch prediction, and speculative execution help mitigate dependency issues and increase IPC, making processors more efficient. </p> <p>In the evolution of computing, ILP has been a key driver behind performance gains. While traditional single-core processors aimed to maximize ILP within a single thread, modern computing often combines ILP with thread-level parallelism (TLP) and data-level parallelism (DLP) to achieve even greater performance scalability. </p>"},{"location":"ai/pages/parallelism/#understanding-thread-level-parallelism-tlp","title":"Understanding Thread-Level Parallelism (TLP)","text":""},{"location":"ai/pages/parallelism/#why-tlp-matters","title":"Why TLP Matters","text":"<p>As extracting additional ILP from a single sequential thread becomes increasingly difficult, modern architectures have shifted toward Thread-Level Parallelism (TLP) to enhance performance. TLP allows multiple threads to execute concurrently, improving processor utilization and throughput.</p>"},{"location":"ai/pages/parallelism/#types-of-tlp","title":"Types of TLP","text":"<ul> <li>TLP from Multiprogramming: The processor executes multiple independent programs by switching between them, optimizing CPU usage by reducing idle time. This is beneficial in operating systems that need to handle multiple user applications simultaneously, such as running a web browser, a music player, and background system tasks concurrently.</li> <li>TLP from Multithreaded Applications: A single program is divided into multiple concurrent threads, each executing a portion of the task simultaneously. This enhances performance for applications requiring high responsiveness or heavy computation, such as video processing, gaming engines, web servers handling multiple client requests, and parallel database transactions.</li> </ul>"},{"location":"ai/pages/parallelism/#how-multithreading-uses-tlp","title":"How Multithreading Uses TLP","text":"<p>Multithreading enables processors to execute multiple threads in parallel, improving utilization of resources. This is particularly useful when individual threads face stalls (e.g., waiting for memory access).</p> <p> Source: Dr. Chen Pan (UTSA)</p> <p>Each thread maintains its own user state, including:</p> <ul> <li>Program Counter (PC)</li> <li>General-Purpose Registers (GPRs)</li> <li>Stack and Memory Context</li> </ul>"},{"location":"ai/pages/parallelism/#flynns-taxonomy","title":"Flynn\u2019s Taxonomy","text":"<p>Flynn\u2019s Taxonomy categorizes computer architectures based on instruction and data processing.</p> <p></p> Category Description Usage Examples SISD (Single Instruction, Single Data) Traditional sequential processing: one instruction operates on one data element at a time. Early CPUs, simple microcontrollers SIMD (Single Instruction, Multiple Data) A single instruction is simultaneously applied to multiple data elements. GPUs, vector processors, deep learning accelerators MISD (Multiple Instruction, Single Data) Multiple instructions process the same data stream. This model is rarely used in practice. Fault-tolerant systems in critical applications (e.g., redundant military systems) MIMD (Multiple Instruction, Multiple Data) Different processors execute different instructions on separate data streams. Multi-core CPUs, distributed computing, cloud systems <p>Modern computing relies on a blend of SIMD, MIMD, ILP, and TLP to maximize efficiency. GPUs, for example, utilize SIMD for high throughput while leveraging TLP to manage thousands of concurrent threads.</p>"},{"location":"ai/pages/parallelism/#the-big-picture-of-tlp","title":"The Big Picture of TLP","text":"<p>ILP and TLP represent two complementary approaches to increasing computational performance. While ILP focuses on extracting fine-grained parallelism within a single thread, TLP exploits coarse-grained parallelism across multiple threads. Modern architectures integrate both ILP and TLP to achieve optimal performance across diverse workloads, from single-threaded applications to massively parallel computing environments.</p> <p>Future discussions will explore how modern hardware, such as multi-core processors, simultaneous multithreading (SMT), and heterogeneous computing, further enhance parallel execution.</p>"},{"location":"ai/pages/parallelism/#understanding-graphics-processing-units-gpus","title":"Understanding Graphics Processing Units (GPUs)","text":""},{"location":"ai/pages/parallelism/#gpu-vs-cpu-architectural-differences","title":"GPU vs. CPU: Architectural Differences","text":"<p> Source: Dr. Chen Pan UTSA</p> <p>While CPUs are optimized for low-latency, sequential processing, GPUs are designed for high-throughput, massively parallel computation. This fundamental difference arises from their respective design goals:</p> <ul> <li>CPUs: Prioritize fast execution of a few tasks, featuring deep cache hierarchies and complex branch prediction.</li> <li>GPUs: Optimize for parallel execution of thousands of lightweight threads, with a large number of simple cores and minimal caching.</li> </ul> <p> Source: Dr. Chen Pan UTSA</p>"},{"location":"ai/pages/parallelism/#the-simd-and-simt-execution-model","title":"The SIMD and SIMT Execution Model","text":"<p>GPUs leverage Single Instruction Multiple Data (SIMD) and Single Instruction Multiple Threads (SIMT) architectures to achieve parallelism:</p> <p></p> <ul> <li>SIMD: A single instruction operates on multiple data elements simultaneously, making it efficient for tasks like matrix operations and image processing.</li> <li>SIMT: Extends SIMD by organizing threads into warps, where each thread executes the same instruction but on different data elements.</li> </ul>"},{"location":"ai/pages/parallelism/#understanding-warps","title":"Understanding Warps","text":"<p>A warp is a group of threads that execute the same instruction simultaneously on different data elements. Warps are the fundamental unit of execution in a GPU's Streaming Multiprocessors (SMs). Typically, a warp consists of 32 threads in modern NVIDIA GPUs, but this number can vary by architecture.</p> <ul> <li>Warp Scheduling: A GPU schedules warps dynamically to maximize computational throughput. If one warp is waiting on memory, another can execute to utilize processing resources efficiently.</li> <li>Divergence Handling: If different threads within a warp take different execution paths (due to conditional branching), the warp may have to serialize execution, reducing efficiency.</li> </ul> <p>By structuring workloads to minimize warp divergence, developers can optimize GPU performance and fully leverage parallel execution capabilities.</p>"},{"location":"ai/pages/parallelism/#cuda-programming-model","title":"CUDA Programming Model","text":"<p>CUDA (Compute Unified Device Architecture) is NVIDIA\u2019s parallel computing platform and programming model, designed to enable software developers to leverage GPU acceleration. CUDA allows fine-grained control over GPU execution, making it suitable for high-performance applications such as scientific computing, deep learning, and real-time rendering.</p>"},{"location":"ai/pages/parallelism/#cuda-execution-model","title":"CUDA Execution Model","text":"<p> Grid-Block-Thread Hierarchy Source: NVIDIA Developer</p> <ul> <li>Grid-Block-Thread Hierarchy: CUDA organizes parallel execution into a hierarchy:<ul> <li>A Grid consists of multiple Blocks.</li> <li>A Block consists of multiple Threads.</li> <li>Each Thread executes a CUDA kernel function independently.</li> </ul> </li> <li>Warp Execution: Threads in a block are further grouped into warps of 32 threads, which execute in lockstep.</li> <li>Memory Model: CUDA provides various types of memory:<ul> <li>Global Memory: Accessible by all threads, but has high latency.</li> <li>Shared Memory: Faster, but shared among threads in a block.</li> <li>Registers and Local Memory: Used by individual threads for fast access.</li> </ul> </li> </ul> <p> Memory Model Source: NVIDIA Developer</p>"},{"location":"ai/pages/parallelism/#fine-grained-multithreading-in-cuda","title":"Fine-Grained Multithreading in CUDA","text":"<p>Fine-grained multithreading is a key mechanism in CUDA architectures that allows GPUs to maintain high utilization despite memory latency. It works by rapidly switching between active warps, ensuring that the GPU's computational resources remain busy.</p> <p> Source: Dr. Chen Pan UTSA</p> <ul> <li>Thread-Level Parallelism (TLP): Each Streaming Multiprocessor (SM) manages multiple warps, allowing another warp to execute while one waits for memory access.</li> <li>Instruction-Level Parallelism (ILP): Within a warp, independent instructions can be issued in parallel to optimize execution throughput.</li> <li>Occupancy: The number of active warps per SM impacts overall performance. Maximizing occupancy ensures better latency hiding and higher computational efficiency.</li> </ul> <p>By designing CUDA kernels to increase TLP and ILP, developers can take full advantage of fine-grained multithreading, reducing idle cycles and enhancing performance.</p>"},{"location":"ai/pages/parallelism/#streaming-multiprocessors-sms","title":"Streaming Multiprocessors (SMs)","text":"<p> Source: Dr. Chen Pan UTSA</p> <p>Streaming Multiprocessors (SMs) are the fundamental processing units in an NVIDIA GPU. Each SM contains multiple CUDA cores, warp schedulers, and execution pipelines, allowing efficient parallel execution of CUDA threads.</p> <ul> <li>Warp Scheduling: Each SM schedules and executes multiple warps concurrently, ensuring high throughput.</li> <li>Execution Units: An SM consists of CUDA cores, Special Function Units (SFUs) for mathematical operations, and Load/Store Units (LD/ST) for memory access.</li> <li>Memory Hierarchy within SMs:<ul> <li>Shared Memory: A low-latency memory space shared among threads within a block.</li> <li>Registers: The fastest memory available, allocated per thread.</li> <li>L1 Cache: Improves data locality and reduces global memory access latency.</li> </ul> </li> </ul> <p>Optimizing CUDA programs requires balancing workloads across multiple SMs, managing memory efficiently, and reducing warp divergence.</p>"},{"location":"electronics/pages/","title":"Overview","text":"<p>Mesuring Equipment</p> <p>Electronics:</p> <ul> <li>Operational Amplifier</li> <li>Analog to Digital Converter</li> </ul>"},{"location":"electronics/pages/adc/","title":"Analog to Digital Converters","text":""},{"location":"electronics/pages/adc/#sampling-rate","title":"Sampling Rate","text":"<p>Sampling rate (or sampling frequency) refers to the number of samples taken per second from an analog signal to convert it into a digital signal.</p> <p>Nyquist Theorem:</p> <ul> <li>According to the Nyquist-Shannon sampling theorem, the sampling rate must be at least twice the maximum frequency present in the analog signal to accurately reconstruct the signal.</li> </ul> \\[ f_s \\geq 2 \\times f_{max} \\]"},{"location":"electronics/pages/adc/#bandwidth","title":"Bandwidth","text":"<p>Bandwidth is the range of frequencies within a given signal.</p> <p></p> <p>In ADC context, bandwidth typically refers to the range of frequencies that the ADC can accurately sample and digitize. Realize this is NOT the standard definition found in other electronic fields.</p> <p></p> <p>Anti-Aliasing Filters:</p> <ul> <li>Usually placed before the ADC to ensure that the input signal bandwidth does not exceed half the sampling rate.</li> <li>Usually a low pass filter </li> </ul> \\[ BW \\leq \\frac{f_s}{2} \\] <p></p>"},{"location":"electronics/pages/adc/#signal-to-noise-ratio-snr","title":"Signal-to-Noise Ratio (SNR)","text":"<p>Signal-to-Noise Ratio (SNR) is an error metric used to measure the quality of the ADC output.</p>"},{"location":"electronics/pages/adc/#mathematical-representation","title":"Mathematical Representation","text":"<ul> <li>Let the error between ADC input \\( V(t) \\) and digital output \\( V_q(t) \\) be:</li> </ul> \\[ e(t) = V(t) - V_q(t) \\] <ul> <li>The SNR is defined as:</li> </ul> \\[ SNR = \\frac{E[V^2(t)]}{E[e^2(t)]} \\]"},{"location":"electronics/pages/adc/#importance","title":"Importance","text":"<ul> <li>Higher SNR means better ADC performance and less quantization error.</li> </ul>"},{"location":"electronics/pages/adc/#snr-for-an-n-bit-adc","title":"SNR for an N-bit ADC","text":"<ul> <li>For an N-bit ADC given a full-scale sine wave input, the SNR in decibels is:</li> </ul> \\[ SNR_{dB} = 6.02 \\times N + 1.76 \\text{ dB} \\] <ul> <li>This is often simplified as approximately 6 dB per bit.</li> </ul>"},{"location":"electronics/pages/adc/#example-number-of-adc-levels","title":"Example - Number of ADC Levels","text":"<ul> <li>For an 8-bit ADC:</li> </ul> \\[ SNR_{dB} = 6.02 \\times 8 + 1.76 \\approx 48 \\text{ dB} \\]"},{"location":"electronics/pages/adc/#example-step-size-of-adc-levels","title":"Example \u2013 Step-size of ADC Levels","text":"<ul> <li> <p>Question: How many volts does each level step-size represent for an 8-bit ADC ranging from \u22121 V to +1 V?</p> </li> <li> <p>Calculation:</p> </li> </ul> \\[ \\text{Step size} = \\frac{\\text{Voltage range}}{\\text{Number of levels}} = \\frac{2\\,\\text{V}}{256} \\approx 7.8\\,\\text{mV/level} \\] <p>Note: Amplitude \\( A = 1\\,\\text{V} \\), hence input voltage range is \\( 2A = 2\\,\\text{V} \\).</p>"},{"location":"electronics/pages/adc/#improving-snr","title":"Improving SNR","text":"<ul> <li>Use a higher number of bits (higher resolution ADC).</li> <li>Amplify the input signal to fully utilize the ADC input range.</li> </ul>"},{"location":"electronics/pages/adc/#required-amplification-and-input-offset-considerations","title":"Required Amplification and Input Offset Considerations","text":""},{"location":"electronics/pages/adc/#required-amplification","title":"Required Amplification","text":""},{"location":"electronics/pages/adc/#scenario-i","title":"Scenario (i)","text":"<ul> <li>If you can guarantee the peak-to-peak voltage of the original signal is \\(\\bar{V}_{sig}\\), then the desired gain is:</li> </ul> \\[ \\text{Desired gain} = \\frac{\\text{ADC input range}}{\\bar{V}_{sig, p-p}} \\] <ul> <li>Example: If the ADC range is 0 V to 5 V and \\(\\bar{V}_{sig, p-p} = 20\\,\\text{mV}\\), then:</li> </ul> \\[ \\text{Desired gain} = \\frac{5\\,\\text{V}}{20\\,\\text{mV}} = 250 \\]"},{"location":"electronics/pages/adc/#scenario-ii","title":"Scenario (ii)","text":"<ul> <li>If you cannot guarantee the peak-to-peak voltage of the original signal, you should leave some 'headroom' to avoid overflow, which is worse for SNR.</li> <li>Typically, amplify the input signal to fill 1/8 to 1/2 of the ADC's input range.</li> </ul>"},{"location":"electronics/pages/adc/#input-offset-considerations","title":"Input Offset Considerations","text":"<ul> <li>Often the input signal is both positive and negative.</li> <li>The ADC input typically expects only positive inputs.</li> <li>Therefore, a DC offset must be applied.</li> </ul> <p>Example:</p> <ul> <li>A full-scale signal might be represented as:</li> </ul> \\[ v(t) = 2.5\\sin(2\\pi ft) + 2.5\\,\\text{V} \\] <ul> <li> <p>If the original signal is \\( v(t) = 0.004\\sin(\\omega t) \\), you need to:</p> <ol> <li>Apply a DC offset.</li> <li>Amplify the signal.</li> </ol> </li> <li> <p>Different circuits and methods can achieve this for amplification and DC offset.</p> </li> </ul>"},{"location":"electronics/pages/adc/#evaluating-the-transfer-function","title":"Evaluating the Transfer Function","text":""},{"location":"electronics/pages/adc/#gain-bandwidth-product-gbp","title":"Gain-Bandwidth Product (GBP)","text":"<p>The Gain-Bandwidth Product (GBP) is a key performance metric of an operational amplifier that defines the trade-off between gain and bandwidth. It states that for a given operational amplifier, the product of the closed-loop gain and the bandwidth remains constant.</p> \\[ \\text{Gain} \\times \\text{Bandwidth} = \\text{GBP} (Hz) \\] <ul> <li>This means that increasing the gain will reduce the available bandwidth and vice versa.</li> </ul>"},{"location":"electronics/pages/adc/#example-calculation","title":"Example Calculation","text":"<ul> <li>Suppose an op-amp has a GBP of 10 MHz.</li> <li>If it is configured with a gain of 400, the bandwidth can be estimated as:</li> </ul> \\[ \\text{Bandwidth} = \\frac{GBP}{Gain} = \\frac{10 MHz}{400} = 25 kHz \\] <ul> <li>This low bandwidth may limit high-frequency applications.</li> </ul>"},{"location":"electronics/pages/adc/#improving-bandwidth-with-multiple-stages","title":"Improving Bandwidth with Multiple Stages","text":"<ul> <li>Instead of using a single-stage amplifier with a gain of 400, consider using two cascaded amplifiers, each with a gain of 20.</li> <li>If both amplifiers have a GBP of 10 MHz:</li> </ul> \\[ \\text{Bandwidth} = \\frac{10 MHz}{20} = 500 kHz \\] <ul> <li>This significantly increases the bandwidth compared to a single-stage design.</li> </ul>"},{"location":"electronics/pages/adc/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>Higher gain configurations lead to reduced bandwidth, affecting high-speed applications.</li> <li>Using multiple amplifier stages distributes gain and helps maintain higher bandwidth.</li> <li>Important for ADC signal conditioning to ensure accurate data acquisition.</li> </ul>"},{"location":"electronics/pages/adc/#hiletgo-ads1256-5v-8-channel-24-bit-adc-data-acquisition-board-module","title":"HiLetgo ADS1256 5V 8 Channel 24 Bit ADC Data Acquisition Board Module","text":"<p>The HiLetgo ADS1256 is a high-precision, 24-bit analog-to-digital converter (ADC) data acquisition module. It provides excellent resolution and accuracy for analog signal measurement, suitable for various precision measurement applications.</p> <ul> <li>Resolution: 24-bit ADC providing extremely fine quantization.</li> <li>Channels: 8 single-ended or 4 differential analog input channels.</li> <li>Operating Voltage: 5V operation, compatible with most microcontrollers and development boards.</li> <li>High Precision: Designed for high-precision applications with low noise and high stability.</li> <li>Data Rate: Programmable data rates up to 30,000 samples per second (30 kSPS).</li> </ul> <p>The Good: - High sampling rate</p> <p>The Bad: - Works at 5V, too high for many gpios. Had to buy SN74AHCT125N to use for the teensy 4.0 and Jetson Nano</p> <p>Amazon</p>"},{"location":"electronics/pages/equipment/","title":"Measuring Equipment","text":""},{"location":"electronics/pages/equipment/#oscilloscope","title":"Oscilloscope","text":""},{"location":"electronics/pages/equipment/#probes","title":"Probes","text":"<p>Oscilloscope probes connect the circuit under test to the oscilloscope. A typical probe consists of a probe head, flexible cable, and a BNC connector for oscilloscope input. The probe tip includes attachments like a spring-loaded hook for better contact and a ground connection (alligator clip).</p> <p>Types of Oscilloscope Probes</p> <p></p> <ol> <li>Passive Probes \u2013 The most common type, available in 1\u00d7 (low-frequency, low-voltage) and 10\u00d7 (high-frequency, high-voltage) configurations.  </li> <li>Active Probes \u2013 Include transistors and amplifiers, requiring power. Suitable for high-frequency, low-capacitance applications.  </li> <li>Differential Probes \u2013 Measure voltage between two points instead of ground. Ideal for floating measurements.  </li> <li>Current Probes \u2013 Detect magnetic fields from current flow and convert them to voltage. Used for non-intrusive current measurement.  </li> </ol> <p>Passive Probes: Ideal vs. Real</p> <p>An ideal probe would have infinite bandwidth and zero attenuation, but real probes have frequency-dependent behavior.  </p> <ul> <li>1\u00d7 Setting: Used for signals &lt;10 MHz and voltages &lt;1V. Low-pass filter effect due to oscilloscope input capacitance.  </li> <li>10\u00d7 Setting: Reduces amplitude by 10\u00d7, making it better for high-voltage, high-frequency signals. Compensation cancels out oscilloscope capacitance.  </li> </ul> <p>Probe Compensation</p> <p>Probe capacitance must be adjusted to match the oscilloscope\u2019s internal capacitance for accurate measurements. Poor compensation results in distorted waveforms. Steps: </p> <ol> <li>Connect the probe to the oscilloscope\u2019s compensation terminal.  </li> <li>Adjust the probe\u2019s capacitance using the small screw near the probe head until a clean square wave is displayed.  </li> <li>Always compensate when switching probes or using different oscilloscope channels.  </li> </ol> <p></p> <p>Setting 1\u00d7 &amp; 10\u00d7 Probes</p> <p>Most measurements use 10\u00d7 probes. Ensure the oscilloscope matches the probe setting:  </p> <ul> <li>Some models autodetect 10\u00d7 if the probe has a metal detection pin.  </li> <li>Otherwise, manually adjust the oscilloscope settings.  </li> </ul> <p>Probe Loading Effect</p> <p>Probes affect circuit behavior through input resistance and input capacitance:  </p> <ul> <li>Input resistance slightly reduces the measured signal\u2019s amplitude.  </li> <li>Input capacitance loads the circuit, impacting rise times at high frequencies.  </li> </ul> <p>\ud83d\udcfa Resources: </p> <ul> <li>Probe Loading Affects Your Measurement \u2013 Tektronix </li> <li>EEVblog #453 \u2013 Mysteries of x1 Oscilloscope Probes Revealed </li> <li>All About Circuits</li> </ul>"},{"location":"electronics/pages/opamp/","title":"Operational Amplifiers","text":""},{"location":"electronics/pages/opamp/#negative-feedback","title":"Negative Feedback","text":""},{"location":"electronics/pages/opamp/#overview","title":"Overview","text":"<p>Negative feedback is a crucial concept in operational amplifiers that enhances stability and control.</p>"},{"location":"electronics/pages/opamp/#important-equations","title":"Important Equations","text":"<ul> <li>Closed-loop Transfer Function (Gain):</li> </ul> \\[ H_{closed}(j\\omega) = \\frac{A(j\\omega)}{1 + A(j\\omega)B(j\\omega)} \\] <ul> <li>Where \\( A(j\\omega) \\) is the open-loop gain</li> <li> <p>\\( B(j\\omega) \\) is the feedback factor</p> </li> <li> <p>The Non-Inverting Amplifier examples:</p> </li> </ul> <p></p>"},{"location":"electronics/pages/opamp/#stability-considerations","title":"Stability Considerations","text":""},{"location":"electronics/pages/opamp/#overview_1","title":"Overview","text":"<p>Understanding stability is essential to ensure that negative feedback systems do not lead to oscillations or performance degradation.</p>"},{"location":"electronics/pages/opamp/#stability-analysis","title":"Stability Analysis","text":"<ul> <li>Oscillations: Occur if system poles are near or on the imaginary axis</li> <li>Nyquist Stability Criterion: Determines stability based on the system's loop gain</li> <li>Routh-Hurwitz Criterion: Used to analyze stability in the time domain</li> </ul>"},{"location":"electronics/pages/opamp/#gain-and-phase-margins","title":"Gain and Phase Margins","text":"<ul> <li>Gain Margin (G.M.): Distance (in dB) from 0 dB to magnitude at frequency \\( f_{\\pm180} \\)</li> <li>Phase Margin (P.M.): Distance (in degrees) from \\( \\pm180^\\circ \\) to the frequency where gain crosses 0 dB</li> <li> <p>Loop Gain Analysis:</p> \\[ Loop Gain = A(j\\omega)B(j\\omega) \\] </li> </ul>"},{"location":"electronics/pages/opamp/#stability-guidelines","title":"Stability Guidelines","text":"<ul> <li>A system is stable if G.M. \u2265 10 dB and P.M. \u2265 45\u00b0</li> <li>If the loop gain crosses \\( -1 \\) at a critical frequency, the system may oscillate</li> </ul>"},{"location":"electronics/pages/opamp/#example-transfer-function-for-active-inverting-low-pass-filter-lpf","title":"Example: Transfer Function for Active Inverting Low-Pass Filter (LPF)","text":"<p>This example demonstrates how to compute the transfer function and the cut off frequnecy of a first-order inverting LPF using op-amp feedback.</p> <p></p> <p>Example solved here</p>"},{"location":"electronics/pages/opamp/#example-stability-via-gain-and-phase-margin","title":"Example: Stability via Gain and Phase Margin","text":"<p>This example applies gain and phase margin concepts to assess system stability.</p> <p>Example solved here</p>"},{"location":"senior_design/pages/","title":"Senior Design","text":""},{"location":"senior_design/pages/#resources","title":"Resources","text":"<ul> <li>OpenBionics<ul> <li>Open source modular robot and prosthetic hands</li> </ul> </li> </ul>"}]}